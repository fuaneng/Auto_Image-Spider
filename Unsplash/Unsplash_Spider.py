import os, time, hashlib, csv, threading, random, urllib.parse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
import redis
import redis.exceptions # ç”¨äºæ›´å…·ä½“çš„ Redis å¼‚å¸¸å¤„ç†

# ---------- å…¨å±€æ»šåŠ¨ç­–ç•¥å¸¸é‡ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰ ----------
MAX_SCROLLS = 100  # æœ€å¤§çš„æ»šåŠ¨æ¬¡æ•°é™åˆ¶ï¼Œé˜²æ­¢æ— é™å¾ªç¯
NO_NEW_ROUNDS_TO_STOP = 2    # è¿ç»­å¤šå°‘è½®æ¬¡æœªå‘ç°æ–°å›¾ç‰‡æ—¶ï¼Œåˆ¤å®šé¡µé¢å†…å®¹å·²åŠ è½½å®Œæ¯•å¹¶åœæ­¢æŠ“å–
SMALL_SCROLL_AMOUNT = 200   # æ»šåŠ¨é‡ï¼ˆåƒç´ ï¼‰ã€‚ç”¨äºåœ¨æ¥è¿‘åº•éƒ¨æ—¶è¿›è¡Œå¾®è°ƒï¼Œè§¦å‘æ‡’åŠ è½½ã€‚
# ğŸš€ æ–°å¢ï¼šé»˜è®¤å¹¶å‘çº¿ç¨‹æ•°
DEFAULT_MAX_THREADS = 5 


class UnsplashSpider:
    """
    Unsplash çˆ¬è™«ç±»ï¼Œè´Ÿè´£å•ä¸ªæ ‡ç­¾çš„çˆ¬å–ä»»åŠ¡ï¼Œå¹¶ç®¡ç†æµè§ˆå™¨å®ä¾‹ã€‚
    """
    def __init__(self, chrome_driver_path, use_headless=True, redis_host='localhost', redis_port=6379):
        # âš ï¸ æ£€æŸ¥ driver è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(chrome_driver_path):
            raise FileNotFoundError(f"ChromeDriver not found at path: {chrome_driver_path}")

        service = Service(executable_path=chrome_driver_path)
        options = Options()
        # éšè—è‡ªåŠ¨åŒ–æ§åˆ¶ä¿¡æ¯
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        # è®¾ç½® User-Agent
        options.add_argument(
            'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        )
        # è®¾ç½®æ— å¤´æ¨¡å¼
        if use_headless:
            options.add_argument('--headless=new')
        options.add_argument("--window-size=1920,1080")
        
        # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼Œæé«˜é€Ÿåº¦ï¼ˆå¯é€‰ï¼Œä½†ä¼šå¢åŠ è·å– URL çš„å¤æ‚æ€§ï¼Œæ­¤å¤„ä¿ç•™ï¼‰
        # prefs = {"profile.managed_default_content_settings.images": 2}
        # options.add_experimental_option("prefs", prefs)

        self.driver = webdriver.Chrome(service=service, options=options)
        self.base_url = 'https://unsplash.com/'
        
        # å¤šçº¿ç¨‹å…±äº« CSV å†™å…¥é”
        self.csv_lock = threading.Lock() 
        self.redis_key = 'image_md5_set_unsplash'
        
        # ã€å…³é”®æŠ“å–å…ƒç´  1ï¼šå›¾ç‰‡å¡ç‰‡å®¹å™¨ã€‘
        self.image_card_selector = 'figure[data-testid="asset-grid-masonry-figure"]'

        # Redis è¿æ¥ï¼ˆå¯é€‰ï¼Œç”¨äºè·¨ä»»åŠ¡å»é‡ï¼‰
        try:
            self.redis = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
            self.redis.ping()
        except redis.exceptions.ConnectionError:
            print(f"âš ï¸ Redis è¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_md5 = set()
        except Exception:
            self.redis = None
            self.visited_md5 = set()


    # ---------------------- å¢é‡è§£ææ–°åŠ è½½çš„å›¾ç‰‡å…ƒç´  ----------------------
    def get_images(self, tag, csv_path, elements):
        """
        è§£æå›¾ç‰‡å¡ç‰‡å…ƒç´ ï¼Œæå–å›¾ç‰‡URLã€æ ‡é¢˜ï¼Œè¿›è¡Œå»é‡å¹¶å†™å…¥CSVã€‚
        é›†æˆäº† Stale Element å¼•ç”¨å¤±æ•ˆçš„é‡è¯•å¤„ç†ã€‚
        """
        successful_writes = 0
        for card_ele in elements:
            
            # ğŸš€ Stale Element é‡è¯•é€»è¾‘
            max_retries = 2
            attempt = 0
            processed_successfully = False
            
            while attempt < max_retries and not processed_successfully:
                try:
                    # --- åŸå§‹çš„å¤„ç†é€»è¾‘å— START ---
                    
                    # ã€å…³é”®æŠ“å–å…ƒç´  2ï¼šæœ€é«˜è´¨é‡ä¸‹è½½é“¾æ¥ã€‘
                    image_url = None
                    try:
                        download_btn = card_ele.find_element(
                            By.CSS_SELECTOR,
                            "a[data-testid='non-sponsored-photo-download-button']"
                        )
                        image_url = download_btn.get_attribute('href') or ''
                    except NoSuchElementException:
                        pass
                    
                    # ã€å…³é”®æŠ“å–å…ƒç´  3ï¼šIMG æ ‡ç­¾åŠ URL å¤‡é€‰æ–¹æ¡ˆã€‘
                    if not image_url:
                        try:
                            img_ele = card_ele.find_element(By.CSS_SELECTOR, 'img')
                            # ä¼˜å…ˆ srcset
                            srcset = img_ele.get_attribute('srcset') or ''
                            if srcset:
                                parts = [p.strip() for p in srcset.split(',') if p.strip()]
                                if parts:
                                    image_url = parts[-1].split()[0]
                            if not image_url:
                                image_url = img_ele.get_attribute('src') or ''
                        except Exception:
                            image_url = None

                    if not image_url:
                        processed_successfully = True # æ ‡è®°ä¸ºæˆåŠŸå¤„ç†(è·³è¿‡)ï¼Œé€€å‡ºwhile
                        continue

                    # è§„èŒƒåŒ–é“¾æ¥
                    image_url_cleaned = urllib.parse.urljoin(self.base_url, image_url)

                    # ã€å…³é”®æŠ“å–å…ƒç´  4ï¼šå›¾ç‰‡æ ‡é¢˜æå–ã€‘
                    title = "N/A"
                    try:
                        img_for_title = card_ele.find_element(By.CSS_SELECTOR, 'img')
                        alt = img_for_title.get_attribute('alt') # 1. ä¼˜å…ˆ alt å±æ€§
                        if alt and alt.strip():
                            title = alt.strip()
                        else:
                            a_link = card_ele.find_element(By.CSS_SELECTOR, 'a.photoInfoLink-mG0SPO') # 2. å¤‡é€‰ï¼šä¿¡æ¯é“¾æ¥
                            a_title = a_link.get_attribute('title')
                            if a_title and a_title.strip():
                                title = a_title.strip()
                            elif title == "N/A": 
                                try:
                                    # 3. æœ€ç»ˆå¤‡é€‰ï¼šå›¾æ³¨/è¯´æ˜æ–‡å­—
                                    caption = card_ele.find_element(By.CSS_SELECTOR, 'figcaption, .photo-info, .photoMeta')
                                    txt = caption.text.strip()
                                    if txt:
                                        title = txt
                                except Exception:
                                    pass
                    except Exception:
                        pass

                    # å»é‡ï¼šä½¿ç”¨ URL çš„ md5
                    md5_hash = hashlib.md5(image_url_cleaned.encode('utf-8')).hexdigest()
                    if self.is_duplicate(md5_hash):
                        processed_successfully = True # æ ‡è®°ä¸ºæˆåŠŸå¤„ç†(é‡å¤)ï¼Œé€€å‡ºwhile
                        continue

                    # image name
                    image_name = os.path.basename(urllib.parse.urlparse(image_url_cleaned).path)
                    if not image_name or len(image_name) < 5:
                        image_name = md5_hash + ".jpg"

                    # å†™å…¥ CSV
                    self.write_to_csv(title, image_name, image_url_cleaned, csv_path, tag)
                    successful_writes += 1
                    print(f"[{tag}] âœ”ï¸ å†™å…¥ï¼š{image_name}")
                    
                    processed_successfully = True # æ ‡è®°ä¸ºæˆåŠŸå¤„ç†ï¼Œé€€å‡ºwhile
                    
                    # --- åŸå§‹çš„å¤„ç†é€»è¾‘å— END ---

                except NoSuchElementException:
                    print(f"[{tag}] [WARN] å…ƒç´ ç»“æ„å¼‚å¸¸ï¼Œè·³è¿‡å½“å‰å¡ç‰‡ã€‚")
                    processed_successfully = True # é€€å‡ºwhile
                
                except StaleElementReferenceException:
                    attempt += 1
                    print(f"[{tag}] [STALE] å…ƒç´ å¼•ç”¨å¤±æ•ˆï¼Œé‡è¯•ç¬¬ {attempt} æ¬¡...")
                    time.sleep(0.5) # çŸ­æš‚ç­‰å¾… DOM ç¨³å®š
                    if attempt >= max_retries:
                        print(f"[{tag}] [FAIL] å…ƒç´ å¼•ç”¨é‡è¯• {max_retries} æ¬¡åä»å¤±æ•ˆï¼Œè·³è¿‡è¯¥å…ƒç´ ã€‚")
                        processed_successfully = True # é€€å‡ºwhile
                
                except Exception as e:
                    print(f"[{tag}] [âœ—] å¤„ç†å…ƒç´ å‡ºé”™: {e}")
                    processed_successfully = True # é€€å‡ºwhile

        return successful_writes


    # ---------------------- é¡µé¢æ»šåŠ¨å¹¶æŠ“å–ç›´åˆ°ç¨³å®šï¼ˆå¼ºåŒ–æ»šåŠ¨æ“ä½œï¼‰ ----------------------
    def crawl_page(self, search_url: str, tag: str, csv_path: str) -> bool:
        """
        æ»šåŠ¨åŠ è½½å›¾ç‰‡å¹¶è°ƒç”¨ get_images è¿›è¡Œè§£æã€‚
        """
        wait = WebDriverWait(self.driver, 10)
        image_card_selector = self.image_card_selector

        try:
            print(f"[{tag}] [INFO] å¯¼èˆªåˆ°ï¼š{search_url}")
            self.driver.get(search_url)
            # ç­‰å¾…å›¾ç‰‡å¡ç‰‡å…ƒç´ å‡ºç°
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, image_card_selector)))
            print(f"[{tag}] [âˆš] é¡µé¢å·²åŠ è½½ï¼Œæ‰¾åˆ°å›¾ç‰‡å¡ç‰‡å®¹å™¨ã€‚")
        except TimeoutException:
            print(f"[{tag}] [âœ—] é¡µé¢åŠ è½½è¶…æ—¶æˆ–æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡å¡ç‰‡ã€‚")
            return False
        except Exception as e:
            print(f"[{tag}] [âœ—] é¡µé¢åŠ è½½å¼‚å¸¸ï¼š{e}")
            return False

        # ---------- æ»šåŠ¨ä¸»å¾ªç¯ ----------
        scroll_cycle_count = 0
        processed_count = 0
        no_new_content_rounds = 0

        # å°è¯•ç‚¹å‡» body ç¡®ä¿ç„¦ç‚¹åœ¨é¡µé¢ä¸Š
        try:
            self.driver.find_element(By.TAG_NAME, 'body').click()
        except Exception:
            pass

        while scroll_cycle_count < MAX_SCROLLS:
            
            # 1. è§£æå½“å‰å·²åŠ è½½ä½†æœªå¤„ç†çš„å›¾ç‰‡
            all_elements = self.driver.find_elements(By.CSS_SELECTOR, image_card_selector)
            current_count = len(all_elements)

            if current_count > processed_count:
                new_elements = all_elements[processed_count:]
                written = self.get_images(tag, csv_path, new_elements)
                processed_count = current_count
            
            # 2. æ»šåŠ¨æ“ä½œ (ä¼˜å…ˆä½¿ç”¨ END é”®)
            try:
                body = self.driver.find_element(By.TAG_NAME, 'body')
                body.send_keys(Keys.END)
                time.sleep(random.uniform(1.0, 2.0))
            except Exception:
                try:
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(random.uniform(1.0, 2.0))
                except Exception as js_e:
                    print(f"[{tag}] [FATAL] JavaScript æ»šåŠ¨å¤±è´¥: {js_e}")
                    break

            # 3. ç­‰å¾…æ–°å†…å®¹å‡ºç°
            try:
                wait.until(
                    lambda driver: len(driver.find_elements(By.CSS_SELECTOR, image_card_selector)) > processed_count
                )
                no_new_content_rounds = 0
            except TimeoutException:
                no_new_content_rounds += 1
                print(f"[{tag}] [âš ] ç­‰å¾…è¶…æ—¶ï¼Œæœªå‘ç°æ–°å›¾ç‰‡ã€‚({no_new_content_rounds}/{NO_NEW_ROUNDS_TO_STOP})")

                # ã€å…³é”®æŠ“å–å…ƒç´  6ï¼šLoad more æŒ‰é’®ã€‘ å°è¯•ç‚¹å‡» 'Load more'
                try:
                    load_more_button = self.driver.find_element(
                        By.CSS_SELECTOR,
                        "button.loadMoreButton-pYP1fq" 
                    )
                    if load_more_button.is_displayed() and load_more_button.is_enabled():
                        print(f"[{tag}] [INFO] å‘ç° 'Load more' æŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»ã€‚")
                        load_more_button.click()
                        no_new_content_rounds = 0 
                        time.sleep(random.uniform(2.0, 3.0)) 
                        print(f"[{tag}] [âˆš] 'Load more' æŒ‰é’®å·²ç‚¹å‡»ï¼Œé‡æ–°ç­‰å¾…æ–°å†…å®¹åŠ è½½ã€‚")
                        scroll_cycle_count += 1
                        continue # è·³åˆ°ä¸‹ä¸€è½®å¾ªç¯
                except NoSuchElementException:
                    pass
                except Exception as click_e:
                    print(f"[{tag}] [WARN] ç‚¹å‡» 'Load more' æŒ‰é’®å‡ºé”™: {click_e}")
                    
                # æ£€æŸ¥æ˜¯å¦çœŸçš„åˆ°åº•éƒ¨
                try:
                    scroll_height = self.driver.execute_script("return document.body.scrollHeight")
                    current_scroll = self.driver.execute_script("return window.scrollY + window.innerHeight")
                except Exception:
                    scroll_height = current_scroll = 0

                if abs(scroll_height - current_scroll) < 50:
                    print(f"[{tag}] [INFO] æ»šåŠ¨æ¡å·²åˆ°è¾¾é¡µé¢æœ€åº•éƒ¨ã€‚")
                else:
                    # å¾®è°ƒæ»šåŠ¨
                    try:
                        self.driver.execute_script(f"window.scrollBy(0, {SMALL_SCROLL_AMOUNT});")
                        time.sleep(1)
                    except Exception:
                        pass

                # 4. å¦‚æœè¶…è¿‡é˜ˆå€¼åˆ™ç»“æŸ
                if no_new_content_rounds >= NO_NEW_ROUNDS_TO_STOP:
                    print(f"[{tag}] [å®Œæˆ] å·²è¿ç»­ {NO_NEW_ROUNDS_TO_STOP} è½®æœªå‘ç°æ–°å†…å®¹ï¼Œåˆ¤å®š'{tag}'å·²æŠ“å–å®Œæ¯•ã€‚")
                    
                    # é€€å‡ºå‰æœ€åä¸€æ¬¡è§£æ
                    all_elements_final = self.driver.find_elements(By.CSS_SELECTOR, image_card_selector)
                    final_count = len(all_elements_final)
                    if final_count > processed_count:
                        new_elements_final = all_elements_final[processed_count:]
                        self.get_images(tag, csv_path, new_elements_final)
                    break

            scroll_cycle_count += 1
            if scroll_cycle_count >= MAX_SCROLLS:
                print(f"[{tag}] [âš ] è¾¾åˆ°æœ€å¤§æ»šåŠ¨æ¬¡æ•° {MAX_SCROLLS}ï¼Œå¼ºåˆ¶åœæ­¢ã€‚")
                break

            time.sleep(random.uniform(0.5, 1.5)) # å°éšæœºç­‰å¾…

        print(f"[{tag}] [âˆš] æ ‡ç­¾ '{tag}' æ»šåŠ¨æŠ“å–å®Œæˆï¼Œå…±æ‰¾åˆ°çº¦ {processed_count} å¼ å›¾ç‰‡ã€‚")
        return True


    # ---------------------- å»é‡é€»è¾‘ ----------------------
    def is_duplicate(self, md5_hash):
        """æ£€æŸ¥å¹¶è®°å½• MD5 å“ˆå¸Œä»¥è¿›è¡Œå»é‡ã€‚"""
        if self.redis:
            try:
                if self.redis.sismember(self.redis_key, md5_hash):
                    return True
                self.redis.sadd(self.redis_key, md5_hash)
            except Exception:
                # Redis å¼‚å¸¸æ—¶é™çº§åˆ°å†…å­˜å»é‡
                self.redis = None 
                if not hasattr(self, 'visited_md5'):
                    self.visited_md5 = set()
                if md5_hash in self.visited_md5:
                    return True
                self.visited_md5.add(md5_hash)
        
        if not self.redis:
            if md5_hash in self.visited_md5:
                return True
            self.visited_md5.add(md5_hash)
        return False

    # ---------------------- å†™å…¥ CSV ----------------------
    def write_to_csv(self, title, name, url, csv_path, tag):
        """å†™å…¥ CSV æ–‡ä»¶ï¼Œä½¿ç”¨çº¿ç¨‹é”ä¿è¯å¤šçº¿ç¨‹å®‰å…¨ã€‚"""
        try:
            with self.csv_lock:
                os.makedirs(os.path.dirname(csv_path), exist_ok=True)
                is_file_empty = not os.path.exists(csv_path) or os.stat(csv_path).st_size == 0
                with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    if is_file_empty:
                        writer.writerow(['Title', 'ImageName', 'URL', 'Tag'])
                    writer.writerow([title, name, url, tag])
        except Exception as e:
            print(f"[{tag}] [âœ—] å†™å…¥ CSV å‡ºé”™: {e}")

    # ---------------------- é€€å‡ºæµè§ˆå™¨ ----------------------
    def quit(self):
        """å…³é—­æµè§ˆå™¨å®ä¾‹ã€‚"""
        try:
            if self.driver:
                self.driver.quit()
        except Exception:
            pass


# ---------------------- ğŸš€ å¤šçº¿ç¨‹ Worker å‡½æ•° ----------------------
def run_spider_task(tag: str, chrome_driver_path: str, save_dir: str, csv_name: str, use_headless: bool, redis_host: str, redis_port: int, semaphore: threading.Semaphore):
    """
    å•ä¸ªæ ‡ç­¾çš„çˆ¬å–ä»»åŠ¡æ‰§è¡Œä½“ï¼Œåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œã€‚
    æ¯ä¸ªä»»åŠ¡éƒ½ä¼šå®ä¾‹åŒ–ä¸€ä¸ªç‹¬ç«‹çš„ UnsplashSpiderï¼ˆå«æµè§ˆå™¨ï¼‰ã€‚
    """
    spider = None
    csv_path = os.path.join(save_dir, csv_name)

    try:
        # --- 1. è·å–ä¿¡å·é‡ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰ ---
        semaphore.acquire() 
        print(f"\n[CONTROL] æ­£åœ¨ä¸ºæ ‡ç­¾ '{tag}' å¯åŠ¨æ–°çš„æµè§ˆå™¨å®ä¾‹...")

        # --- 2. å®ä¾‹åŒ– Spider (å«ç‹¬ç«‹æµè§ˆå™¨) ---
        spider = UnsplashSpider(
            chrome_driver_path=chrome_driver_path, 
            use_headless=use_headless,
            redis_host=redis_host,
            redis_port=redis_port
        )
        
        # --- 3. å‡†å¤‡ URL å¹¶æ‰§è¡Œçˆ¬å–ä»»åŠ¡ ---
        # ä¿®å¤ Unsplash ç‰¹æœ‰ URL ç¼–ç é€»è¾‘ï¼šå°†ç©ºæ ¼æ›¿æ¢ä¸ºç ´æŠ˜å·
        tag_with_dashes = tag.replace(' ', '-')
        encoded_tag = urllib.parse.quote(tag_with_dashes).replace('+', '-')
        search_url = f"{spider.base_url}s/photos/{encoded_tag}"
        
        print(f"[{tag}] --- å¼€å§‹å¤„ç†ï¼šURL: {search_url}")
        spider.crawl_page(search_url, tag, csv_path)
        print(f"[{tag}] [CONTROL] æ ‡ç­¾æŠ“å–ä»»åŠ¡å®Œæˆã€‚")

    except FileNotFoundError as e:
        print(f"[{tag}] [ERROR] å¯åŠ¨å¤±è´¥ï¼Œé©±åŠ¨æœªæ‰¾åˆ°: {e}")
    except Exception as e:
        print(f"[{tag}] [ERROR] å¤„ç†æ ‡ç­¾æ—¶å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {e}")
    finally:
        # --- 4. ç¡®ä¿å…³é—­æµè§ˆå™¨å®ä¾‹å’Œé‡Šæ”¾ä¿¡å·é‡ ---
        if spider:
            spider.quit()
        semaphore.release()
        print(f"[{tag}] [CONTROL] çº¿ç¨‹å·²ç»“æŸå¹¶é‡Šæ”¾ä¿¡å·é‡ã€‚")


# ---------------------- å¯åŠ¨è„šæœ¬ç¤ºä¾‹ (ä¸»æ§åˆ¶é€»è¾‘) ----------------------
if __name__ == '__main__':
    # âš ï¸ é…ç½®éƒ¨åˆ†ï¼šè¯·ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„è·¯å¾„å’Œè®¾ç½®
    chrome_driver_path = r'C:\Program Files\Google\chromedriver-win32\chromedriver.exe'
    save_dir = r'D:\work\çˆ¬è™«\çˆ¬è™«æ•°æ®\unsplash'
    tag_file_path = r'D:\work\çˆ¬è™«\çˆ¬è™«æ•°æ®\unsplash\ram_tag_list_å¤‡ä»½.txt'
    csv_name = 'all_records_unsplash.csv'
    use_headless = True
    
    # ğŸš€ å¹¶å‘æ§åˆ¶è®¾ç½®
    MAX_THREADS = DEFAULT_MAX_THREADS  # è®¾ç½®æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆæµè§ˆå™¨å®ä¾‹æ•°ï¼‰
    semaphore = threading.Semaphore(MAX_THREADS)

    os.makedirs(save_dir, exist_ok=True)
    
    # 1. è¯»å–æ ‡ç­¾
    try:
        with open(tag_file_path, 'r', encoding='utf-8') as f:
            tags = [line.strip() for line in f if line.strip()]
    except FileNotFoundError:
        print(f"[é”™è¯¯] æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {tag_file_path}")
        exit()

    print(f"\n--- å‘ç° {len(tags)} ä¸ªæ ‡ç­¾ï¼Œå°†ä½¿ç”¨ {MAX_THREADS} ä¸ªå¹¶å‘çº¿ç¨‹å¤„ç† ---")

    threads = []
    
    # 2. åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
    for tag in tags:
        # ä¸ºæ¯ä¸ªæ ‡ç­¾åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„çº¿ç¨‹
        t = threading.Thread(
            target=run_spider_task,
            args=(tag, chrome_driver_path, save_dir, csv_name, use_headless, 'localhost', 6379, semaphore),
            name=f"Thread-{tag}"
        )
        threads.append(t)
        t.start()
        
    # 3. ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for t in threads:
        t.join()

    print("\n[CONTROL] æ‰€æœ‰æ ‡ç­¾çˆ¬å–ä»»åŠ¡å·²å®Œæˆï¼")