import os
import re
import csv
import time
import requests
import urllib3
import redis
from lxml import etree
from threading import Lock
from concurrent.futures import ThreadPoolExecutor

# --- é…ç½®å¸¸é‡ ---
BASE_URL = "https://libreshot.com/?s={tag}"
TAG_FILE_PATH = r"D:\myproject\Code\çˆ¬è™«\çˆ¬è™«æ•°æ®\libreshot\ram_tag_list_å¤‡ä»½.txt"
CSV_DIR_PATH = r"D:\myproject\Code\çˆ¬è™«\çˆ¬è™«æ•°æ®\libreshot\results"  # æ›´æ”¹ä¸ºä½ æƒ³è¦çš„CSVè¾“å‡ºç›®å½•
CSV_FILENAME = "libreshot_images.csv"
MAX_WORKERS = 10  # çº¿ç¨‹æ± å¤§å°
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_KEY = 'libreshot_image_url_set' # ä½¿ç”¨å»é‡

class LibreshotScraper:
    """
    é«˜æ•ˆçš„ Libreshot çˆ¬è™«ï¼Œæ”¯æŒå¤šçº¿ç¨‹å’Œ Redis/å†…å­˜å»é‡ã€‚
    """
    def __init__(self, csv_dir_path, csv_filename, redis_host=REDIS_HOST, redis_port=REDIS_PORT):
        """
        åˆå§‹åŒ–çˆ¬è™«å®ä¾‹ï¼Œé›†æˆ Redis/å†…å­˜å»é‡é€»è¾‘ã€‚
        """
        self.csv_dir_path = csv_dir_path
        self.csv_path = os.path.join(self.csv_dir_path, csv_filename) 
        self.csv_lock = Lock() # ç”¨äº CSV å†™å…¥çš„çº¿ç¨‹é”
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # ç¦ç”¨ SSL è­¦å‘Š
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        # --- å»é‡åˆå§‹åŒ–é€»è¾‘ ---
        try:
            # å°è¯•è¿æ¥ Redis
            self.redis = redis.StrictRedis(host=redis_host, port=redis_port, decode_responses=True)
            # å°è¯•æ‰§è¡Œä¸€æ¬¡ ping æ¥éªŒè¯è¿æ¥
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸï¼Œä½¿ç”¨ Redis é›†åˆè¿›è¡Œå»é‡ã€‚")
        except redis.exceptions.ConnectionError as e:
            print(f"âš ï¸ Redis è¿æ¥å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            # å†…å­˜å»é‡é›†åˆ
            self.visited_urls = set()
        except Exception as e:
            print(f"âš ï¸ Redis åˆå§‹åŒ–é‡åˆ°å…¶ä»–é”™è¯¯ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_urls = set()

    def load_tags(self, file_path):
        """
        ä»æœ¬åœ°æ–‡ä»¶åŠ è½½æ ‡ç­¾åˆ—è¡¨ã€‚
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                # å»é™¤ç©ºç™½è¡Œå’Œé¦–å°¾ç©ºæ ¼
                tags = [line.strip() for line in f if line.strip()]
            print(f"âœ… æˆåŠŸåŠ è½½ {len(tags)} ä¸ªæ ‡ç­¾ã€‚")
            return tags
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯: æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ° -> {file_path}")
            return []
        except Exception as e:
            print(f"âŒ é”™è¯¯: è¯»å–æ ‡ç­¾æ–‡ä»¶å‡ºé”™ -> {e}")
            return []

    def is_url_visited(self, url):
        """
        ä½¿ç”¨ Redis æˆ–å†…å­˜é›†åˆæ£€æŸ¥ URL æ˜¯å¦å·²è¢«è®¿é—®ã€‚
        æˆåŠŸæ·»åŠ ï¼ˆæœªè®¿é—®è¿‡ï¼‰è¿”å› Trueï¼Œå¦åˆ™ï¼ˆå·²è®¿é—®è¿‡ï¼‰è¿”å› Falseã€‚
        """
        if self.redis:
            # Redis: SADD è¿”å› 1 è¡¨ç¤ºæ·»åŠ æˆåŠŸï¼ˆæœªé‡å¤ï¼‰ï¼Œè¿”å› 0 è¡¨ç¤ºå·²å­˜åœ¨ï¼ˆé‡å¤ï¼‰
            return self.redis.sadd(REDIS_KEY, url) == 1
        else:
            # å†…å­˜å»é‡
            if url not in self.visited_urls:
                self.visited_urls.add(url)
                return True
            return False
            
    def write_to_csv(self, title, name, url, csv_path, tag):
        """
        å†™å…¥ CSV æ–¹æ³•ï¼Œå·²é›†æˆçº¿ç¨‹é”ï¼Œç¡®ä¿å¤šçº¿ç¨‹å®‰å…¨ã€‚
        """
        try:
            with self.csv_lock: 
                os.makedirs(os.path.dirname(csv_path), exist_ok=True)
                is_file_empty = not os.path.exists(csv_path) or os.stat(csv_path).st_size == 0
                
                with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    
                    if is_file_empty:
                        writer.writerow(['Title', 'ImageName', 'URL', 'TAG'])
                        
                    writer.writerow([title, name, url, tag])
        except Exception as e:
            print(f"[{tag}] [âœ—] å†™å…¥ CSV å‡ºé”™: {e}")

    def clean_image_url(self, image_url):
        """
        å»é™¤å›¾ç‰‡ URL ä¸­çš„åˆ†è¾¨ç‡å‚æ•°ï¼ˆå¦‚ -508x339.jpgï¼‰ä»¥è·å–åŸå›¾ URLã€‚
        å¹¶æå–å›¾ç‰‡åç§°ã€‚
        """
        # å°è¯•åŒ¹é…å¹¶å»é™¤å½¢å¦‚ '-å®½åº¦xé«˜åº¦' çš„éƒ¨åˆ†
        pattern = r'(-\d+x\d+)(\.[a-zA-Z0-9]+)$'

        match = re.search(pattern, image_url)
        if match:
            # åŸå›¾ URL: æ›¿æ¢åˆ†è¾¨ç‡éƒ¨åˆ†
            original_url = image_url.replace(match.group(1), '')
        else:
            original_url = image_url

        # å›¾ç‰‡åç§°æå–
        name_with_ext = original_url.split('/')[-1]
        name_without_params = name_with_ext.split('?')[0]
        image_name = os.path.splitext(name_without_params)[0]
        
        return original_url, image_name

    def parse_page(self, html_content, tag):
        """
        è§£æ HTML å†…å®¹ï¼Œæå–å›¾ç‰‡ä¿¡æ¯å¹¶ä¿å­˜ã€‚
        """
        if not html_content:
            return
            
        tree = etree.HTML(html_content)
        
        # ä½¿ç”¨ XPath æŸ¥æ‰¾æ‰€æœ‰ img.lazy å…ƒç´ 
        image_elements = tree.xpath('//div[@class="posts"]//img[@class="attachment-post-thumb size-post-thumb wp-post-image lazy"]')

        count = 0
        for img_el in image_elements:
            try:
                # æå–å›¾ç‰‡ URL
                image_url_with_res = img_el.get('data-src')
                if not image_url_with_res:
                    continue

                # æå–æ ‡é¢˜
                a_el = img_el.xpath('./ancestor::a[@class="featured-media"]')[0]
                title = a_el.get('title')
                
                # --- æ•°æ®æ¸…æ´—ä¸æå– ---
                original_url, image_name = self.clean_image_url(image_url_with_res)

                # --- å»é‡æ£€æŸ¥ ---
                if self.is_url_visited(original_url):
                    continue
                
                # --- ä¿å­˜æ•°æ® ---
                self.write_to_csv(title, image_name, original_url, self.csv_path, tag)
                count += 1

            except IndexError:
                print(f"[{tag}] [!] è­¦å‘Š: æ— æ³•è·å–å›¾ç‰‡çš„çˆ¶çº§ a å…ƒç´ æˆ–æ ‡é¢˜ï¼Œè·³è¿‡æ­¤å›¾ç‰‡ã€‚")
                continue
            except Exception as e:
                print(f"[{tag}] [âœ—] è§£ææ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        print(f"[{tag}] [âœ”] å®Œæˆè§£æï¼Œæ–°æ•°æ®å†™å…¥ {count} æ¡ã€‚")


    def scrape_tag(self, tag):
        """
        å¤„ç†å•ä¸ªæ ‡ç­¾çš„çˆ¬å–ä»»åŠ¡ã€‚
        
        å°†æ ‡ç­¾ä¸­çš„ç©ºæ ¼æ›¿æ¢ä¸º '+' è¿›è¡Œ URL ç¼–ç ã€‚
        """
        # --- [ä¿®æ”¹å¼€å§‹] ---
        # ä½¿ç”¨ str.replace() å°†æ ‡ç­¾ä¸­çš„ç©ºæ ¼æ›¿æ¢ä¸º '+'
        encoded_tag = tag.replace(' ', '+')
        url = BASE_URL.format(tag=encoded_tag)
        # --- [ä¿®æ”¹ç»“æŸ] ---
        
        print(f"[{tag}] å¼€å§‹çˆ¬å– URL: {url}")
        
        try:
            # å»ºè®®çš„è¯·æ±‚å¤´å’Œ SSL å¿½ç•¥è®¾ç½®
            response = requests.get(url, headers=self.headers, verify=False, timeout=15)
            response.raise_for_status()  # æ£€æŸ¥ HTTP é”™è¯¯
            
            self.parse_page(response.content, tag)

        except requests.exceptions.RequestException as e:
            print(f"[{tag}] [âœ—] è¯·æ±‚å‡ºé”™: {e}")
        except Exception as e:
            print(f"[{tag}] [âœ—] å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    def run_scraper(self):
        """
        ä¸»è¿è¡Œæ–¹æ³•ï¼šåŠ è½½æ ‡ç­¾å¹¶ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘çˆ¬å–ã€‚
        """
        tags = self.load_tags(TAG_FILE_PATH)
        if not tags:
            print("æ²¡æœ‰å¯ä¾›çˆ¬å–çš„æ ‡ç­¾ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
            return

        print(f"ğŸš€ å¼€å§‹ä½¿ç”¨ {MAX_WORKERS} ä¸ªçº¿ç¨‹å¹¶å‘çˆ¬å– {len(tags)} ä¸ªæ ‡ç­¾...")
        
        start_time = time.time()
        
        # ä½¿ç”¨ ThreadPoolExecutor å®ç°å¤šçº¿ç¨‹
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤æ‰€æœ‰æ ‡ç­¾çš„çˆ¬å–ä»»åŠ¡
            executor.map(self.scrape_tag, tags)
            
        end_time = time.time()
        
        print("\n==============================================")
        print(f"ğŸ‰ æ‰€æœ‰æ ‡ç­¾çˆ¬å–å®Œæˆã€‚")
        print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
        print(f"æ•°æ®å·²ä¿å­˜è‡³: {self.csv_path}")
        print("==============================================")


if __name__ == '__main__':
    scraper = LibreshotScraper(CSV_DIR_PATH, CSV_FILENAME)
    scraper.run_scraper()