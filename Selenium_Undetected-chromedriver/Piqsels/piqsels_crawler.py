import os
import csv
import time
import redis
import urllib3
import undetected_chromedriver as uc 
from threading import Lock
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random 

# --- é…ç½®å¸¸é‡ ---
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_KEY = 'piqsels_image_url_set' 
TAG_FILE_PATH = r"D:\myproject\Code\çˆ¬è™«\çˆ¬è™«æ•°æ®\piqsels\ram_tag_list_å¤‡ä»½.txt"
CSV_DIR_PATH = r"D:\myproject\Code\çˆ¬è™«\çˆ¬è™«æ•°æ®\piqsels"
CSV_FILENAME = "piqsels_data_13.csv"
BASE_URL_TEMPLATE = "https://www.piqsels.com/en/search?q={tag}&page={page}"

# !! æ‚¨çš„è‡ªå®šä¹‰é©±åŠ¨è·¯å¾„ !!
CUSTOM_DRIVER_PATH = r"D:\myproject\chromedriver-win64\chromedriver.exe"

# --- æ”¹è¿›é…ç½®ï¼šå»¶é•¿é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ (ä»é»˜è®¤ 120 ç§’å¢åŠ åˆ° 300 ç§’ï¼Œè§£å†³ ReadTimeoutError) ---
CUSTOM_PAGE_LOAD_TIMEOUT = 300 

class PiqselsImageCrawler:
    """
    é’ˆå¯¹ piqsels.com çš„å•çº¿ç¨‹ä¸²è¡Œçˆ¬è™«ï¼Œé›†æˆ Selenium å¤„ç†åçˆ¬éªŒè¯ï¼Œ
    å¹¶ä½¿ç”¨ Redis/å†…å­˜è¿›è¡Œå»é‡ã€‚
    """

    def __init__(self, csv_dir_path, csv_filename, driver_path, redis_host=REDIS_HOST, redis_port=REDIS_PORT):
        """
        åˆå§‹åŒ–çˆ¬è™«å®ä¾‹ï¼Œé›†æˆ Redis/å†…å­˜å»é‡é€»è¾‘å’Œ Selenium é©±åŠ¨åˆå§‹åŒ–ã€‚
        """
        self.csv_dir_path = csv_dir_path
        self.csv_path = os.path.join(self.csv_dir_path, csv_filename) 
        self.csv_lock = Lock() 
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7'
        }
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # --- Redis/å†…å­˜ å»é‡åˆå§‹åŒ–é€»è¾‘ ---
        self.redis = None
        self.visited_urls = set()
        self._init_deduplication(redis_host, redis_port)

        # --- Selenium åˆå§‹åŒ– (ä½¿ç”¨æŒ‡å®šè·¯å¾„) ---
        self.driver = None
        print("â³ æ­£åœ¨åˆå§‹åŒ– Selenium æµè§ˆå™¨...")
        try:
            # 1. åˆ›å»º Chrome Options
            options = uc.ChromeOptions()
            # è®¾ç½®é¡µé¢åŠ è½½ç­–ç•¥ä¸º eagerï¼ˆç­‰å¾…åŸºæœ¬çš„ DOM åŠ è½½å®Œæ¯•ï¼Œå¯èƒ½æ›´å¿«ï¼‰
            options.page_load_strategy = 'eager'
            
            self.driver = uc.Chrome(
                headless=False,
                use_subprocess=True,
                driver_executable_path=driver_path,
                options=options # ä¼ é€’ Options å¯¹è±¡
            ) 
            
            # 2. æ ¸å¿ƒæ”¹è¿›ï¼šè®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼Œä»¥é¿å… ReadTimeoutError
            self.driver.set_page_load_timeout(CUSTOM_PAGE_LOAD_TIMEOUT)
            
            print(f"âœ… Selenium æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸã€‚é¡µé¢åŠ è½½è¶…æ—¶å·²è®¾ç½®ä¸º {CUSTOM_PAGE_LOAD_TIMEOUT} ç§’ã€‚")

        except Exception as e:
            print(f"âŒ Selenium æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print("â— è¯·æ£€æŸ¥ï¼š1. æ‚¨çš„ Chrome æµè§ˆå™¨æ˜¯å¦å·²å®‰è£…ã€‚ 2. ChromeDriver ç‰ˆæœ¬æ˜¯å¦ä¸ Chrome æµè§ˆå™¨ç‰ˆæœ¬å…¼å®¹ã€‚ 3. æ‚¨å¯¹é©±åŠ¨æ–‡ä»¶æ˜¯å¦æœ‰æ‰§è¡Œæƒé™ ([Errno 13] Permission denied)ã€‚")
            self.driver = None


    def _init_deduplication(self, redis_host, redis_port):
        """åˆå§‹åŒ– Redis æˆ–å†…å­˜å»é‡æœºåˆ¶ã€‚"""
        try:
            self.redis = redis.StrictRedis(host=redis_host, port=redis_port, decode_responses=True)
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸï¼Œä½¿ç”¨ Redis é›†åˆè¿›è¡Œå»é‡ã€‚")
        except redis.exceptions.ConnectionError as e:
            print("âš ï¸ Redis è¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚") 
            self.redis = None
            self.visited_urls = set()
        except Exception as e:
            print(f"âš ï¸ Redis åˆå§‹åŒ–é‡åˆ°å…¶ä»–é”™è¯¯ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_urls = set()


    def _is_url_visited(self, url):
        """æ£€æŸ¥ URL æ˜¯å¦å·²è¢«è®¿é—®ï¼ˆå³å·²ä¿å­˜ï¼‰ã€‚"""
        unique_id = os.path.basename(url) 
        
        if self.redis:
            return not self.redis.sadd(REDIS_KEY, unique_id)
        else:
            if unique_id in self.visited_urls:
                return True
            self.visited_urls.add(unique_id)
            return False


    def write_to_csv(self, title, name, url, tag):
        """
        å†™å…¥ CSV æ–¹æ³•ï¼Œå·²é›†æˆçº¿ç¨‹é”ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨ã€‚
        """
        csv_path = self.csv_path
        try:
            with self.csv_lock: 
                os.makedirs(os.path.dirname(csv_path), exist_ok=True)
                is_file_empty = not os.path.exists(csv_path) or os.stat(csv_path).st_size == 0
                
                with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    
                    if is_file_empty:
                        writer.writerow(['Title', 'ImageName', 'URL', 'TAG'])
                        
                    writer.writerow([title, name, url, tag])
            print(f"[{tag}] [âœ“] æˆåŠŸå†™å…¥ CSV: {name}")
        except Exception as e:
            print(f"[{tag}] [âœ—] å†™å…¥ CSV å‡ºé”™: {e}")


    def _get_page_source(self, url, tag, page_num):
        """
        ä½¿ç”¨ Selenium é©±åŠ¨æµè§ˆå™¨ï¼Œå¤„ç† Cloudflare éªŒè¯å¹¶è·å–é¡µé¢æºä»£ç ã€‚
        è¿”å›ï¼šHTMLå†…å®¹ï¼ˆæˆåŠŸï¼‰ï¼ŒNoneï¼ˆå¯é‡è¯•å¤±è´¥ï¼Œå¦‚ç½‘ç»œè¶…æ—¶/404ï¼‰ï¼Œ""ï¼ˆä¸å¯é‡è¯•å¤±è´¥ï¼Œå¦‚ No resultsï¼‰
        """
        if not self.driver:
            return None 

        print(f"[{tag}] å°è¯•è®¿é—® URL: {url}")
        # self.driver.get() å— CUSTOM_PAGE_LOAD_TIMEOUT é™åˆ¶
        self.driver.get(url)

        try:
            # 1. ç­‰å¾… ID ä¸º "main" çš„ä¸»å†…å®¹åŒºåŸŸåŠ è½½å‡ºæ¥ (æœ€é•¿ 1200 ç§’ç”¨äºæ‰‹åŠ¨ Cloudflare)
            WebDriverWait(self.driver, 1200).until(
                EC.presence_of_element_located((By.ID, "main"))
            )
            
            # --- æ£€æŸ¥ â€œNo resultsâ€ å…ƒç´  (å¿«é€Ÿè·³è¿‡é€»è¾‘) ---
            try:
                # å°è¯•æ‰¾åˆ° <span class="notfound">No results</span> å…ƒç´ 
                no_results_element = self.driver.find_element(By.CLASS_NAME, "notfound")
                if no_results_element and no_results_element.text.strip().lower() == "no results":
                    # å‘ç° No resultsï¼Œæ ‡è®°ä¸ºä¸å¯é‡è¯•çš„å¤±è´¥
                    print(f"[{tag}] [Page {page_num}] âŒ æ£€æµ‹åˆ° 'No results'ï¼Œ**å¿«é€Ÿè·³è¿‡å½“å‰æ ‡ç­¾ï¼Œæ— éœ€é‡è¯•**ã€‚")
                    return "" # è¿”å› "" ä½œä¸ºç‰¹æ®Šæ ‡è®°
            except Exception:
                # æ‰¾ä¸åˆ°è¯¥å…ƒç´ æ˜¯æ­£å¸¸æƒ…å†µï¼ˆæœ‰ç»“æœï¼‰ï¼Œç»§ç»­æ‰§è¡Œ
                pass
            
            # 2. ç»§ç»­ç­‰å¾…å›¾ç‰‡åˆ—è¡¨ (é€šè¿‡ Cloudflare éªŒè¯å’Œå†…å®¹åŠ è½½çš„å…³é”®æ­¥éª¤)
            WebDriverWait(self.driver, 15).until( # ç¼©çŸ­åç»­ç­‰å¾…æ—¶é—´
                EC.presence_of_element_located((By.ID, "flow"))
            )
            
            print(f"[{tag}] âœ… é¡µé¢åŠ è½½æˆåŠŸæˆ–å·²é€šè¿‡éªŒè¯ã€‚")
            
            # 3. æ£€æŸ¥ 404
            page_text = self.driver.page_source
            if "page not found" in page_text.lower() or self.driver.title.lower().startswith("404"):
                print(f"[{tag}] âš ï¸ æ£€æµ‹åˆ°å¯èƒ½æ˜¯ 404 é¡µé¢ï¼Œåœæ­¢åˆ†é¡µã€‚")
                return None # æ ‡è®°ä¸ºå¤±è´¥ï¼Œäº¤ç”± start_crawl_for_tag å¤„ç†
            
            return self.driver.page_source

        except Exception as e:
            # é¡µé¢åŠ è½½è¶…æ—¶æˆ– Cloudflare éªŒè¯å¡ä½ï¼Œè¿”å› None å…è®¸é‡è¯•
            print(f"[{tag}] âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶æˆ–å‡ºç°å¼‚å¸¸ (Message: {e.msg if hasattr(e, 'msg') else e})ã€‚è¿”å› None å…è®¸é‡è¯•ã€‚")
            return None


    def _parse_page(self, html_content, tag):
        """
        è§£æ HTML å†…å®¹ï¼Œæå–å›¾ç‰‡ä¿¡æ¯ã€‚
        """
        if not html_content:
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
        ul_flow = soup.find('ul', id='flow')
        
        if not ul_flow:
            print(f"[{tag}] âš ï¸ é¡µé¢ä¸­æœªæ‰¾åˆ° ID ä¸º 'flow' çš„å›¾ç‰‡åˆ—è¡¨å®¹å™¨ã€‚")
            return []

        image_items = ul_flow.find_all('li', class_='item')
        
        if not image_items:
            print(f"[{tag}] âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡å…ƒç´ ã€‚")
            return [] 

        data_list = []
        for item in image_items:
            try:
                # 1. æå–å›¾ç‰‡æ ‡é¢˜ (Title) - img å…ƒç´ çš„ alt å±æ€§
                img_tag = item.find('img', class_='lazy')
                title = img_tag.get('alt', 'N/A').strip() if img_tag else 'N/A'

                # 2. æå– URL (1kå›¾url) - 'a' å…ƒç´ ä¸‹çš„ 'about' å±æ€§å€¼ï¼Œå»é™¤ '-thumbnail'
                license_a_tag = item.find('a', rel="license") 
                if license_a_tag:
                    about_url = license_a_tag.get('about')
                    if about_url and about_url.endswith('-thumbnail.jpg'):
                        full_url = about_url.replace('-thumbnail.jpg', '.jpg')
                        # 3. æå–å›¾ç‰‡åç§° (ImageName) - ä» URL å­—æ®µä¸­æå–
                        image_name = os.path.basename(full_url)
                    else:
                        continue 
                else:
                    continue 

                # 4. å»é‡æ£€æŸ¥
                if self._is_url_visited(full_url):
                    print(f"[{tag}] [~] URL å·²å­˜åœ¨ï¼Œè·³è¿‡: {image_name}")
                    continue

                data_list.append({
                    'title': title,
                    'name': image_name,
                    'url': full_url,
                    'tag': tag
                })

            except Exception as e:
                print(f"[{tag}] [âœ—] è§£æå•ä¸ªå›¾ç‰‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                continue

        return data_list


    def start_crawl_for_tag(self, tag):
        """
        é’ˆå¯¹å•ä¸ªæ ‡ç­¾ï¼Œæ‰§è¡Œä¸²è¡Œåˆ†é¡µçˆ¬å–ã€‚åœ¨åŠ è½½å¤±è´¥ã€æ— æ•°æ®æˆ– 404 æ—¶å¿«é€Ÿè·³è¿‡ã€‚
        """
        if not self.driver:
            print(f"[{tag}] [âœ—] çˆ¬è™«æ— æ³•å¯åŠ¨ï¼ŒSelenium é©±åŠ¨ç¼ºå¤±ã€‚")
            return

        print(f"\n--- âš¡ï¸ å¼€å§‹çˆ¬å–æ ‡ç­¾: {tag} ---")
        page = 1
        max_retries = 3 

        while True:
            url = BASE_URL_TEMPLATE.format(tag=tag, page=page)
            print(f"[{tag}] [Page {page}] æ­£åœ¨å¤„ç†...")
            
            html_content = None
            retry_count = 0
            
            # --- é¡µé¢åŠ è½½ä¸é‡è¯•é€»è¾‘ ---
            while retry_count < max_retries:
                # _get_page_source è¿”å› None (éœ€é‡è¯•/å¤±è´¥) æˆ– "" (No results/ä¸å¯é‡è¯•) æˆ– HTML
                html_content = self._get_page_source(url, tag, page) 
                
                # æ£€æŸ¥ç‰¹æ®Šæ ‡è®°ï¼š"" è¡¨ç¤º No results (ç”± _get_page_source å†…éƒ¨æ£€æµ‹)
                if html_content == "": 
                    retry_count = max_retries # æ ‡è®°ä¸ºä¸å¯é‡è¯•çš„å¤±è´¥ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    break 
                
                if html_content is not None:
                    break # æˆåŠŸè·å– HTML
                
                # å¦‚æœæ˜¯ Noneï¼Œåˆ™é‡è¯•
                print(f"[{tag}] [Page {page}] åŠ è½½å¤±è´¥ï¼Œé‡è¯• ({retry_count + 1}/{max_retries})...")
                retry_count += 1
                time.sleep(5) 

            # --- å¿«é€Ÿè·³è¿‡å’Œæœ€ç»ˆå¤±è´¥åˆ¤æ–­ ---
            if html_content is None or html_content == "": 
                # None: é‡è¯•åä»å¤±è´¥ (è¶…æ—¶/éªŒè¯æœªè¿‡/404)
                # "": No results (æ— éœ€é‡è¯•)
                print(f"[{tag}] [Page {page}] â— æ£€æµ‹åˆ° 404/åŠ è½½å¤±è´¥/No resultsï¼Œ**åœæ­¢åˆ†é¡µå¹¶è·³è¿‡å½“å‰æ ‡ç­¾**ã€‚")
                break # è·³å‡º while True å¾ªç¯ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ª tag

            # --- è§£ææ•°æ® ---
            image_data = self._parse_page(html_content, tag)

            if not image_data:
                # è§£æè¿”å›ç©ºåˆ—è¡¨ï¼ˆæœªæå–åˆ°ä»»ä½•æ–°æ•°æ® æˆ– é¡µé¢ä¸­æ— å›¾ç‰‡å…ƒç´ ï¼‰
                print(f"[{tag}] [Page {page}] â— **æœªè§£æåˆ°å›¾ç‰‡å…ƒç´ ï¼Œåœæ­¢åˆ†é¡µå¹¶è·³è¿‡å½“å‰æ ‡ç­¾**ã€‚")
                break # è·³å‡º while True å¾ªç¯ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ª tag
            
            # å†™å…¥ CSV
            for data in image_data:
                self.write_to_csv(data['title'], data['name'], data['url'], data['tag'])

            page += 1
            # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            time.sleep(2 + random.uniform(1, 3)) # éšæœºç­‰å¾… 3 åˆ° 5 ç§’


    def run(self):
        """
        ä¸»æ‰§è¡Œæ–¹æ³•ï¼šè¯»å–æ ‡ç­¾æ–‡ä»¶ï¼Œå¹¶å¯¹æ¯ä¸ªæ ‡ç­¾å¯åŠ¨çˆ¬å–ã€‚
        """
        if not self.driver:
            print("âŒ çˆ¬è™«æ— æ³•å¯åŠ¨ï¼Œè¯·è§£å†³ Selenium é©±åŠ¨åˆå§‹åŒ–é”™è¯¯ã€‚")
            return

        tag_list = []
        try:
            with open(TAG_FILE_PATH, 'r', encoding='utf-8') as f:
                tag_list = [line.strip() for line in f if line.strip()]
            print(f"âœ… æˆåŠŸè¯»å– {len(tag_list)} ä¸ªæ ‡ç­¾ã€‚")
        except FileNotFoundError:
            print(f"âŒ æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°: {TAG_FILE_PATH}")
            return

        for tag in tag_list:
            self.start_crawl_for_tag(tag)
        
        if self.driver:
            self.driver.quit()
            print("\nğŸ‰ æ‰€æœ‰æ ‡ç­¾çˆ¬å–å®Œæˆï¼Œæµè§ˆå™¨å·²å…³é—­ã€‚")

if __name__ == '__main__':
    crawler = PiqselsImageCrawler(
        csv_dir_path=CSV_DIR_PATH, 
        csv_filename=CSV_FILENAME,
        driver_path=CUSTOM_DRIVER_PATH # ä¼ é€’æœ¬åœ°é©±åŠ¨è·¯å¾„
    )
    crawler.run()