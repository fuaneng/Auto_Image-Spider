# -*- coding: utf-8 -*-
import os
import csv
import time
import requests
import redis
import re
from threading import Lock
from urllib.parse import urljoin
import urllib3
from bs4 import BeautifulSoup, Tag
import undetected_chromedriver as uc
from selenium.common.exceptions import WebDriverException, TimeoutException
from concurrent.futures import ThreadPoolExecutor
from queue import Queue
from typing import Optional, Set, Dict, List, Tuple

# --- é…ç½®å¸¸é‡ ---
BASE_URL = 'https://en.girlstop.info/'
MODEL_NAME_FILE = r'R:\py\Auto_Image-Spider\Selenium_Undetected-chromedriver\en_girlstop_info\model_name.txt'
CSV_DIR_PATH = r'R:\py\Auto_Image-Spider\Selenium_Undetected-chromedriver\en_girlstop_info'
LOGS_SUBDIR = 'csv_logs'

ENABLE_DOWNLOAD = False
MAX_DOWNLOAD_WORKERS = 10

CUSTOM_BROWSER_PATH = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
CHROMEDRIVER_PATH = r"C:\Program Files\Google\chromedriver-win64\chromedriver.exe"

REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_KEY = 'girlstop_image_url_set'

CHROME_MAIN_VERSION = 142

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class GirlstopSpider:
    """
    çˆ¬å– en.girlstop.info ç½‘ç«™æ¨¡ç‰¹ä½œå“é›†å’Œå›¾ç‰‡ URL çš„çˆ¬è™«ã€‚
    """

    def __init__(self, csv_dir_path: str, custom_browser_path: str, redis_host: str = REDIS_HOST, redis_port: int = REDIS_PORT):
        self.csv_dir_path = csv_dir_path
        self.csv_log_path = os.path.join(self.csv_dir_path, LOGS_SUBDIR)

        self.csv_lock = Lock()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': BASE_URL
        }

        self.download_queue: Optional[Queue] = None
        self.executor: Optional[ThreadPoolExecutor] = None
        if ENABLE_DOWNLOAD:
            self.download_queue = Queue()
            self.executor = ThreadPoolExecutor(max_workers=MAX_DOWNLOAD_WORKERS)
            print(f"âœ… å¯ç”¨å¼‚æ­¥ä¸‹è½½ï¼Œæœ€å¤§çº¿ç¨‹æ•°: {MAX_DOWNLOAD_WORKERS}")
        else:
            print("âš ï¸ æœªå¯ç”¨å›¾ç‰‡ä¸‹è½½åŠŸèƒ½ã€‚")

        print(f"ğŸ¤– åˆå§‹åŒ– undetected-chromedriver é©±åŠ¨ (æœŸæœ›ä¸»ç‰ˆæœ¬: {CHROME_MAIN_VERSION})...")
        self.driver: Optional[uc.Chrome] = None
        try:
            options = uc.ChromeOptions()
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')

            # æ³¨æ„ï¼šå¦‚æœ Cloudflare éªŒè¯é¢‘ç¹ï¼Œå»ºè®®å…ˆæ³¨é‡Šä¸‹ä¸€è¡Œä»¥ä¾¿æ‰‹åŠ¨é€šè¿‡éªŒè¯
            options.add_argument('--headless=new')

            # ä½¿ç”¨ä¸´æ—¶ profile ç›®å½•
            options.add_argument(f'--user-data-dir={os.path.join(os.getcwd(), "temp_chrome_profile")}')

            # å°è¯•ä¸åŒçš„æ„é€ ç­¾åä»¥å…¼å®¹ä¸åŒ uc ç‰ˆæœ¬
            got_driver = False
            # æ–¹å¼1: é€‚ç”¨äºè¾ƒæ–°ç‰ˆæœ¬ï¼ˆversion_main + browser_executable_pathï¼‰
            try:
                self.driver = uc.Chrome(options=options,
                                        version_main=CHROME_MAIN_VERSION,
                                        browser_executable_path=custom_browser_path)
                got_driver = True
            except TypeError:
                # å‚æ•°ç­¾åå¯èƒ½ä¸è¢«æ”¯æŒï¼Œç»§ç»­å°è¯•å…¶ä»–ç­¾å
                self.driver = None
            except Exception as e:
                # æŸäº›ç‰ˆæœ¬ä¼šæŠ›å‡ºå…¶å®ƒå¼‚å¸¸ï¼Œæˆ‘ä»¬è®°ä¸‹æ¥å¹¶å°è¯•å¤‡é€‰
                print(f"  -> å°è¯•ç¬¬ä¸€ç§åˆå§‹åŒ–æ–¹å¼å¤±è´¥: {e}")

            if not got_driver:
                # æ–¹å¼2: å°è¯•ä¼ å…¥ executable_path / driver_executable_path
                try:
                    # å…ˆè¯• driver_executable_path
                    self.driver = uc.Chrome(options=options,
                                            version_main=CHROME_MAIN_VERSION,
                                            driver_executable_path=CHROMEDRIVER_PATH,
                                            browser_executable_path=custom_browser_path)
                    got_driver = True
                except Exception as e:
                    print(f"  -> å°è¯•å¸¦ driver_executable_path åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.driver = None

            if not got_driver:
                try:
                    # æ–¹å¼3: æŸäº›è€ç‰ˆæœ¬ä½¿ç”¨ executable_path ç›´æ¥ä½ç½®å‚æ•°
                    self.driver = uc.Chrome(executable_path=CHROMEDRIVER_PATH, options=options)
                    got_driver = True
                except Exception as e:
                    print(f"  -> å°è¯• executable_path åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.driver = None

            if not got_driver or not self.driver:
                raise RuntimeError("æ— æ³•é€šè¿‡ä»»ä½•å·²çŸ¥æ–¹å¼åˆå§‹åŒ– undetected-chromedriverï¼Œè¯·æ£€æŸ¥ chromedriver ç‰ˆæœ¬ã€è·¯å¾„å’Œ uc ç‰ˆæœ¬ã€‚")

            self.driver.set_page_load_timeout(60)
            print("âœ… undetected-chromedriver é©±åŠ¨åˆå§‹åŒ–æˆåŠŸ (å¯èƒ½ä¸ºæ— å¤´æ¨¡å¼)ã€‚")
        except Exception as e:
            print(f"âŒ uc é©±åŠ¨åˆå§‹åŒ–å¤±è´¥ã€‚è¯·æ£€æŸ¥é©±åŠ¨è·¯å¾„ ({CHROMEDRIVER_PATH}) æˆ–æµè§ˆå™¨è·¯å¾„/ç‰ˆæœ¬: {e}")
            print("   å»ºè®®ï¼šä¸´æ—¶æ³¨é‡Šæ‰ --headless=new ä»¥æ‰‹å·¥é€šè¿‡ Cloudflare éªŒè¯åå†æ¢å¤æ— å¤´æ¨¡å¼ã€‚")
            self.driver = None
            # æ³¨æ„ï¼šä¸è¦ç›´æ¥ returnï¼Œè¿™æ ·è°ƒç”¨è€…å¯ä»¥åˆ¤æ–­ driver æ˜¯å¦å­˜åœ¨
        # å»é‡
        self.redis: Optional[redis.StrictRedis] = None
        self.visited_urls: Set[str] = set()
        try:
            self.redis = redis.StrictRedis(host=redis_host, port=redis_port, decode_responses=True)
            self.redis.ping()
            print(f"âœ… Redis è¿æ¥æˆåŠŸï¼Œä½¿ç”¨ Redis é›†åˆ ({REDIS_KEY}) è¿›è¡Œå»é‡ã€‚")
        except redis.exceptions.ConnectionError as e:
            print(f"âš ï¸ Redis è¿æ¥å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
        except Exception as e:
            print(f"âš ï¸ Redis åˆå§‹åŒ–é‡åˆ°å…¶ä»–é”™è¯¯ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None

        # Ensure CSV log dir exists
        try:
            os.makedirs(self.csv_log_path, exist_ok=True)
            print(f"âœ… CSV æ—¥å¿—ç›®å½•å‡†å¤‡å°±ç»ª: {self.csv_log_path}")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åˆ›å»º CSV æ—¥å¿—ç›®å½• {self.csv_log_path}: {e}")

    @staticmethod
    def _sanitize_filename(filename: str) -> str:
        safe_filename = re.sub(r'[\\/:*?"<>|]', '_', filename)
        safe_filename = safe_filename.strip()
        return safe_filename[:150]

    def download_worker(self, url: str, title: str, model_name: str) -> bool:
        safe_model_name = self._sanitize_filename(model_name)
        safe_title = self._sanitize_filename(title)
        save_dir = os.path.join(self.csv_dir_path, safe_model_name, safe_title)
        image_name = url.split('/')[-1]
        if '?' in image_name:
            image_name = image_name.split('?')[0]
        if not image_name:
            image_name = "default_image.jpg"
        save_path = os.path.join(save_dir, image_name)

        if os.path.exists(save_path):
            return True

        try:
            os.makedirs(save_dir, exist_ok=True)
        except Exception as e:
            print(f"[{model_name}] [âœ—] åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥ {save_dir}: {e}")
            return False

        try:
            response = requests.get(url, headers=self.headers, stream=True, verify=False, timeout=20)
            response.raise_for_status()
            with self.csv_lock:
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
            print(f"[{model_name}] [âœ“] ä¸‹è½½æˆåŠŸ: {image_name} -> {os.path.join(safe_model_name, safe_title)}")
            return True
        except requests.exceptions.RequestException as e:
            print(f"[{model_name}] [âœ—] ä¸‹è½½å¤±è´¥: {url}, é”™è¯¯: {e}")
            return False

    def is_url_visited(self, url: str) -> bool:
        """
        æ£€æŸ¥ URL æ˜¯å¦å·²è¢«å¤„ç†è¿‡ã€‚è¿”å› True è¡¨ç¤ºå·²è®¿é—®ï¼ˆä¸åº”å†æ¬¡å†™å…¥ï¼‰ã€‚
        """
        if self.redis:
            try:
                if self.redis.sismember(REDIS_KEY, url):
                    return True
                # ä¸å­˜åœ¨åˆ™åŠ å…¥é›†åˆå¹¶è¿”å› Falseï¼ˆè¡¨ç¤ºæœªè®¿é—®ï¼‰
                self.redis.sadd(REDIS_KEY, url)
                return False
            except Exception as e:
                print(f"âš ï¸ Redis å»é‡å¼‚å¸¸ ({e})ï¼Œå›é€€åˆ°å†…å­˜å»é‡ã€‚")
                self.redis = None  # å›é€€
        # å†…å­˜å»é‡
        if url in self.visited_urls:
            return True
        self.visited_urls.add(url)
        return False

    def write_to_csv(self, title: str, url: str, model_name: str):
        """å°†æ•°æ®å†™å…¥ä»¥ model_name å‘½åçš„ç‹¬ç«‹ CSV æ–‡ä»¶ï¼Œå¹¶å°†ä¸‹è½½ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—ã€‚"""
        if ENABLE_DOWNLOAD and self.download_queue:
            self.download_queue.put((url, title, model_name))

        safe_model_name = self._sanitize_filename(model_name)
        csv_filename = f"{safe_model_name}_results.csv"
        model_csv_path = os.path.join(self.csv_log_path, csv_filename)

        # ä¿è¯ url æ˜¯ç»å¯¹ URLï¼ˆæ–¹ä¾¿åç»­ä½¿ç”¨ï¼‰
        full_url = urljoin(BASE_URL, url)

        name = full_url.split('/')[-1]
        if '?' in name:
            name = name.split('?')[0]

        tag = model_name

        try:
            with self.csv_lock:
                os.makedirs(os.path.dirname(model_csv_path), exist_ok=True)
                is_file_empty = not os.path.exists(model_csv_path) or os.stat(model_csv_path).st_size == 0
                with open(model_csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    if is_file_empty:
                        writer.writerow(['Title', 'ImageName', 'URL', 'model_name'])
                    writer.writerow([title, name, full_url, tag])
            # æ€»æ˜¯æ‰“å°å†™å…¥ç¡®è®¤ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰
            print(f"[{model_name}] [âœ“] å†™å…¥ CSV: {title} -> {model_csv_path}")
        except Exception as e:
            print(f"[{model_name}] [âœ—] å†™å…¥ CSV å‡ºé”™: {e}")

    def _safe_get_url(self, url: str) -> Optional[str]:
        """
        ä½¿ç”¨ uc é©±åŠ¨å®‰å…¨åœ°è®¿é—® URLã€‚
        âœ… æ™ºèƒ½ç­‰å¾…é€»è¾‘ï¼šé¦–æ¬¡è®¿é—®ç­‰å¾… 30 ç§’ï¼Œä¹‹åä»…ç­‰å¾… 5 ç§’ã€‚
        """
        if not self.driver:
            print("âš ï¸ driver æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä½¿ç”¨ Selenium è·å–é¡µé¢ã€‚")
            return None

        # åˆ¤æ–­æ˜¯å¦ç¬¬ä¸€æ¬¡è®¿é—®ç½‘ç«™ï¼ˆå¯æ ¹æ® self._has_waited æ ‡å¿—ï¼‰
        if not hasattr(self, "_has_waited"):
            self._has_waited = False

        try:
            self.driver.get(url)

            # æ™ºèƒ½ç­‰å¾…é€»è¾‘
            if not self._has_waited:
                wait_time = 30
                self._has_waited = True
                print(f"   -> é¦–æ¬¡è®¿é—®ï¼Œç­‰å¾…é¡µé¢åŠ è½½å’Œåçˆ¬æ£€æŸ¥ ({wait_time} ç§’)...")
            else:
                wait_time = 5
                print(f"   -> é¡µé¢åŠ è½½ä¸­ (ç­‰å¾… {wait_time} ç§’)...")

            time.sleep(wait_time)

            page_title = self.driver.title or ""
            if 'just a moment' in page_title or 'Cloudflare' in page_title or 'Verify you are human' in page_title:
                print("==========================================================")
                print("âš ï¸ æ£€æµ‹åˆ° Cloudflare éªŒè¯é¡µé¢ï¼")
                print("ã€å»ºè®®ã€‘æ³¨é‡Šæ‰ `--headless=new` æ‰‹åŠ¨é€šè¿‡ä¸€æ¬¡éªŒè¯åå†é‡æ–°è¿è¡Œã€‚")
                print("==========================================================")
                time.sleep(5)

            return self.driver.page_source

        except TimeoutException:
            print(f"[{url}] é¡µé¢åŠ è½½è¶…æ—¶ã€‚")
            return None
        except WebDriverException as e:
            print(f"[{url}] è®¿é—®å‡ºé”™: {e}")
            return None


    def extract_details_from_page(self, detail_url: str, model_name: str, title: str):
        full_detail_url = urljoin(BASE_URL, detail_url)
        html_content = self._safe_get_url(full_detail_url)
        if not html_content:
            print(f"[{model_name}] æ— æ³•è·å–è¯¦æƒ…é¡µå†…å®¹: {full_detail_url}")
            return
        soup = BeautifulSoup(html_content, 'html.parser')
        image_link_tags = soup.find_all('a', class_='fullimg')
        if not image_link_tags:
            print(f"[{model_name}] [âœ—] æœªèƒ½åœ¨è¯¦æƒ…é¡µæ‰¾åˆ°ä»»ä½•å›¾ç‰‡é“¾æ¥: {full_detail_url}")
            return
        print(f"[{model_name}] æ‰¾åˆ° {len(image_link_tags)} å¼ å›¾ç‰‡é“¾æ¥ã€‚å¼€å§‹è®°å½•...")
        for link_tag in image_link_tags:
            if 'href' in link_tag.attrs:
                image_url = link_tag['href']
                # ç»Ÿä¸€è½¬ä¸ºç»å¯¹ URL
                image_url_abs = urljoin(full_detail_url, image_url)
                if not self.is_url_visited(image_url_abs):
                    self.write_to_csv(title, image_url_abs, model_name)
                else:
                    # å·²è®¿é—®åˆ™è·³è¿‡
                    pass
            else:
                print(f"[{model_name}] [âœ—] å‘ç°ä¸€ä¸ªç¼ºå°‘ 'href' å±æ€§çš„é“¾æ¥æ ‡ç­¾ã€‚")

    def scrape_model_page(self, model_name: str):
        encoded_model_name = model_name.replace(' ', '+')
        model_url = f"{BASE_URL}models.php?name={encoded_model_name}"
        print(f"\n======== å¼€å§‹çˆ¬å–æ¨¡ç‰¹: {model_name} ========")
        print(f"è®¿é—® URL: {model_url}")

        html_content = self._safe_get_url(model_url)
        if not html_content:
            print(f"[{model_name}] æ— æ³•è·å–é¡µé¢å†…å®¹ï¼Œè·³è¿‡ã€‚")
            return

        soup = BeautifulSoup(html_content, 'html.parser')
        detail_link_tags = soup.select('a[href^="/psto.php?id="], a[href^="psto.php?id="]')

        unique_posts: Dict[str, str] = {}
        for link_tag in detail_link_tags:
            relative_url = link_tag.get('href')
            if not relative_url:
                continue
            current_tag: Optional[Tag] = link_tag
            thumb_div: Optional[Tag] = None
            while current_tag and current_tag.name != 'body':
                current_tag = current_tag.find_parent()
                if current_tag and current_tag.name == 'div' and 'thumb' in current_tag.get('class', []):
                    thumb_div = current_tag
                    break
            title_tag = None
            if thumb_div:
                title_tag = thumb_div.select_one('strong.post_title a')
            title = 'Unknown Title'
            if title_tag:
                title = title_tag.get_text(strip=True)
            elif link_tag.get_text(strip=True):
                title = link_tag.get_text(strip=True)
            # æ”¾å®½æ¡ä»¶ï¼šå³ä¾¿æ ‡é¢˜ä¸º Unknown ä¹Ÿè®°å½•
            if relative_url not in unique_posts:
                unique_posts[relative_url] = title

        if not unique_posts:
            if self.driver and 'just a moment' not in (self.driver.title or "") and 'Cloudflare' not in (self.driver.title or ""):
                print(f"[{model_name}] [âœ—] æœªæ‰¾åˆ°ä»»ä½•ä½œå“é›†é“¾æ¥ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–æˆ–åŠ è½½å¤±è´¥ã€‚")
            return

        print(f"[{model_name}] æ‰¾åˆ° {len(unique_posts)} ä¸ªä½œå“é›†ã€‚")
        for relative_url, title in unique_posts.items():
            if relative_url:
                self.extract_details_from_page(relative_url, model_name, title)
            else:
                print(f"[{model_name}] [âœ—] å‘ç°æ— æ•ˆä½œå“é“¾æ¥/æ ‡é¢˜ã€‚")

    def run(self, model_names_file: str):
        if not self.driver:
            print("âŒ é©±åŠ¨æœªæˆåŠŸåˆå§‹åŒ–ï¼Œç¨‹åºé€€å‡ºã€‚è¯·å‚è€ƒä¸Šæ–¹åˆå§‹åŒ–é”™è¯¯ä¿¡æ¯ã€‚")
            return

        model_names: List[str] = []
        try:
            with open(model_names_file, 'r', encoding='utf-8') as f:
                model_names = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"âŒ æ¨¡ç‰¹åç§°æ–‡ä»¶æœªæ‰¾åˆ°: {model_names_file}")
            return
        except UnicodeDecodeError:
            print(f"âŒ è¯»å–æ–‡ä»¶æ—¶é‡åˆ°ç¼–ç é”™è¯¯ï¼Œè¯·ç¡®ä¿ {model_names_file} æ˜¯ UTF-8 ç¼–ç ã€‚")
            return

        if not model_names:
            print("âŒ æ¨¡ç‰¹åç§°æ–‡ä»¶ä¸ºç©ºã€‚")
            return

        print(f"\næ€»å…±éœ€è¦å¤„ç† {len(model_names)} ä½æ¨¡ç‰¹ã€‚")

        for model_name in model_names:
            self.scrape_model_page(model_name)

        try:
            self.driver.quit()
        except Exception:
            pass
        print("\n==================================================")
        print("âœ… ç½‘é¡µçˆ¬å–å’Œæ•°æ®è®°å½•é˜¶æ®µå®Œæˆã€‚")

        if ENABLE_DOWNLOAD and self.download_queue and self.executor and self.download_queue.qsize() > 0:
            total_tasks = self.download_queue.qsize()
            print(f"ğŸš€ å¼€å§‹å¼‚æ­¥ä¸‹è½½ï¼Œæ€»ä»»åŠ¡æ•°: {total_tasks}...")
            futures = []
            while not self.download_queue.empty():
                task = self.download_queue.get()
                future = self.executor.submit(self.download_worker, *task)
                futures.append(future)
            for i, future in enumerate(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
                if (i + 1) % 50 == 0 or (i + 1) == total_tasks:
                    print(f"   ä¸‹è½½è¿›åº¦: {i + 1}/{total_tasks} ({((i + 1) / total_tasks) * 100:.2f}%)")
            self.executor.shutdown(wait=True)
            print("ğŸ‰ æ‰€æœ‰å¼‚æ­¥ä¸‹è½½ä»»åŠ¡å®Œæˆã€‚")
        elif ENABLE_DOWNLOAD:
            print("âš ï¸ æœªå‘ç°æ–°çš„ä¸‹è½½ä»»åŠ¡ã€‚")

        print("==================================================")
        print("ç¨‹åºå…¨éƒ¨æ‰§è¡Œå®Œæ¯•ã€‚")


if __name__ == '__main__':
    # ç¡®ä¿å›¾ç‰‡ä¸‹è½½æ ¹ç›®å½•å­˜åœ¨
    try:
        os.makedirs(CSV_DIR_PATH, exist_ok=True)
        os.makedirs(os.path.join(CSV_DIR_PATH, LOGS_SUBDIR), exist_ok=True)
    except Exception as e:
        print(f"âŒ æ— æ³•åˆ›å»ºç›®å½• {CSV_DIR_PATH} æˆ–å­ç›®å½•: {e}")

    spider = GirlstopSpider(
        csv_dir_path=CSV_DIR_PATH,
        custom_browser_path=CUSTOM_BROWSER_PATH
    )

    if spider.driver:
        spider.run(MODEL_NAME_FILE)
    else:
        print("ç¨‹åºæœªå¯åŠ¨ï¼ˆdriver æœªåˆå§‹åŒ–ï¼‰ã€‚è¯·æ£€æŸ¥ä¸Šæ–¹é”™è¯¯ä¿¡æ¯å¹¶ä¿®æ­£ã€‚")
