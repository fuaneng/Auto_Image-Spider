import os
import csv
import re
import requests
import urllib3
from threading import Lock
from concurrent.futures import ThreadPoolExecutor 
from typing import List, Optional, Set, Tuple

# --- é…ç½®å¸¸é‡ ---
# ã€å¿…é¡»ä¿®æ”¹ã€‘CSV æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•
CSV_LOGS_DIR = r'R:\py\Auto_Image-Spider\Selenium_Undetected-chromedriver\en_girlstop_info\csv_logs'

# ã€å¿…é¡»ä¿®æ”¹ã€‘å›¾ç‰‡ä¸‹è½½çš„æ ¹ç›®å½• (è¿™å°†æ˜¯ [DOWNLOAD_ROOT]/[Model_Name]/[Title]/ çš„æ ¹ç›®å½•)
DOWNLOAD_ROOT = r'R:\py\Auto_Image-Spider\Selenium_Undetected-chromedriver\en_girlstop_info\models' 

# ã€å¯é€‰é…ç½®ã€‘å¦‚æœåªæƒ³ä¸‹è½½ç‰¹å®š CSV æ–‡ä»¶ä¸­çš„å›¾ç‰‡ï¼Œè¯·åœ¨è¿™é‡Œå¡«å†™æ–‡ä»¶ååˆ—è¡¨ï¼ˆåŒ…å« .csv åç¼€ï¼‰
# æ ¼å¼ï¼š['Mila-A_results.csv', 'ModelB_results.csv']ã€‚å¦‚æœç•™ç©ºï¼ˆ[]ï¼‰ï¼Œåˆ™ä¸‹è½½æ‰€æœ‰æ–‡ä»¶ã€‚
TARGET_CSV_FILENAMES: List[str] = ['Susann-A_results.csv','Serena-J_results.csv'] 

# ã€å¯é€‰é…ç½®ã€‘æ¯ä¸ª CSV æ–‡ä»¶é™åˆ¶å¤„ç†çš„ç›¸å†Œæ•°é‡ï¼ˆTitleï¼‰ã€‚
# 5 è¡¨ç¤ºé™åˆ¶å‰ 5 ä¸ªç›¸å†Œã€‚
# 0 æˆ–è´Ÿæ•°è¡¨ç¤ºä¸é™åˆ¶ï¼Œä¸‹è½½æ‰€æœ‰ç›¸å†Œã€‚
ALBUM_LIMIT_PER_CSV = 10 

# ã€é…ç½®ã€‘ä¸‹è½½çº¿ç¨‹æ•°
MAX_DOWNLOAD_WORKERS = 10 

# å¿½ç•¥ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å…¨å±€çº¿ç¨‹é”ï¼Œç”¨äºæ–‡ä»¶ç³»ç»Ÿæ“ä½œ
file_lock = Lock()

def _sanitize_filename(filename: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶åæˆ–æ–‡ä»¶å¤¹åï¼Œç§»é™¤ä¸å®‰å…¨çš„å­—ç¬¦ï¼Œå¹¶é™åˆ¶é•¿åº¦ã€‚
    """
    # ç§»é™¤ Windows/Linux æ–‡ä»¶åä¸­ä¸å…è®¸çš„å­—ç¬¦
    safe_filename = re.sub(r'[\\/:*?"<>|]', '_', filename)
    safe_filename = safe_filename.strip()
    return safe_filename[:150] 

def download_worker(url: str, title: str, model_name: str) -> bool:
    """
    æ‰§è¡Œå•ä¸ªæ–‡ä»¶çš„ä¸‹è½½ä»»åŠ¡ï¼Œå¹¶æ ¹æ® model_name å’Œ title è¿›è¡Œåˆ†ç±»å­˜å‚¨ã€‚
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Referer': url # ä½¿ç”¨å›¾ç‰‡ URL ä½œä¸º Referer
    }
    
    # 1. æ„å»ºå­˜å‚¨è·¯å¾„
    safe_model_name = _sanitize_filename(model_name)
    safe_title = _sanitize_filename(title)
    
    # å­˜å‚¨è·¯å¾„ï¼š[DOWNLOAD_ROOT]/[model_name]/[title]/
    save_dir = os.path.join(DOWNLOAD_ROOT, safe_model_name, safe_title)
    
    # ä» URL ä¸­æå–æ–‡ä»¶å
    image_name = url.split('/')[-1]
    if '?' in image_name:
         image_name = image_name.split('?')[0] 
    if not image_name:
        image_name = "default_image.jpg"
        
    save_path = os.path.join(save_dir, image_name)

    if os.path.exists(save_path):
        print(f"[{model_name}] [âœ“] æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {image_name}")
        return True 
    
    # 2. ç¡®ä¿ç›®æ ‡æ–‡ä»¶å¤¹å­˜åœ¨
    try:
        os.makedirs(save_dir, exist_ok=True)
    except Exception as e:
        print(f"[{model_name}] [âœ—] åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥ {save_dir}: {e}")
        return False

    # 3. ä¸‹è½½æ–‡ä»¶
    try:
        response = requests.get(url, headers=headers, stream=True, verify=False, timeout=20)
        response.raise_for_status() 
        
        # å†™å…¥æ–‡ä»¶
        with file_lock: # ä½¿ç”¨é”ï¼Œç¡®ä¿æ–‡ä»¶ç³»ç»Ÿæ“ä½œäº’æ–¥
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    
        print(f"[{model_name}] [âœ“] ä¸‹è½½æˆåŠŸ: {image_name} -> {os.path.join(safe_model_name, safe_title)}")
        return True

    except requests.exceptions.RequestException as e:
        print(f"[{model_name}] [âœ—] ä¸‹è½½å¤±è´¥: {url}, é”™è¯¯: {e}")
        return False

def run_downloader():
    """ä¸»ç¨‹åºé€»è¾‘ï¼šè¯»å–é€‰å®š/æ‰€æœ‰ CSV æ–‡ä»¶ä¸­çš„ç›¸å†Œä»»åŠ¡ï¼Œå¹¶å¯åŠ¨å¼‚æ­¥ä¸‹è½½ã€‚"""
    
    print(f"ğŸš€ å¼€å§‹å¼‚æ­¥ä¸‹è½½ï¼Œæœ€å¤§çº¿ç¨‹æ•°: {MAX_DOWNLOAD_WORKERS}...")
    
    # æ£€æŸ¥ç›¸å†Œé™åˆ¶ï¼Œå¦‚æœ <= 0 åˆ™è®¾ç½®ä¸ºä¸é™åˆ¶ (None)
    album_limit = ALBUM_LIMIT_PER_CSV if ALBUM_LIMIT_PER_CSV > 0 else None
    
    all_tasks: List[Tuple[str, str, str]] = []
    
    # è·å–ç›®æ ‡æ–‡ä»¶åé›†åˆï¼Œå¦‚æœä¸ºç©ºåˆ™å¤„ç†æ‰€æœ‰æ–‡ä»¶
    target_filenames: Optional[Set[str]] = set(TARGET_CSV_FILENAMES) if TARGET_CSV_FILENAMES else None
    
    if target_filenames:
        print(f"   -> å·²æŒ‡å®šç›®æ ‡ CSV æ–‡ä»¶æ•°é‡: {len(target_filenames)}")
    else:
        print("   -> æ¨¡å¼: ä¸‹è½½æ‰€æœ‰æ¨¡ç‰¹çš„ä»»åŠ¡ã€‚")

    if album_limit:
        print(f"   -> é™åˆ¶æ¨¡å¼: æ¯ä¸ªæ–‡ä»¶åªä¸‹è½½å‰ {album_limit} ä¸ªç›¸å†Œã€‚")
    else:
        print("   -> é™åˆ¶æ¨¡å¼: ä¸é™åˆ¶ç›¸å†Œæ•°é‡ã€‚")


    # 1. éå† CSV ç›®å½•ï¼Œè¯»å–æ‰€æœ‰ä»»åŠ¡
    for filename in os.listdir(CSV_LOGS_DIR):
        if filename.endswith('_results.csv'):
            
            # è¿‡æ»¤é€»è¾‘
            if target_filenames and filename not in target_filenames:
                continue
            
            csv_path = os.path.join(CSV_LOGS_DIR, filename)
            print(f"   -> è¯»å–ä»»åŠ¡æ–‡ä»¶: {filename}")
            
            processed_albums: Set[str] = set()
            
            try:
                with open(csv_path, 'r', newline='', encoding='utf-8-sig') as f:
                    reader = csv.reader(f)
                    try:
                        next(reader) # è·³è¿‡è¡¨å¤´
                    except StopIteration:
                        continue
                        
                    for row in reader:
                        # CSV åˆ—ç»“æ„: ['Title', 'ImageName', 'URL', 'model_name']
                        if len(row) >= 4:
                            title, _, url, model_name = row
                            
                            # ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ã€‘
                            # 1. åˆ¤æ–­å½“å‰ç›¸å†Œæ ‡é¢˜æ˜¯å¦å·²åœ¨é›†åˆä¸­
                            is_new_album = title not in processed_albums
                            
                            # 2. å¦‚æœæ˜¯æ–°çš„ç›¸å†Œï¼Œå¹¶ä¸”è¾¾åˆ°é™åˆ¶ï¼Œåˆ™ä¸å†æ·»åŠ ä»»åŠ¡ï¼Œç›´æ¥é€€å‡ºè¯»å–å½“å‰æ–‡ä»¶
                            if is_new_album and album_limit is not None and len(processed_albums) >= album_limit:
                                break 
                            
                            # 3. å°†å½“å‰ç›¸å†Œæ ‡é¢˜åŠ å…¥é›†åˆ
                            processed_albums.add(title)
                            
                            # 4. æ·»åŠ ä¸‹è½½ä»»åŠ¡ (æ‰€æœ‰å±äºå‰ N ä¸ªç›¸å†Œçš„å›¾ç‰‡éƒ½ä¼šè¢«æ·»åŠ )
                            all_tasks.append((url, title, model_name))
            except Exception as e:
                print(f"âš ï¸ è¯»å– CSV æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                
            print(f"   -> å·²ä» {filename} æ”¶é›† {len(processed_albums)} ä¸ªç›¸å†Œçš„ä»»åŠ¡ã€‚")


    if not all_tasks:
        print("âŒ æœªåœ¨æŒ‡å®š CSV æ–‡ä»¶ä¸­æ‰¾åˆ°ä»»ä½•ä¸‹è½½ä»»åŠ¡æˆ–ç›®æ ‡æ–‡ä»¶ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚")
        return

    total_tasks = len(all_tasks)
    print(f"\nğŸ‰ æ€»å…±æ”¶é›†åˆ° {total_tasks} ä¸ªä¸‹è½½ä»»åŠ¡ã€‚")

    # 2. å¯åŠ¨å¼‚æ­¥ä¸‹è½½
    with ThreadPoolExecutor(max_workers=MAX_DOWNLOAD_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
        futures = [executor.submit(download_worker, *task) for task in all_tasks]

        # ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ (å¹¶æä¾›ç®€å•è¿›åº¦)
        for i, future in enumerate(futures):
            future.result() 
            if (i + 1) % 50 == 0 or (i + 1) == total_tasks:
                print(f"   ä¸‹è½½è¿›åº¦: {i + 1}/{total_tasks} ({((i + 1) / total_tasks) * 100:.2f}%)")

    print("\nâœ… æ‰€æœ‰å¼‚æ­¥ä¸‹è½½ä»»åŠ¡å®Œæˆã€‚")

# --- ç¨‹åºå…¥å£ ---
if __name__ == '__main__':
    # ç¡®ä¿æ—¥å¿—å’Œä¸‹è½½æ ¹ç›®å½•å­˜åœ¨ (è¿™é‡Œåªæ£€æŸ¥ï¼Œä¸å¤„ç† FileExistsErrorï¼Œå‡è®¾å¸¸é‡è®¾ç½®æ­£ç¡®)
    os.makedirs(DOWNLOAD_ROOT, exist_ok=True)
    os.makedirs(CSV_LOGS_DIR, exist_ok=True) 
    
    run_downloader()