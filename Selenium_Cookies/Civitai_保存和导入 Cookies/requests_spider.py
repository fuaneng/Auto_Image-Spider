import os
import csv
import json
import time
import redis
import urllib3
import requests
import random 
from urllib.parse import unquote
from threading import Lock
from seleniumwire import webdriver  
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, StaleElementReferenceException
from selenium.webdriver.common.keys import Keys 


# ---------------- é…ç½®åŒº ----------------
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_KEY = 'image_md5_set_civitai'

CHROME_DRIVER_PATH = r"C:\Program Files\Google\chromedriver-win64\chromedriver.exe" 
USER_DATA_DIR = r"R:\py\Civitai_ä¿å­˜å’Œå¯¼å…¥ Cookies\civitai_data"
TAG_TXT_PATH = r"R:\py\Auto_Image-Spider\Selenium_Cookies\Civitai_ä¿å­˜å’Œå¯¼å…¥ Cookies\tag.txt"
CSV_DIR_PATH = r"R:\py\Auto_Image-Spider\Selenium_Cookies\Civitai_ä¿å­˜å’Œå¯¼å…¥ Cookies\Civitai_å›¾ç‰‡æ•°æ®_CSV"

BASE_IMG_URL = "https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/"
API_KEYWORD = "api/trpc/image.getInfinite"

# æ»šåŠ¨ä¸åŠ è½½å‚æ•° 
SCROLL_WAIT_TIME = 2.0      # æ»šåŠ¨åå¼ºåˆ¶ç­‰å¾…æ—¶é—´ (ç§’)
MAX_SCROLLS = 200           # æœ€å¤§ç¿»é¡µæ¬¡æ•°ï¼ˆé˜²æ­¢æ­»å¾ªç¯ï¼‰
NO_NEW_ROUNDS_TO_STOP = 3   # è¿ç»­ N è½®æ— æ–°å›¾ç‰‡æ—¶è®¤ä¸ºå·²åˆ°åº•éƒ¨


class CivitaiSpider:
    # ... (è¾…åŠ©å‡½æ•°ä¿æŒ V11/V10 ä¸å˜)
    def __init__(self):
        self.csv_lock = Lock()
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        try:
            self.redis = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸã€‚")
        except Exception as e:
            print(f"âš ï¸ Redis è¿æ¥å¤±è´¥ ({e})ï¼Œä½¿ç”¨å†…å­˜æ¨¡å¼ã€‚")
            self.redis = None
            self.visited_urls = set()

    def write_to_csv(self, name, url, csv_path, tag):
        try:
            with self.csv_lock:
                os.makedirs(os.path.dirname(csv_path), exist_ok=True)
                is_new = not os.path.exists(csv_path)
                with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    if is_new:
                        writer.writerow(['ImageName', 'URL', 'TAG'])
                    writer.writerow([name, url, tag])
        except Exception as e:
            print(f"[{tag}] å†™å…¥ CSV å‡ºé”™: {e}")

    def is_duplicate(self, url):
        if self.redis:
            return self.redis.sismember(REDIS_KEY, url) 
        return url in self.visited_urls

    def mark_visited(self, url):
        if self.redis:
            self.redis.sadd(REDIS_KEY, url)
        else:
            self.visited_urls.add(url)

    def setup_browser(self):
        options = Options()
        options.add_argument(f"user-data-dir={USER_DATA_DIR}")
        options.add_argument("--start-maximized")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--ignore-certificate-errors")

        service = Service(CHROME_DRIVER_PATH)

        seleniumwire_options = {
            'proxy': {
                'http': 'http://127.0.0.1:7897',
                'https': 'http://127.0.0.1:7897',
                'no_proxy': 'localhost,127.0.0.1'
            }
        }

        driver = webdriver.Chrome(service=service, options=options, seleniumwire_options=seleniumwire_options)
        return driver

    def extract_api_urls(self, driver, tag):
        urls = []
        for request in driver.requests:
            if API_KEYWORD in request.url and request.response:
                urls.append(request.url)
        
        unique_urls = list(dict.fromkeys(urls))
        
        if unique_urls:
            print(f"[{tag}] ğŸ” æ•è·åˆ° {len(unique_urls)} æ¡å”¯ä¸€ API è¯·æ±‚")
            return unique_urls
        return []

    def fetch_images(self, api_url, tag, csv_path):
        print(f"[{tag}] ğŸŒ æ­£åœ¨è¯·æ±‚ API: {api_url[:100]}...")
        try:
            r = requests.get(api_url, timeout=30) 
            r.raise_for_status()
            data = r.json()
        except Exception as e:
            print(f"[{tag}] âŒ è¯·æ±‚å¤±è´¥: {e}")
            return None

        try:
            items = data['result']['data']['json']['items']
        except Exception:
            print(f"[{tag}] âš ï¸ JSON ç»“æ„ä¸ç¬¦åˆé¢„æœŸæˆ– 'items' å­—æ®µç¼ºå¤±")
            return None

        count = 0
        for item in items:
            img_id = item.get("id")
            img_url = item.get("url")
            
            if not img_id or not img_url:
                continue
            
            full_url = f"{BASE_IMG_URL}{img_url}/{img_id}" 
            
            if not self.is_duplicate(full_url):
                self.mark_visited(full_url)
                img_name = f"{img_id}.jpg"
                self.write_to_csv(img_name, full_url, csv_path, tag)
                count += 1
        
        print(f"[{tag}] âœ… æˆåŠŸæå– {count} å¼ æ–°å›¾ç‰‡ã€‚")

        try:
            next_cursor = data['result']['data']['json'].get('nextCursor')
            if next_cursor:
                print(f"[{tag}] â¡ï¸ å‘ç°ä¸‹ä¸€é¡µ cursor: {next_cursor[:10]}...")
            return next_cursor
        except Exception:
            return None


    # ---------------- ä¸»çˆ¬å–å‡½æ•° (V12: æ¢å¤ V9 æ»šåŠ¨æ ¸å¿ƒ) ----------------
    def crawl_tag(self, tag):
        """
        çˆ¬å–å•ä¸ªæ ‡ç­¾ä¸‹çš„å›¾ç‰‡ï¼Œæ¢å¤ V9 çš„ç»„åˆæ»šåŠ¨ï¼Œå¹¶ä½¿ç”¨ V11 çš„ API å¤„ç†é€»è¾‘ã€‚
        """
        driver = self.setup_browser()
        url = f"https://civitai.com/images?tags={tag}"
        csv_path = os.path.join(CSV_DIR_PATH, f"tag_{tag}.csv")
        image_card_selector = 'div.relative.flex-1' 
        wait = WebDriverWait(driver, 20) # ä¿æŒ 20 ç§’ç­‰å¾…æ–°å¡ç‰‡
        
        last_cursor = None
        no_new_content_count = 0 
        current_element_count = 0

        print(f"\nğŸš€ å¼€å§‹çˆ¬å–æ ‡ç­¾ [{tag}] ...")
        
        try:
            driver.get(url)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, image_card_selector)))
            
            # 1. åˆå§‹åŒ–ï¼Œå¤„ç†ç¬¬ä¸€é¡µæ•°æ® (V11 é€»è¾‘)
            current_element_count = len(driver.find_elements(By.CSS_SELECTOR, image_card_selector))
            print(f"[{tag}] [âˆš] é¡µé¢å·²åŠ è½½ï¼Œæ‰¾åˆ° {current_element_count} å¼ åˆå§‹å›¾ç‰‡ã€‚")
            
            initial_urls = self.extract_api_urls(driver, tag)
            initial_cursor_url = next((u for u in initial_urls if '%22cursor%22%3Anull' in u), None)
            
            if initial_cursor_url:
                print(f"[{tag}] âš™ï¸ æ­£åœ¨å¤„ç†åˆå§‹é¡µ API...")
                last_cursor = self.fetch_images(unquote(initial_cursor_url), tag, csv_path)
            else:
                print(f"[{tag}] âš ï¸ æœªæ•è·åˆ°åˆå§‹é¡µ API è¯·æ±‚ (cursor:null)ã€‚")

            del driver.requests  # æ¸…ç©ºè¯·æ±‚å†å²

            # è·å–ç„¦ç‚¹
            driver.find_element(By.TAG_NAME, 'body').click()
            time.sleep(1)

        except TimeoutException:
            print(f"[{tag}] [âœ—] é¡µé¢åŠ è½½è¶…æ—¶æˆ–æœªæ‰¾åˆ°åˆå§‹å›¾ç‰‡å¡ç‰‡ã€‚")
            driver.quit()
            return
        except Exception as e:
            print(f"[{tag}] [âœ—] é¡µé¢åŠ è½½å¼‚å¸¸: {e}")
            driver.quit()
            return


        # --------------------- ä¸»æ»šåŠ¨/åŠ è½½å¾ªç¯ ---------------------
        for page_attempt in range(MAX_SCROLLS):
            print(f"\n[{tag}] ==== å¼€å§‹ç¬¬ {page_attempt+1} / {MAX_SCROLLS} è½®ç¿»é¡µå°è¯• ====")
            
            # 2. **æ ¸å¿ƒæ»šåŠ¨æ“ä½œ (V9 ç»„åˆæ‹³)**
            print(f"[{tag}] ğŸŒ€ æ»šåŠ¨å°è¯•: Keys.END + scrollTo(bottom)")
            try:
                # å°è¯•é”®ç›˜æ»šåŠ¨
                body = driver.find_element(By.TAG_NAME, 'body')
                body.send_keys(Keys.END)
                time.sleep(0.5)
            except Exception:
                pass 

            # å°è¯• JavaScript æ»šåŠ¨
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # **å¼ºåˆ¶ç­‰å¾…ï¼Œç¡®ä¿è„šæœ¬è¢«è§¦å‘**
            print(f"[{tag}] â³ æ»šåŠ¨åå¼ºåˆ¶ç­‰å¾… {SCROLL_WAIT_TIME} ç§’...")
            time.sleep(SCROLL_WAIT_TIME)
            
            
            # 3. ç­‰å¾…æ–°å¡ç‰‡åŠ è½½ï¼ˆç¡®è®¤ç¿»é¡µç”Ÿæ•ˆï¼‰
            try:
                print(f"[{tag}] â³ ç­‰å¾…æ–°å¡ç‰‡åŠ è½½ (å½“å‰: {current_element_count} å¼ )...")
                wait.until(
                    lambda d: len(d.find_elements(By.CSS_SELECTOR, image_card_selector)) > current_element_count
                )
                
                new_count = len(driver.find_elements(By.CSS_SELECTOR, image_card_selector))
                print(f"[{tag}] âœ… æ»šåŠ¨æˆåŠŸï¼ŒåŠ è½½äº† {new_count - current_element_count} å¼ æ–°å¡ç‰‡ã€‚")
                current_element_count = new_count
                no_new_content_count = 0 

            except TimeoutException:
                # 4. è¶…æ—¶å¤„ç†
                no_new_content_count += 1
                print(f"[âš ] ç­‰å¾…è¶…æ—¶ï¼Œæœªå‘ç°æ–°å›¾ç‰‡ã€‚({no_new_content_count}/{NO_NEW_ROUNDS_TO_STOP})")

                # **V12 ä¿®æ­£ï¼šä¸å†è¿›è¡Œ `scrollHeight` æ£€æŸ¥ï¼Œåªä¾èµ–è¿ç»­è¶…æ—¶è®¡æ•°**
                if no_new_content_count >= NO_NEW_ROUNDS_TO_STOP: 
                    print(f"[{tag}] ğŸ’¤ è¿ç»­ {NO_NEW_ROUNDS_TO_STOP} æ¬¡æœªåŠ è½½æ–°å†…å®¹ï¼Œåˆ¤å®šä¸ºå·²åˆ°åº•éƒ¨ï¼Œåœæ­¢ã€‚")
                    break
                
                del driver.requests
                continue 

            # 5. è·å–å¹¶å¤„ç†æ–°çš„ API è¯·æ±‚ (V11 é€»è¾‘)
            urls = self.extract_api_urls(driver, tag)
            filtered_urls = [u for u in urls if '%22cursor%22%3Anull' not in u]

            if not filtered_urls:
                print(f"[{tag}] âš ï¸ é¡µé¢åŠ è½½äº†æ–°å¡ç‰‡ï¼Œä½†æœªæ•è·åˆ°æ–°çš„åˆ†é¡µ API è¯·æ±‚ã€‚")
                del driver.requests
                continue
            
            latest_valid_url = unquote(filtered_urls[-1]) 
            
            # 6. æ£€æŸ¥ cursor æ˜¯å¦é‡å¤
            if last_cursor and f"cursor%22%3A%22{last_cursor}" in latest_valid_url:
                print(f"[{tag}] âŒ Cursor æœªæ›´æ–° (æ•è·åˆ°é‡å¤è¯·æ±‚)ï¼Œæ¸…é™¤è¯·æ±‚å¹¶ç»§ç»­ä¸‹ä¸€è½®æ»šåŠ¨ã€‚")
                del driver.requests
                continue

            # 7. è·å–å’Œå¤„ç†å›¾ç‰‡æ•°æ®
            next_cursor = self.fetch_images(latest_valid_url, tag, csv_path)
            
            # 8. æ›´æ–° cursor å¹¶æ¸…é™¤è¯·æ±‚å†å²
            if not next_cursor:
                print(f"[{tag}] âœ… å·²åˆ°æœ€åä¸€é¡µ (nextCursor ä¸ºç©º)ã€‚")
                break
            
            last_cursor = next_cursor
            del driver.requests 

        if page_attempt == MAX_SCROLLS - 1:
            print(f"[âš ] è¾¾åˆ°æœ€å¤§ç¿»é¡µæ¬¡æ•° {MAX_SCROLLS}ï¼Œå¼ºåˆ¶åœæ­¢ã€‚")

        driver.quit()
        print(f"[{tag}] ğŸ¯ å®Œæˆçˆ¬å–ã€‚")

    # ---------------- ä¸»ç¨‹åºå…¥å£ ----------------
    def run(self):
        if not os.path.exists(TAG_TXT_PATH):
            print(f"âŒ æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {TAG_TXT_PATH}")
            return

        with open(TAG_TXT_PATH, 'r', encoding='utf-8') as f:
            tags = [t.strip() for t in f if t.strip()]

        for tag in tags:
            self.crawl_tag(tag)


if __name__ == "__main__":
    spider = CivitaiSpider()
    spider.run()