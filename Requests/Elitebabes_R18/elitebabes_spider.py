import requests
import urllib3
import os
import csv
import re
import redis
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from bs4 import BeautifulSoup
from lxml import etree
from tqdm import tqdm

# --- é…ç½®å¸¸é‡ ---
BASE_URL = "https://www.elitebabes.com/"
API_URL_TEMPLATE = "https://www.elitebabes.com/gridapi/?content=channel_old&nr=6512&sort=trending&mpage={}"
START_PAGE = 1
# ç›¸å†Œ/è¯¦æƒ…é¡µè§£æçº¿ç¨‹æ•°
ALBUM_PARSING_THREADS = 20
# å›¾ç‰‡ä¸‹è½½çº¿ç¨‹æ•°
DOWNLOAD_THREADS = 50

# CSV/ä¸‹è½½è·¯å¾„é…ç½®
CSV_DIR_PATH = r"R:\py\Auto_Image-Spider\Requests\Elitebabes_R18"
CSV_FILENAME = "all_images_data.csv"
DOWNLOAD_PATH = os.path.join(CSV_DIR_PATH, "images")

# Redis é…ç½®
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_KEY = 'elitebabes_r18_image_url_set' # ä½¿ç”¨ å›¾ç‰‡URL ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦

class ElitebabesSpider:
    """
    Elitebabes å›¾ç‰‡çˆ¬è™«ç±»ï¼šè´Ÿè´£ç›¸å†Œåˆ—è¡¨è·å–ã€è¯¦æƒ…é¡µè§£æã€æ•°æ®å­˜å‚¨å’Œå¼‚æ­¥ä¸‹è½½ã€‚
    é›†æˆäº† Redis/å†…å­˜å»é‡é€»è¾‘å’Œçº¿ç¨‹æ± ç®¡ç†ã€‚
    """
    def __init__(self, csv_dir_path=CSV_DIR_PATH, csv_filename=CSV_FILENAME, 
                 download_path=DOWNLOAD_PATH, redis_host=REDIS_HOST, redis_port=REDIS_PORT):
        """
        åˆå§‹åŒ–çˆ¬è™«å®ä¾‹ï¼Œé›†æˆ Redis/å†…å­˜å»é‡é€»è¾‘ã€‚
        """
        self.csv_dir_path = csv_dir_path
        self.csv_path = os.path.join(self.csv_dir_path, csv_filename) 
        self.download_path = download_path
        self.csv_lock = Lock() # ç”¨äº CSV å†™å…¥çš„çº¿ç¨‹é”
        self.album_urls = [] # ç”¨äºå­˜å‚¨æ‰€æœ‰ç›¸å†ŒURLå’Œæ ‡é¢˜çš„åˆ—è¡¨
        self.image_data_list = [] # ç”¨äºå­˜å‚¨æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯çš„åˆ—è¡¨

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.elitebabes.com/watch-4-beauty/',    # é¿å… 403 é”™è¯¯ï¼Œå¦‚æœæ˜¯æ¨¡ç‰¹ /dakota-3/
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'X-Requested-With': 'XMLHttpRequest'
        }
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.csv_dir_path, exist_ok=True)
        os.makedirs(self.download_path, exist_ok=True)

        # --- å»é‡åˆå§‹åŒ–é€»è¾‘ ---
        try:
            self.redis = redis.StrictRedis(host=redis_host, port=redis_port, decode_responses=True)
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸï¼Œä½¿ç”¨ Redis é›†åˆè¿›è¡Œå»é‡ã€‚")
        except redis.exceptions.ConnectionError as e:
            print(f"âš ï¸ Redis è¿æ¥å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            # å†…å­˜å»é‡é›†åˆ
            self.visited_urls = set()
        except Exception as e:
            print(f"âš ï¸ Redis åˆå§‹åŒ–é‡åˆ°å…¶ä»–é”™è¯¯ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_urls = set()
            
        # åˆå§‹åŒ– CSV æ–‡ä»¶å¤´
        self._initialize_csv()

    def _initialize_csv(self):
        """åˆå§‹åŒ– CSV æ–‡ä»¶ï¼Œå†™å…¥è¡¨å¤´ï¼ˆå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼‰ã€‚"""
        fieldnames = ['æ ‡é¢˜', 'å›¾ç‰‡åç§°', 'å›¾ç‰‡URL', 'æ‰€å±ç›¸å†Œ', 'watch-4-beautyæ ‡ç­¾']
        if not os.path.exists(self.csv_path) or os.path.getsize(self.csv_path) == 0:
            with self.csv_lock:
                with open(self.csv_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    print(f"ğŸ“„ CSV æ–‡ä»¶å·²åˆ›å»º: {self.csv_path}")
        else:
            print(f"ğŸ“„ CSV æ–‡ä»¶å·²å­˜åœ¨: {self.csv_path}")

    def is_url_visited(self, url):
        """æ£€æŸ¥ URL æ˜¯å¦å·²è¢«è®¿é—®æˆ–å¤„ç†ï¼ˆå»é‡ï¼‰ã€‚"""
        if self.redis:
            # Redis å»é‡ï¼šæ£€æŸ¥é›†åˆä¸­æ˜¯å¦å­˜åœ¨
            return self.redis.sismember(REDIS_KEY, url)
        else:
            # å†…å­˜å»é‡ï¼šæ£€æŸ¥ set ä¸­æ˜¯å¦å­˜åœ¨
            return url in self.visited_urls

    def mark_url_visited(self, url):
        """å°† URL æ ‡è®°ä¸ºå·²è®¿é—®æˆ–å¤„ç†ã€‚"""
        if self.redis:
            # Redis æ ‡è®°ï¼šæ·»åŠ åˆ°é›†åˆ
            self.redis.sadd(REDIS_KEY, url)
        else:
            # å†…å­˜æ ‡è®°ï¼šæ·»åŠ åˆ° set
            self.visited_urls.add(url)

    def crawl_album_list(self):
        """
        ç¬¬ä¸€éƒ¨åˆ†ï¼šæœç´¢ watch-4-beautyï¼Œè·å–ç›¸å†Œåˆ—è¡¨çš„ URL å’Œæ ‡é¢˜ã€‚
        """
        print("\n--- 1. ğŸš€ å¼€å§‹è·å–ç›¸å†Œåˆ—è¡¨ ---")
        page = START_PAGE
        while True:
            url = API_URL_TEMPLATE.format(page)
            print(f"   => æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ: {url}")
            try:
                response = requests.get(url, headers=self.headers, verify=False, timeout=15)
                # API è¿”å›çš„ä¸æ˜¯æ ‡å‡†çš„ JSONï¼Œè€Œæ˜¯åŒ…å« HTML çš„æ–‡æœ¬
                if response.status_code == 200 and response.text.strip():
                    # æ£€æŸ¥å“åº”å†…å®¹æ˜¯å¦åŒ…å«ç›¸å†Œåˆ—è¡¨çš„ HTML
                    if "li style=" not in response.text:
                         print(f"   => ç¬¬ {page} é¡µå“åº”ä¸åŒ…å«ç›¸å†Œåˆ—è¡¨ï¼Œæˆ–è€…å·²åˆ°è¾¾æœ«é¡µã€‚")
                         break
                         
                    # ä½¿ç”¨ lxml/etree è§£æåŒ…å« HTML ç‰‡æ®µçš„å“åº”
                    # å› ä¸ºå“åº”å†…å®¹æ²¡æœ‰æ ¹æ ‡ç­¾ï¼Œéœ€è¦æ‰‹åŠ¨æ·»åŠ ä¸€ä¸ª
                    html_content = f"<html><body><ul>{response.text}</ul></body></html>"
                    tree = etree.HTML(html_content)
                    album_elements = tree.xpath('//li/figure/a')
                    
                    if not album_elements:
                        print(f"   => ç¬¬ {page} é¡µæœªæ‰¾åˆ°ç›¸å†Œå…ƒç´ ï¼Œåœæ­¢çˆ¬å–ã€‚")
                        break
                        
                    for a_tag in album_elements:
                        album_url = a_tag.get('href')
                        album_title = a_tag.get('title')
                        if album_url and album_title:
                            self.album_urls.append({
                                'album_url': album_url,
                                'album_title': album_title
                            })
                            
                    print(f"   => ç¬¬ {page} é¡µè·å– {len(album_elements)} ä¸ªç›¸å†Œã€‚")
                    page += 1
                else:
                    print(f"   => å“åº”çŠ¶æ€ç é 200 ({response.status_code}) æˆ–å†…å®¹ä¸ºç©ºï¼Œåœæ­¢çˆ¬å–ã€‚")
                    break
            except requests.exceptions.RequestException as e:
                print(f"   => è¯·æ±‚ç¬¬ {page} é¡µå¤±è´¥: {e}ï¼Œåœæ­¢çˆ¬å–ã€‚")
                break
            
        print(f"--- 1. âœ… ç›¸å†Œåˆ—è¡¨è·å–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(self.album_urls)} ä¸ªç›¸å†Œã€‚---")

    def parse_album_page(self, album_info):
        """
        ç¬¬äºŒéƒ¨åˆ†ï¼šè®¿é—®å›¾ç‰‡è¯¦æƒ…é¡µï¼Œè·å–å›¾ç‰‡ä¿¡æ¯ã€‚
        """
        album_url = album_info['album_url']
        album_title = album_info['album_title']
        
        try:
            response = requests.get(album_url, headers=self.headers, verify=False, timeout=15)
            if response.status_code != 200:
                print(f"   [WARN] è®¿é—®ç›¸å†Œé¡µå¤±è´¥ {album_url}: çŠ¶æ€ç  {response.status_code}")
                return None
            
            # ä½¿ç”¨ lxml è§£æ HTML é¡µé¢
            tree = etree.HTML(response.text)
            
            # æ‰¾åˆ°æ‰€æœ‰åŒ…å«å›¾ç‰‡ä¿¡æ¯çš„ <li> å…ƒç´ ä¸‹çš„ <a> æ ‡ç­¾
            image_elements = tree.xpath('//ul[@class="list-gallery static css"]/li/a')
            
            extracted_images = []
            for a_tag in image_elements:
                # æå–æœ€å¤§çš„åˆ†è¾¨ç‡çš„å›¾ç‰‡ URL
                data_srcset = a_tag.get('data-srcset')
                image_url = ''
                
                # --- START: é€šç”¨åŸå›¾ URL æå–é€»è¾‘ ---
                if data_srcset:
                    # 1. è·å– data-srcset ä¸­ç¬¬ä¸€ä¸ª URLï¼ˆæœ€é«˜åˆ†è¾¨ç‡ï¼‰
                    # æ ¼å¼å¦‚ï¼šhttps://cdn.../0002-01_2400.jpg 2400w, ...
                    first_url_part = data_srcset.split(',')[0].strip()
                    # æå–çº¯ URLï¼ˆå»é™¤åé¢çš„ ' 2400w' ç­‰æè¿°ç¬¦ï¼‰
                    max_res_url = first_url_part.split(' ')[0]
                    
                    # 2. ä» max_res_url ä¸­ç§»é™¤éšæœºåˆ†è¾¨ç‡åç¼€
                    
                    # ç¤ºä¾‹: max_res_url = https://cdn.elitebabes.com/content/250585/0002-01_2400.jpg
                    
                    # æ‰¾åˆ°æœ€åä¸€ä¸ª '/'ï¼Œåˆ†ç¦»è·¯å¾„å’Œæ–‡ä»¶å
                    path_part, filename_part = max_res_url.rsplit('/', 1)
                    
                    # æ‰¾åˆ°æœ€åä¸€ä¸ª '_' å’Œå€’æ•°ç¬¬ä¸€ä¸ª '.'
                    last_underscore = filename_part.rfind('_')
                    last_dot = filename_part.rfind('.')
                    
                    if last_underscore != -1 and last_dot != -1 and last_underscore < last_dot:
                        # å‡è®¾åˆ†è¾¨ç‡åç¼€åœ¨æœ€åä¸€ä¸ª '_' å’Œ '.' ä¹‹é—´
                        # ç§»é™¤ '_åˆ†è¾¨ç‡' éƒ¨åˆ†
                        # '0002-01_2400.jpg' -> '0002-01' + '.jpg'
                        base_name = filename_part[:last_underscore]
                        extension = filename_part[last_dot:]
                        clean_filename = base_name + extension
                        
                        image_url = path_part + '/' + clean_filename
                    else:
                        # å¦‚æœæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œåˆ™ä½¿ç”¨æœ€é«˜åˆ†è¾¨ç‡çš„ URL ä½œä¸ºå…œåº•
                        image_url = max_res_url
                # --- END: é€šç”¨åŸå›¾ URL æå–é€»è¾‘ ---
                
                # å¦‚æœæ²¡æœ‰ data-srcsetï¼Œå°è¯•ä½¿ç”¨ href å±æ€§
                if not image_url:
                    image_url = a_tag.get('href')

                # æå– img æ ‡ç­¾çš„ alt å±æ€§ä½œä¸ºæ ‡é¢˜
                img_tag = a_tag.find('img')
                if img_tag is not None:
                    image_title = img_tag.get('alt', '').strip()
                else:
                    image_title = album_title # å¤‡ç”¨æ ‡é¢˜
                    
                # æå–å›¾ç‰‡åç§° (ä½¿ç”¨å»é™¤åç¼€åçš„ image_url)
                if image_url:
                    # ç¤ºä¾‹: image_url = https://cdn.../250585/0002-01.jpg
                    name_parts = image_url.split('/')[-2:]
                    # ä»…ä¿ç•™æ–‡ä»¶åéƒ¨åˆ†ï¼Œå»é™¤æ‰©å±•å
                    base_name = os.path.splitext(name_parts[-1])[0] 
                    # é‡æ–°æ„é€ å›¾ç‰‡åç§°
                    # ç¤ºä¾‹: 250585_0002-01
                    image_name_for_file = f"{name_parts[-2]}_{base_name}"
                    
                    if not self.is_url_visited(image_url):
                        self.mark_url_visited(image_url) # æ ‡è®°ä¸ºå·²å¤„ç†
                        
                        extracted_images.append({
                            'æ ‡é¢˜': image_title,
                            'å›¾ç‰‡åç§°': image_name_for_file, # ç”¨äºæ–‡ä»¶å
                            'å›¾ç‰‡URL': image_url,
                            'æ‰€å±ç›¸å†Œ': album_title,
                            'watch-4-beautyæ ‡ç­¾': 'watch-4-beauty',
                        })

            return extracted_images
            
        except requests.exceptions.RequestException as e:
            print(f"   [ERROR] è§£æç›¸å†Œé¡µè¯·æ±‚å¤±è´¥ {album_url}: {e}")
            return None
        except Exception as e:
            print(f"   [ERROR] è§£æç›¸å†Œé¡µå†…å®¹å¤±è´¥ {album_url}: {e}")
            return None

    def _process_album_parsing(self, album_info):
        """çº¿ç¨‹æ± æ‰§è¡Œå‡½æ•°ï¼šè§£æå•ä¸ªç›¸å†Œå¹¶å­˜å‚¨æ•°æ®ã€‚"""
        images_data = self.parse_album_page(album_info)
        if images_data:
            self.image_data_list.extend(images_data) # æ”¶é›†æ•°æ®åˆ°åˆ—è¡¨
            self.save_to_csv(images_data) # å®æ—¶å†™å…¥ CSV
            print(f"   [INFO] å·²è§£æ {album_info['album_title']}ï¼Œè·å– {len(images_data)} å¼ å›¾ç‰‡ã€‚")


    def start_crawl(self, download_enabled=False):
        """
        å¯åŠ¨çˆ¬è™«ä»»åŠ¡ï¼Œæ”¯æŒä»…çˆ¬å–æ•°æ®æˆ–çˆ¬å–+ä¸‹è½½ä¸¤ç§æ¨¡å¼ã€‚
        """
        # 1. çˆ¬å–ç›¸å†Œåˆ—è¡¨
        self.crawl_album_list()
        
        if not self.album_urls:
            print("ğŸ›‘ æ²¡æœ‰æ‰¾åˆ°ç›¸å†Œåˆ—è¡¨ï¼Œçˆ¬è™«ç»“æŸã€‚")
            return

        print(f"\n--- 2. âš¡ï¸ å¼€å§‹å¼‚æ­¥è§£æ {len(self.album_urls)} ä¸ªç›¸å†Œ ({ALBUM_PARSING_THREADS} çº¿ç¨‹) ---")
        
        # 2. å¼‚æ­¥è§£æç›¸å†Œè¯¦æƒ…é¡µ
        # ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶å‘è§£æ
        with ThreadPoolExecutor(max_workers=ALBUM_PARSING_THREADS) as executor:
            # æäº¤ä»»åŠ¡
            future_to_album = {executor.submit(self._process_album_parsing, album_info): album_info for album_info in self.album_urls}
            
            # ä½¿ç”¨ tqdm è¿›åº¦æ¡è¿½è¸ªä»»åŠ¡å®Œæˆæƒ…å†µ
            for future in tqdm(as_completed(future_to_album), total=len(self.album_urls), desc="ç›¸å†Œè§£æè¿›åº¦"):
                # ç®€å•å¤„ç†å¼‚å¸¸ï¼Œä¸å½±å“å…¶ä»–çº¿ç¨‹
                try:
                    future.result()
                except Exception as exc:
                    album_info = future_to_album[future]
                    print(f"   [ERROR] ç›¸å†Œ {album_info['album_url']} åœ¨è§£ææ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")

        print("--- 2. âœ… æ‰€æœ‰ç›¸å†Œè§£æå®Œæˆã€‚---")
        
        # 3. å¦‚æœå¯ç”¨äº†ä¸‹è½½ï¼Œåˆ™å¯åŠ¨ä¸‹è½½ä»»åŠ¡
        if download_enabled:
            # ä» CSV æ–‡ä»¶é‡æ–°åŠ è½½æ•°æ®ï¼Œç¡®ä¿ä¸‹è½½çš„æ˜¯æœ€æ–°çš„å®Œæ•´åˆ—è¡¨
            self.image_data_list = self._load_data_from_csv()
            if self.image_data_list:
                self.start_download()
            else:
                print("ğŸ›‘ CSV ä¸­æ²¡æœ‰å›¾ç‰‡æ•°æ®ï¼Œæ— æ³•å¯åŠ¨ä¸‹è½½ã€‚")


    def save_to_csv(self, data_list):
        """
        å°†å›¾ç‰‡ä¿¡æ¯å†™å…¥ CSV æ–‡ä»¶ã€‚
        """
        fieldnames = ['æ ‡é¢˜', 'å›¾ç‰‡åç§°', 'å›¾ç‰‡URL', 'æ‰€å±ç›¸å†Œ', 'watch-4-beautyæ ‡ç­¾']
        with self.csv_lock: # ä½¿ç”¨é”ç¡®ä¿å¤šçº¿ç¨‹å†™å…¥å®‰å…¨
            with open(self.csv_path, 'a', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                for data in data_list:
                    writer.writerow(data)


    def _load_data_from_csv(self):
        """ä» CSV æ–‡ä»¶åŠ è½½æ‰€æœ‰æ•°æ®ï¼Œç”¨äºå•ç‹¬å¯åŠ¨ä¸‹è½½æˆ–ç¡®ä¿å®Œæ•´åˆ—è¡¨ã€‚"""
        data = []
        if os.path.exists(self.csv_path):
            try:
                with open(self.csv_path, 'r', newline='', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        data.append(row)
                print(f"ğŸ’¾ å·²ä» CSV æ–‡ä»¶åŠ è½½ {len(data)} æ¡å›¾ç‰‡è®°å½•ã€‚")
            except Exception as e:
                print(f"   [ERROR] åŠ è½½ CSV æ–‡ä»¶å¤±è´¥: {e}")
        return data


    def download_image(self, image_data):
        """
        ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸‹è½½å•å¼ å›¾ç‰‡åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ã€‚
        """
        image_url = image_data['å›¾ç‰‡URL']
        album_title = image_data['æ‰€å±ç›¸å†Œ']
        image_name = image_data['å›¾ç‰‡åç§°'] # åŒ…å«å”¯ä¸€ ID å’Œåˆ†è¾¨ç‡å‰ç¼€

        # 1. åˆ›å»ºå­æ–‡ä»¶å¤¹ï¼ˆä»¥ç›¸å†Œæ ‡é¢˜å‘½åï¼Œå¹¶è¿›è¡Œå®‰å…¨æ–‡ä»¶åå¤„ç†ï¼‰
        # ç§»é™¤ Windows æ–‡ä»¶åéæ³•å­—ç¬¦
        safe_album_title = re.sub(r'[\\/:*?"<>|]', '', album_title).strip()
        album_dir = os.path.join(self.download_path, safe_album_title)
        os.makedirs(album_dir, exist_ok=True)
        
        # 2. æ„é€ æœ€ç»ˆæ–‡ä»¶è·¯å¾„ï¼ˆä¿ç•™åŸå§‹æ‰©å±•åï¼‰
        _, ext = os.path.splitext(image_url.split('?')[0])
        if not ext: ext = '.jpg' # é»˜è®¤æ‰©å±•å
        
        final_file_path = os.path.join(album_dir, f"{image_name}{ext}")

        # 3. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆäºŒæ¬¡å»é‡/æ–­ç‚¹ç»­ä¼ ï¼‰
        if os.path.exists(final_file_path):
            # print(f"      [SKIP] æ–‡ä»¶å·²å­˜åœ¨: {final_file_path}")
            return final_file_path # è¿”å›å·²å­˜åœ¨çš„æ–‡ä»¶è·¯å¾„

        # 4. ä¸‹è½½å›¾ç‰‡
        try:
            # æ·»åŠ  Referer å¤´ä»¥é¿å… 403 é”™è¯¯
            download_headers = self.headers.copy()
            download_headers['Referer'] = BASE_URL # éšä¾¿ä¸€ä¸ª referrer 
            
            response = requests.get(image_url, headers=download_headers, stream=True, verify=False, timeout=30)
            
            if response.status_code == 200:
                with open(final_file_path, 'wb') as f:
                    # ä½¿ç”¨ response.iter_content èŠ‚çœå†…å­˜
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                # print(f"      [SUCCESS] ä¸‹è½½å®Œæˆ: {final_file_path}")
                return final_file_path
            else:
                # print(f"      [FAIL] ä¸‹è½½å¤±è´¥ {image_url}: çŠ¶æ€ç  {response.status_code}")
                return None
        except requests.exceptions.RequestException as e:
            # print(f"      [ERROR] ä¸‹è½½è¯·æ±‚å¤±è´¥ {image_url}: {e}")
            return None


    def start_download(self):
        """
        å•ç‹¬å¯åŠ¨ä¸‹è½½ä»»åŠ¡ï¼Œä»å·²å­˜å‚¨çš„ CSV æ•°æ®ä¸­è¯»å–ä¿¡æ¯è¿›è¡Œä¸‹è½½ã€‚
        """
        # ç¡®ä¿æ•°æ®å·²ä» CSV æ–‡ä»¶åŠ è½½
        if not self.image_data_list:
            self.image_data_list = self._load_data_from_csv()

        if not self.image_data_list:
            print("ğŸ›‘ æ— æ³•å¯åŠ¨ä¸‹è½½ï¼šè¯·å…ˆè¿è¡Œçˆ¬å–æ¨¡å¼è·å–æ•°æ®åˆ° CSV æ–‡ä»¶ã€‚")
            return

        print(f"\n--- 3. ğŸ’¾ å¼€å§‹å¼‚æ­¥ä¸‹è½½ {len(self.image_data_list)} å¼ å›¾ç‰‡ ({DOWNLOAD_THREADS} çº¿ç¨‹) ---")
        
        # 3. å¼‚æ­¥ä¸‹è½½å›¾ç‰‡
        with ThreadPoolExecutor(max_workers=DOWNLOAD_THREADS) as executor:
            future_to_image = {executor.submit(self.download_image, img_data): img_data for img_data in self.image_data_list}
            
            # ä½¿ç”¨ tqdm è¿›åº¦æ¡è¿½è¸ªä»»åŠ¡å®Œæˆæƒ…å†µ
            download_success_count = 0
            for future in tqdm(as_completed(future_to_image), total=len(self.image_data_list), desc="å›¾ç‰‡ä¸‹è½½è¿›åº¦"):
                try:
                    result = future.result()
                    if result:
                        download_success_count += 1
                except Exception as exc:
                    image_data = future_to_image[future]
                    # print(f"   [ERROR] å›¾ç‰‡ {image_data['å›¾ç‰‡URL']} åœ¨ä¸‹è½½æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
        
        print(f"--- 3. âœ… æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆã€‚æˆåŠŸä¸‹è½½/è·³è¿‡ {download_success_count} å¼ å›¾ç‰‡ã€‚---")


# --- å¯åŠ¨ä»£ç  ---
if __name__ == '__main__':
    # å®ä¾‹åŒ–çˆ¬è™«
    spider = ElitebabesSpider()
    
    # --- è¿è¡Œæ¨¡å¼ç¤ºä¾‹ ---
    
    # ç¤ºä¾‹ 1: ä»…çˆ¬å–æ•°æ®åˆ° CSVï¼Œä¸ä¸‹è½½å›¾ç‰‡
    # print("\n--- æ¨¡å¼ä¸€ï¼šä»…çˆ¬å–æ•°æ®åˆ° CSV (download_enabled=False) ---")
    # spider.start_crawl(download_enabled=False)
    
    # ç¤ºä¾‹ 2: çˆ¬å–æ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡ (å¦‚æœéœ€è¦ï¼Œè¯·å–æ¶ˆä¸‹ä¸€è¡Œçš„æ³¨é‡Š)
    # print("\n--- æ¨¡å¼äºŒï¼šçˆ¬å–æ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡ (download_enabled=True) ---")
    # spider.start_crawl(download_enabled=True) 

    # ç¤ºä¾‹ 3: ä¹Ÿå¯ä»¥åœ¨æ•°æ®æ”¶é›†å®Œæˆåå•ç‹¬å¯åŠ¨ä¸‹è½½ä»»åŠ¡
    # æ³¨æ„ï¼šè¿è¡Œæ­¤æ¨¡å¼å‰ï¼Œè¯·ç¡®ä¿å·²è¿è¡Œè¿‡æ¨¡å¼ä¸€æˆ–äºŒï¼Œä¸” CSV æ–‡ä»¶ä¸­åŒ…å«æ•°æ®
    print("\n--- æ¨¡å¼ä¸‰ï¼šå•ç‹¬å¯åŠ¨ä¸‹è½½ä»»åŠ¡ ---")
    spider.start_download()