import requests
import os
import csv
import time
import urllib3
import redis
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# --- é…ç½®å¸¸é‡ ---
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_KEY = 'babesource_image_url_set' # ä½¿ç”¨ å›¾ç‰‡ URL ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦è¿›è¡Œå»é‡

# æ–‡ä»¶è·¯å¾„é…ç½®
BASE_DIR = r"R:\py\Auto_Image-Spider\Requests\Babesource"
TAG_FILE = os.path.join(BASE_DIR, "äººåtag.txt")
CSV_PATH = os.path.join(BASE_DIR, "all_images_data.csv")
DOWNLOAD_DIR = os.path.join(BASE_DIR, "images")

# ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)
os.makedirs(DOWNLOAD_DIR, exist_ok=True)


class BabesourceSpider:
    """
    Babesource çˆ¬è™«ç±»ï¼šè´Ÿè´£æœç´¢ã€è§£æã€æ•°æ®å­˜å‚¨å’Œå›¾ç‰‡ä¸‹è½½ã€‚
    æ”¯æŒ Redis/å†…å­˜å»é‡å’Œä¸‹è½½åŠŸèƒ½å¼€å…³ã€‚
    """
    def __init__(self, tag_file_path=TAG_FILE, csv_path=CSV_PATH, download_dir=DOWNLOAD_DIR):
        
        self.tag_file_path = tag_file_path
        self.csv_path = csv_path
        self.download_dir = download_dir
        self.csv_lock = Lock()  # ç”¨äº CSV å†™å…¥çš„çº¿ç¨‹é”
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # ç¦ç”¨ SSL è­¦å‘Šï¼Œå› ä¸ºä½¿ç”¨äº† verify=False
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # CSV å­—æ®µåï¼šè¯·æ³¨æ„ï¼Œæ­¤å¤„ä¿ç•™äº† 'ç›¸å†Œ' å’Œ 'æ‰€å±é›†åˆ' ä¸¤ä¸ªå­—æ®µä»¥ä¿æŒæ•°æ®çš„åˆ†ç±»ä¿¡æ¯
        self.csv_fieldnames = ['å›¾ç‰‡URL', 'æ ‡é¢˜', 'åç§°', 'ç›¸å†Œ', 'æ‰€å±é›†åˆ'] 

        # --- å»é‡åˆå§‹åŒ–é€»è¾‘ ---
        try:
            self.redis = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸï¼Œä½¿ç”¨ Redis é›†åˆè¿›è¡Œå»é‡ã€‚")
        except redis.exceptions.ConnectionError as e:
            print(f"âš ï¸ Redis è¿æ¥å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_urls = set()
        except Exception as e:
            print(f"âš ï¸ Redis åˆå§‹åŒ–é‡åˆ°å…¶ä»–é”™è¯¯ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_urls = set()
            
        # ç¡®ä¿ CSV æ–‡ä»¶å¤´éƒ¨å­˜åœ¨
        if not os.path.exists(self.csv_path) or os.path.getsize(self.csv_path) == 0:
            with open(self.csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=self.csv_fieldnames)
                writer.writeheader()


    def is_url_visited(self, url):
        """æ£€æŸ¥ URL æ˜¯å¦å·²è®¿é—® (å»é‡)"""
        if self.redis:
            # Redis: SADD è¿”å› 1 è¡¨ç¤ºæ·»åŠ æˆåŠŸï¼ˆæœªé‡å¤ï¼‰ï¼Œè¿”å› 0 è¡¨ç¤ºå·²å­˜åœ¨ï¼ˆé‡å¤ï¼‰
            return not self.redis.sadd(REDIS_KEY, url)
        else:
            # å†…å­˜å»é‡
            if url in self.visited_urls:
                return True
            self.visited_urls.add(url)
            return False

    def write_to_csv(self, data):
        """å°†æ•°æ®å†™å…¥ CSV æ–‡ä»¶ (çº¿ç¨‹å®‰å…¨)"""
        # åœ¨å†™å…¥å‰å…ˆè¿›è¡Œå»é‡æ£€æŸ¥
        if self.is_url_visited(data['å›¾ç‰‡URL']):
            return False
            
        with self.csv_lock:
            try:
                with open(self.csv_path, 'a', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=self.csv_fieldnames)
                    writer.writerow(data)
                return True
            except Exception as e:
                print(f"âŒ å†™å…¥ CSV å¤±è´¥: {e}")
                # å†™å…¥å¤±è´¥åˆ™ä»å»é‡é›†åˆä¸­ç§»é™¤
                if self.redis:
                    self.redis.srem(REDIS_KEY, data['å›¾ç‰‡URL'])
                else:
                    self.visited_urls.remove(data['å›¾ç‰‡URL'])
                return False

    def get_html(self, url):
        """å‘é€ HTTP è¯·æ±‚å¹¶è¿”å› BeautifulSoup å¯¹è±¡"""
        try:
            # verify=False å¿½ç•¥ SSL è¯ä¹¦éªŒè¯ï¼Œå› ä¸ºç½‘ç«™å¯èƒ½ä½¿ç”¨è‡ªç­¾åæˆ–æ—§åè®®
            response = requests.get(url, headers=self.headers, verify=False, timeout=10)
            response.raise_for_status() # æ£€æŸ¥ HTTP é”™è¯¯
            # ä½¿ç”¨ lxml è§£æå™¨æé«˜è§£æé€Ÿåº¦
            return BeautifulSoup(response.text, 'lxml') 
        except requests.exceptions.RequestException as e:
            # print(f"âŒ è¯·æ±‚å¤±è´¥: {url} -> {e}")
            return None

    # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šæœç´¢ç›¸å†Œåˆ—è¡¨ ---
    def scrape_album_list(self, tag_name):
        """
        æœç´¢äººåæ ‡ç­¾ï¼Œè·å–æ‰€æœ‰ç›¸å†Œè¯¦æƒ…é¡µ URL
        """
        album_urls = []
        page = 1
        print(f"\n--- ğŸ” å¼€å§‹çˆ¬å–æ ‡ç­¾: {tag_name} ---")
        
        while True:
            search_url = f"https://babesource.com/pornstars/{tag_name}/page{page}.html"
            soup = self.get_html(search_url)
            time.sleep(1) # ç¤¼è²Œæ€§å»¶è¿Ÿ

            if soup is None:
                break

            # æŸ¥æ‰¾ç›¸å†Œå¡ç‰‡å…ƒç´ 
            album_cards = soup.select('.main-content__card.tumba-card')

            # é€€å‡ºæ¡ä»¶ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›¸å†Œå¡ç‰‡ï¼Œåˆ™è®¤ä¸ºæ²¡æœ‰æ›´å¤šé¡µé¢
            if not album_cards:
                print(f"   -> ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°ç›¸å†Œå¡ç‰‡ï¼Œç»“æŸ {tag_name} çš„çˆ¬å–ã€‚")
                break
                
            print(f"   -> å‘ç°ç¬¬ {page} é¡µæœ‰ {len(album_cards)} ä¸ªç›¸å†Œã€‚")
                
            for card in album_cards:
                # è·å–ç›¸å†Œé“¾æ¥
                link_tag = card.select_one('.main-content__card-link')
                if link_tag and 'href' in link_tag.attrs:
                    album_url = link_tag['href']
                    album_urls.append(album_url)
            
            page += 1

        return list(set(album_urls))


    # --- ç¬¬äºŒéƒ¨åˆ†ï¼šè®¿é—®å›¾ç‰‡è¯¦æƒ…é¡µï¼Œè·å–å›¾ç‰‡ä¿¡æ¯ ---
    def scrape_image_details(self, album_url, tag_name):
        """
        è®¿é—®ç›¸å†Œè¯¦æƒ…é¡µï¼Œæå–å›¾ç‰‡ä¿¡æ¯å¹¶å†™å…¥ CSV
        """
        # ç›¸å†Œåç§°ï¼šä»URLä¸­æå–æœ€åä¸€ä¸ª '/' åçš„æ–‡æœ¬ï¼Œå»æ‰ .html
        album_name = album_url.split('/')[-1].replace('.html', '') 

        soup = self.get_html(album_url)
        time.sleep(0.5) # ç¤¼è²Œæ€§å»¶è¿Ÿ
        
        if soup is None:
            return

        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡å®¹å™¨ä¸­çš„åŸå§‹å›¾ç‰‡é“¾æ¥
        image_links = soup.select('.box-massage__tumba .box-massage__card-link')

        if not image_links:
            # print(f"   -> ç›¸å†Œä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡é“¾æ¥ï¼Œè·³è¿‡: {album_name}")
            return

        for link_tag in image_links:
            image_url = link_tag['href'] if 'href' in link_tag.attrs else None
            
            img_tag = link_tag.select_one('picture img')
            # æå– alt å±æ€§ä½œä¸ºæ ‡é¢˜ï¼Œå¦‚æœ alt ç¼ºå¤±åˆ™ä¸ºç©ºå­—ç¬¦ä¸²
            title = img_tag.get('alt', '').strip() if img_tag else '' 

            if image_url:
                # æå– å›¾ç‰‡åç§° (ä¸å¸¦æ‰©å±•å)
                file_with_ext = image_url.split('/')[-1]
                image_name = os.path.splitext(file_with_ext)[0]
                
                data = {
                    'å›¾ç‰‡URL': image_url,
                    'æ ‡é¢˜': title,
                    'åç§°': image_name,
                    'ç›¸å†Œ': album_name,       # ä¾‹å¦‚: nancy-heal-fit-190563
                    'æ‰€å±é›†åˆ': tag_name     # ä¾‹å¦‚: nancy-12284
                }
                
                # å†™å…¥ CSV (å·²åŒ…å«å»é‡é€»è¾‘)
                if self.write_to_csv(data):
                    print(f"   -> å†™å…¥æ•°æ®: {album_name}/{image_name} (URL: {image_url[:50]}...)")
            
            
    # --- ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¯åŠ¨ä¸‹è½½å›¾ç‰‡ ---
    def download_image(self, image_info):
        """
        ä¸‹è½½å•ä¸ªå›¾ç‰‡æ–‡ä»¶
        """
        url = image_info['å›¾ç‰‡URL']
        album_name = image_info['ç›¸å†Œ']
        image_name = image_info['åç§°']
        
        # æå–æ–‡ä»¶æ‰©å±•å
        file_ext = os.path.splitext(url.split('/')[-1])[1]
        
        # æ„é€ æœ¬åœ°ä¿å­˜è·¯å¾„: ç›¸å†Œåä½œä¸ºå­æ–‡ä»¶å¤¹å
        album_dir = os.path.join(self.download_dir, album_name)
        os.makedirs(album_dir, exist_ok=True) 
        
        # å®Œæ•´æ–‡ä»¶è·¯å¾„: .../images/ç›¸å†Œ/åç§°.æ‰©å±•å
        file_path = os.path.join(album_dir, f"{image_name}{file_ext}")
        
        if os.path.exists(file_path):
            return True # æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
        
        try:
            response = requests.get(url, headers=self.headers, stream=True, verify=False, timeout=20)
            response.raise_for_status()

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            print(f"âœ… ä¸‹è½½æˆåŠŸ: {album_name}/{image_name}{file_ext}")
            return True
        except requests.exceptions.RequestException as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥ ({album_name}/{image_name}): {e}")
            return False
        except Exception as e:
            print(f"âŒ å‘ç”Ÿå…¶ä»–é”™è¯¯ ({album_name}/{image_name}): {e}")
            return False


    def start_crawl(self, download_enabled=True):
        """
        çˆ¬è™«ä¸»å…¥å£ï¼šè¯»å–æ ‡ç­¾æ–‡ä»¶ï¼Œæ‰§è¡Œçˆ¬å–å’Œå¯é€‰çš„ä¸‹è½½ã€‚
        """
        print("--- å¯åŠ¨ Babesource çˆ¬è™« ---")
        tag_names = []
        
        # è¯»å–æ ‡ç­¾æ–‡ä»¶
        try:
            with open(self.tag_file_path, 'r', encoding='utf-8') as f:
                tag_names = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"âŒ æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°: {self.tag_file_path}")
            return

        if not tag_names:
            print("âš ï¸ æ ‡ç­¾æ–‡ä»¶ä¸ºç©ºï¼Œæ— ä»»åŠ¡å¯æ‰§è¡Œã€‚")
            return

        # 1. çˆ¬å–æ‰€æœ‰æ ‡ç­¾ä¸‹çš„ç›¸å†Œåˆ—è¡¨
        all_album_urls = []
        for tag in tag_names:
            urls = self.scrape_album_list(tag)
            # å­˜å‚¨ (ç›¸å†ŒURL, äººåæ ‡ç­¾) å¯¹
            all_album_urls.extend([(url, tag) for url in urls]) 
        
        print(f"\n--- âœ… å®Œæˆç›¸å†Œåˆ—è¡¨çˆ¬å–ï¼Œå…±å‘ç° {len(all_album_urls)} ä¸ªç›¸å†Œ ---")
        
        # 2. å¤šçº¿ç¨‹å¤„ç†ç›¸å†Œè¯¦æƒ…é¡µï¼Œæå–å›¾ç‰‡ä¿¡æ¯å¹¶å†™å…¥ CSV
        MAX_WORKERS = 5 
        print(f"--- ğŸš€ å¯åŠ¨ {MAX_WORKERS} çº¿ç¨‹å¤„ç†ç›¸å†Œè¯¦æƒ…é¡µ ---")
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(self.scrape_image_details, url, tag) 
                       for url, tag in all_album_urls]
            
            # ç­‰å¾…æ‰€æœ‰ç›¸å†Œå¤„ç†å®Œæˆ
            for i, future in enumerate(as_completed(futures)):
                pass 

        print("\n--- âœ… æ‰€æœ‰ç›¸å†Œè¯¦æƒ…é¡µä¿¡æ¯æå–å®Œæ¯•å¹¶å†™å…¥ CSV ---")

        # 3. å¯é€‰ï¼šå¯åŠ¨ä¸‹è½½ä»»åŠ¡
        if download_enabled:
            self.start_download() 
        else:
            print("--- ğŸš§ è·³è¿‡å›¾ç‰‡ä¸‹è½½ä»»åŠ¡ (download_enabled=False) ---")

    def start_download(self):
        """
        è¯»å– CSV æ–‡ä»¶ï¼Œå¯åŠ¨å›¾ç‰‡å¼‚æ­¥å¤šçº¿ç¨‹ä¸‹è½½
        """
        print("\n--- ğŸ“¥ å¯åŠ¨å›¾ç‰‡ä¸‹è½½ä»»åŠ¡ ---")
        
        image_list_to_download = []
        try:
            with open(self.csv_path, 'r', newline='', encoding='utf-8') as f:
                reader = csv.DictReader(f) # è‡ªåŠ¨ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºå­—æ®µå
                for row in reader:
                    # ä»…ä¸‹è½½é‚£äº› URL æ²¡æœ‰è¢«å»é‡è¿‡çš„ï¼ˆè™½ç„¶ç†è®ºä¸Šå†™å…¥æ—¶å·²ç»å»é‡ï¼Œè¿™é‡Œæ˜¯åŒé‡ä¿é™©ï¼‰
                    if not self.is_url_visited(row['å›¾ç‰‡URL']):
                        image_list_to_download.append(row)
        except FileNotFoundError:
            print(f"âŒ CSV æ–‡ä»¶æœªæ‰¾åˆ°: {self.csv_path}ã€‚è¯·å…ˆè¿è¡Œçˆ¬å–éƒ¨åˆ†ã€‚")
            return

        if not image_list_to_download:
            print("âš ï¸ CSV æ–‡ä»¶ä¸­æ²¡æœ‰æ–°çš„å›¾ç‰‡ä¿¡æ¯éœ€è¦ä¸‹è½½ã€‚")
            return
            
        print(f"--- å‡†å¤‡ä¸‹è½½ {len(image_list_to_download)} å¼ å›¾ç‰‡ ---")

        # ä½¿ç”¨å¤šçº¿ç¨‹è¿›è¡Œä¸‹è½½
        MAX_DOWNLOAD_WORKERS = 10 
        successful_downloads = 0
        total_downloads = len(image_list_to_download)
        
        with ThreadPoolExecutor(max_workers=MAX_DOWNLOAD_WORKERS) as executor:
            futures = [executor.submit(self.download_image, img_info) 
                       for img_info in image_list_to_download]
                       
            for i, future in enumerate(as_completed(futures)):
                if future.result():
                    successful_downloads += 1
                
        print(f"\n--- âœ… å›¾ç‰‡ä¸‹è½½ä»»åŠ¡å®Œæˆï¼æˆåŠŸä¸‹è½½ {successful_downloads}/{total_downloads} å¼ å›¾ç‰‡ ---")


if __name__ == '__main__':
    # --- å‡†å¤‡å·¥ä½œï¼šåˆ›å»ºç¤ºä¾‹æ ‡ç­¾æ–‡ä»¶ ---
    if not os.path.exists(TAG_FILE):
        print(f"ğŸ’¡ æ­£åœ¨åˆ›å»ºç¤ºä¾‹æ ‡ç­¾æ–‡ä»¶: {TAG_FILE}")
        os.makedirs(os.path.dirname(TAG_FILE), exist_ok=True)
        with open(TAG_FILE, 'w', encoding='utf-8') as f:
            f.write("nancy-12284\n")
            # f.write("å¦ä¸€ä¸ªæ ‡ç­¾-id\n") # å¯ä»¥åœ¨æ­¤æ·»åŠ æ›´å¤šäººåæ ‡ç­¾
            
    spider = BabesourceSpider()
    
    # --- è¿è¡Œæ¨¡å¼ç¤ºä¾‹ ---
    
    # ç¤ºä¾‹ 1: ä»…çˆ¬å–æ•°æ®åˆ° CSVï¼Œä¸ä¸‹è½½å›¾ç‰‡
    # print("\n--- æ¨¡å¼ä¸€ï¼šä»…çˆ¬å–æ•°æ®åˆ° CSV (download_enabled=False) ---")
    # spider.start_crawl(download_enabled=False)
    
    # ç¤ºä¾‹ 2: çˆ¬å–æ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡ (å¦‚æœéœ€è¦ï¼Œè¯·å–æ¶ˆä¸‹ä¸€è¡Œçš„æ³¨é‡Š)
    # print("\n--- æ¨¡å¼äºŒï¼šçˆ¬å–æ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡ (download_enabled=True) ---")
    # spider.start_crawl(download_enabled=True) 

    # ç¤ºä¾‹ 3: ä¹Ÿå¯ä»¥åœ¨æ•°æ®æ”¶é›†å®Œæˆåå•ç‹¬å¯åŠ¨ä¸‹è½½ä»»åŠ¡
    print("\n--- æ¨¡å¼ä¸‰ï¼šå•ç‹¬å¯åŠ¨ä¸‹è½½ä»»åŠ¡ ---")
    spider.start_download()