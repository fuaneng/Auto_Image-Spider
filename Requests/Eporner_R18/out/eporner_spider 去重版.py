import requests
from bs4 import BeautifulSoup
import csv
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- é…ç½®å¸¸é‡ ---
BASE_URL = "https://www.eporner.com"
SEARCH_URL_TEMPLATE = BASE_URL + "/search-photos/{person_name}/{page}/"

# ä½ çš„æœ¬åœ°è·¯å¾„é…ç½®
ROOT_PATH = r"R:\py\Auto_Image-Spider\Requests\Eporner_R18"
PERSON_TAGS_FILE = os.path.join(ROOT_PATH, "äººå.txt") 
CSV_PATH = os.path.join(ROOT_PATH, "image_data.csv") 
IMAGE_DIR = os.path.join(ROOT_PATH, "images")

# ç¡®ä¿æ–‡ä»¶å¤¹å’Œä¸‹è½½ç›®å½•å­˜åœ¨
os.makedirs(IMAGE_DIR, exist_ok=True)
os.makedirs(ROOT_PATH, exist_ok=True)

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# --- å·¥å…·å‡½æ•° ---

def get_html(url):
    """å‘é€GETè¯·æ±‚å¹¶è¿”å›å“åº”æ–‡æœ¬ï¼Œå¤„ç†å¸¸è§å¼‚å¸¸ã€‚ (å¢å¼ºç‰ˆï¼Œè¶…æ—¶è®¾ç½®ä¸º15ç§’)"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=15) 
        response.raise_for_status() 
        return response
    except requests.exceptions.HTTPError as e:
        if response is not None and response.status_code == 404:
            return None 
        print(f"  [ERROR] HTTP Error ({response.status_code}) for {url}: {e}")
        return None
    except requests.exceptions.Timeout:
        print(f"  [ERROR] Request Timeout for {url}: è¯·æ±‚è¶…æ—¶ã€‚")
        return None
    except requests.exceptions.RequestException as e:
        print(f"  [ERROR] Connection Error for {url}: ç½‘ç»œè¿æ¥å¼‚å¸¸ã€‚")
        return None

def read_person_tags(file_path):
    """ä»æŒ‡å®šçš„ .txt æ–‡ä»¶ä¸­è¯»å–äººç‰©æ ‡ç­¾åˆ—è¡¨"""
    tags = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                tag = line.strip()
                if tag and not tag.startswith('#'):
                    tags.append(tag)
        print(f"ğŸ“„ æˆåŠŸä» {file_path} ä¸­è¯»å–åˆ° {len(tags)} ä¸ªäººç‰©æ ‡ç­¾ã€‚")
    except FileNotFoundError:
        print(f"ğŸš¨ é”™è¯¯ï¼šæœªæ‰¾åˆ°äººåæ ‡ç­¾æ–‡ä»¶: {file_path}")
        print("è¯·åœ¨æŒ‡å®šè·¯å¾„åˆ›å»ºæ­¤æ–‡ä»¶ï¼Œå¹¶åœ¨å…¶ä¸­è¾“å…¥è¦æœç´¢çš„äººç‰©æ ‡ç­¾/IDï¼Œæ¯è¡Œä¸€ä¸ªã€‚")
    return tags

def extract_album_links(person_name):
    """
    ç¬¬ä¸€éƒ¨åˆ†ï¼šæœç´¢å›¾ç‰‡é›†åˆæ ‡ç­¾ï¼Œè·å–æ ‡ç­¾åˆ—è¡¨ (å†…éƒ¨ä½¿ç”¨ set è‡ªåŠ¨å»é‡)
    è¿”å›ï¼šåŒ…å«æ‰€æœ‰ç›¸å†Œè¯¦æƒ…é¡µå®Œæ•´URLçš„åˆ—è¡¨ã€‚
    """
    album_links = set()
    page = 1
    
    while True:
        search_url = SEARCH_URL_TEMPLATE.format(person_name=person_name, page=page)
        # print(f"  > æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...")

        response = get_html(search_url)
        if response is None:
            break

        soup = BeautifulSoup(response.text, 'lxml')
        
        album_tags = soup.select('#container.photosgrid a[id^="ah"]') 
        
        if not album_tags:
            break

        for tag in album_tags:
            href = tag.get('href')
            if href and href.startswith('/gallery/'):
                full_url = BASE_URL + href
                album_links.add(full_url)
        
        page += 1
    
    return list(album_links)


def process_album_page(album_url):
    """
    ç¬¬äºŒéƒ¨åˆ†ï¼šè®¿é—®å›¾ç‰‡è¯¦æƒ…é¡µï¼Œè·å–å›¾ç‰‡ä¿¡æ¯ã€‚
    è¿”å›ï¼šè¯¥ç›¸å†Œæ‰€æœ‰å›¾ç‰‡çš„è¯¦ç»†ä¿¡æ¯åˆ—è¡¨ã€‚
    """
    image_data_list = []
    
    response = get_html(album_url)
    if response is None:
        return image_data_list

    soup = BeautifulSoup(response.text, 'lxml')
    
    img_tags = soup.select('div.gallerygrid img[id^="t"]')
    
    for img_tag in img_tags:
        src_url = img_tag.get('src')
        alt_text = img_tag.get('alt', '')
        
        if not src_url:
            continue
            
        # è½¬æ¢ä¸ºå®Œæ•´å›¾ç‰‡URLï¼šå»é™¤ URL ä¸­æœ€åä¸€ä¸ª '_æ•°å­—xæ•°å­—' å½¢å¼çš„ç¼©ç•¥å›¾æ ‡è¯†ç¬¦
        full_image_url = re.sub(r'(_\d+x\d+)\.', '.', src_url)
        
        # æå–æ ‡é¢˜
        title = alt_text.replace('amateur photo', '').replace('porn photo', '').strip()
        
        # æå–åç§°
        name = re.sub(r'-\d+px$', '', title).strip() 

        image_data_list.append({
            'å›¾ç‰‡URL': full_image_url,
            'æ ‡é¢˜': title,
            'åç§°': name,
        })
        
    return image_data_list

# --- æ•°æ®æŒä¹…åŒ– ---
def save_to_csv(data_list, filename):
    """å°†æ•°æ®åˆ—è¡¨å†™å…¥CSVæ–‡ä»¶"""
    if not data_list:
        return
        
    fieldnames = ['å›¾ç‰‡URL', 'æ ‡é¢˜', 'åç§°']
    file_exists = os.path.exists(filename)
    
    try:
        with open(filename, 'a', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            if not file_exists or os.stat(filename).st_size == 0:
                writer.writeheader() 
                
            writer.writerows(data_list)
            
        print(f"ğŸ’¾ æˆåŠŸå°† {len(data_list)} æ¡æ•°æ®è¿½åŠ å†™å…¥åˆ° CSV æ–‡ä»¶ã€‚")
    except IOError as e:
        print(f"  [ERROR] å†™å…¥CSVæ–‡ä»¶å¤±è´¥: {e}")

# --- å¤šçº¿ç¨‹ä¸‹è½½ ---
def download_image(image_info):
    """
    æ ¹æ®å›¾ç‰‡ä¿¡æ¯ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜ã€‚(å†…éƒ¨åŒ…å«æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥)
    """
    url = image_info['å›¾ç‰‡URL']
    title = image_info['æ ‡é¢˜']
    
    ext_match = re.search(r'\.(\w+)$', url)
    extension = f".{ext_match.group(1)}" if ext_match else ".jpg" 

    safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
    
    filename = safe_title + extension
    filepath = os.path.join(IMAGE_DIR, filename)

    # æ£€æŸ¥æœ¬åœ°æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œé¿å…é‡å¤ä¸‹è½½
    if os.path.exists(filepath):
        return f"Skipped: {filename}"
        
    try:
        response = requests.get(url, headers=HEADERS, stream=True, timeout=30)
        response.raise_for_status()

        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
                
        return f"Downloaded: {filename}"
        
    except requests.exceptions.RequestException as e:
        return f"Error downloading {filename} from {url}: {e}"
        
def start_download_executor(all_data):
    """ä½¿ç”¨ThreadPoolExecutorå¯åŠ¨å¤šçº¿ç¨‹ä¸‹è½½"""
    if not all_data:
        print("æ²¡æœ‰å›¾ç‰‡æ•°æ®å¯ä¾›ä¸‹è½½ã€‚")
        return
        
    MAX_WORKERS = 10 
    
    success_count = 0
    error_count = 0
    
    print(f"\nâš¡ å¯åŠ¨ {len(all_data)} ä¸ªå¤šçº¿ç¨‹ä¸‹è½½ä»»åŠ¡...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_info = {executor.submit(download_image, item): item for item in all_data}
        
        for future in as_completed(future_to_info):
            try:
                result = future.result()
                if result.startswith("Downloaded"):
                    success_count += 1
                elif result.startswith("Error"):
                    error_count += 1
                # æ‰“å°è¿›åº¦
                # print(f"  {result}")
            except Exception as exc:
                print(f"  [EXCEPTION] ä»»åŠ¡æ‰§è¡Œæ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
                error_count += 1
                
    print(f"ğŸ‰ æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆï¼ æˆåŠŸ: {success_count}ï¼Œ å¤±è´¥/è·³è¿‡: {len(all_data) - success_count}ï¼Œ é”™è¯¯: {error_count}")


# --- ä¸»é€»è¾‘ ---
def main():
    # 1. è¯»å–æ‰€æœ‰ç›®æ ‡äººç‰©æ ‡ç­¾
    person_tags = read_person_tags(PERSON_TAGS_FILE)
    if not person_tags:
        return
        
    # **æ ¸å¿ƒå»é‡æ•°æ®ç»“æ„**ï¼šä½¿ç”¨å­—å…¸æ¥å­˜å‚¨æ‰€æœ‰å›¾ç‰‡çš„å”¯ä¸€æ•°æ®ï¼Œé”®ä¸º 'å›¾ç‰‡URL'
    all_unique_data_dict = {}

    # 2. éå†æ¯ä¸ªäººç‰©æ ‡ç­¾æ‰§è¡Œçˆ¬å–
    for i, tag in enumerate(person_tags):
        person_name = tag.strip()
        print(f"\n=======================================================")
        print(f"ğŸš€ [ä»»åŠ¡ {i+1}/{len(person_tags)}] å¼€å§‹å¤„ç†äººç‰©æ ‡ç­¾: '{person_name}'")
        print(f"=======================================================")

        # A. æå–ç›¸å†Œé“¾æ¥
        album_urls = extract_album_links(person_name)
        if not album_urls:
            print(f"âš ï¸ æœªæ‰¾åˆ°äººç‰© '{person_name}' çš„ä»»ä½•ç›¸å†Œï¼Œè·³è¿‡ã€‚")
            continue
        print(f"  æ‰¾åˆ° {len(album_urls)} ä¸ªç›¸å†Œã€‚")

        # B. éå†ç›¸å†Œï¼Œæå–å›¾ç‰‡ä¿¡æ¯
        for j, url in enumerate(album_urls):
            # print(f"    > æ­£åœ¨è§£æç›¸å†Œ [{j+1}/{len(album_urls)}]")
            data_list = process_album_page(url)
            
            # C. å®æ—¶æ·»åŠ åˆ°å…¨å±€å»é‡å­—å…¸ä¸­
            for item in data_list:
                # ä»¥å›¾ç‰‡URLä¸ºé”®ï¼Œä¿è¯å…¨å±€å”¯ä¸€æ€§
                all_unique_data_dict[item['å›¾ç‰‡URL']] = item
            
        print(f"ğŸŒŸ äººç‰© '{person_name}' çš„æ•°æ®æ”¶é›†å®Œæˆï¼Œå½“å‰æ€»è®¡ {len(all_unique_data_dict)} æ¡ç‹¬ç‰¹å›¾ç‰‡æ•°æ®ã€‚")
        
    # 3. æ•°æ®æŒä¹…åŒ–å’Œä¸‹è½½
    overall_unique_data = list(all_unique_data_dict.values()) # æœ€ç»ˆåˆ—è¡¨
        
    if not overall_unique_data:
        print("\næ‰€æœ‰äººç‰©éƒ½æ²¡æœ‰æå–åˆ°æ•°æ®ã€‚")
        return
        
    print(f"\n=======================================================")
    print(f"ğŸ“Š æœ€ç»ˆæ€»è®¡ï¼š {len(overall_unique_data)} æ¡å›¾ç‰‡æ•°æ®å‡†å¤‡å†™å…¥å’Œä¸‹è½½ã€‚")
    print(f"=======================================================")

    # D. å†™å…¥ CSV
    # ä¸ºäº†é¿å…å¤šæ¬¡è¿è¡Œé‡å¤å†™å…¥æ•°æ®ï¼Œå¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆæ¸…é™¤å®ƒæˆ–é‡‡å–æ›´å¤æ‚çš„æ›´æ–°é€»è¾‘ã€‚
    # è¿™é‡Œæˆ‘ä»¬ç»§ç»­ä½¿ç”¨è¿½åŠ æ¨¡å¼ï¼Œä½†æ•°æ®å·²ç»æ˜¯å»é‡åçš„ã€‚
    save_to_csv(overall_unique_data, CSV_PATH)
    
    # E. å¯åŠ¨å¤šçº¿ç¨‹ä¸‹è½½
    start_download_executor(overall_unique_data)

if __name__ == '__main__':
    main()