import requests
from bs4 import BeautifulSoup
import csv
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- é…ç½®å¸¸é‡ ---
BASE_URL = "https://www.eporner.com"
SEARCH_URL_TEMPLATE = BASE_URL + "/search-photos/{person_name}/{page}/"

# ä½ çš„æœ¬åœ°è·¯å¾„é…ç½®
ROOT_PATH = r"R:\py\Auto_Image-Spider\Requests\Eporner_R18"
PERSON_TAGS_FILE = os.path.join(ROOT_PATH, "äººå.txt") 
CSV_PATH = os.path.join(ROOT_PATH, "all_images_data.csv") 
IMAGE_DIR = os.path.join(ROOT_PATH, "images")

# âš ï¸ æ–°å¢ä¸‹è½½åŠŸèƒ½å¼€å…³
# è®¾ç½®ä¸º True: çˆ¬å–æ•°æ®åç«‹å³å¯åŠ¨å¤šçº¿ç¨‹ä¸‹è½½
# è®¾ç½®ä¸º False: ä»…çˆ¬å–æ•°æ®å¹¶å†™å…¥ CSVï¼Œä¸å¯åŠ¨ä¸‹è½½
ENABLE_DOWNLOAD = False 

# ç¡®ä¿æ–‡ä»¶å¤¹ç»“æ„å­˜åœ¨
os.makedirs(IMAGE_DIR, exist_ok=True)
os.makedirs(ROOT_PATH, exist_ok=True)

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# --- å·¥å…·å‡½æ•° (ç½‘ç»œè¯·æ±‚/æ–‡ä»¶è¯»å–) ---

def get_html(url):
    """å‘é€GETè¯·æ±‚å¹¶è¿”å›å“åº”æ–‡æœ¬ï¼Œå¤„ç†å¸¸è§å¼‚å¸¸ã€‚"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=15) 
        response.raise_for_status() 
        return response
    except requests.exceptions.HTTPError as e:
        if response is not None and response.status_code == 404:
            return None 
        print(f"  [ERROR] HTTP Error ({response.status_code}) for {url}: {e}")
        return None
    except requests.exceptions.Timeout:
        print(f"  [ERROR] Request Timeout for {url}: è¯·æ±‚è¶…æ—¶ã€‚")
        return None
    except requests.exceptions.RequestException as e:
        print(f"  [ERROR] Connection Error for {url}: ç½‘ç»œè¿æ¥å¼‚å¸¸ã€‚")
        return None

def read_person_tags(file_path):
    """ä»æŒ‡å®šçš„ .txt æ–‡ä»¶ä¸­è¯»å–äººç‰©æ ‡ç­¾åˆ—è¡¨"""
    tags = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                tag = line.strip()
                if tag and not tag.startswith('#'):
                    tags.append(tag)
        print(f"ğŸ“„ æˆåŠŸä» {file_path} ä¸­è¯»å–åˆ° {len(tags)} ä¸ªäººç‰©æ ‡ç­¾ã€‚")
    except FileNotFoundError:
        print(f"ğŸš¨ é”™è¯¯ï¼šæœªæ‰¾åˆ°äººåæ ‡ç­¾æ–‡ä»¶: {file_path}")
        return []
    return tags

# --- ç¬¬ä¸€éƒ¨åˆ†ï¼šæœç´¢ä¸ç›¸å†Œé“¾æ¥æå– ---

def extract_album_links(person_name):
    """
    æœç´¢å›¾ç‰‡é›†åˆæ ‡ç­¾ï¼Œè·å–é“¾æ¥å’Œæ‰€å±é›†åˆåç§°ã€‚
    è¿”å›ï¼šåŒ…å« [(ç›¸å†Œè¯¦æƒ…é¡µå®Œæ•´URL, æ‰€å±é›†åˆåç§°), ...] å…ƒç»„çš„åˆ—è¡¨ã€‚
    """
    album_data = {} 
    page = 1
    
    while True:
        search_url = SEARCH_URL_TEMPLATE.format(person_name=person_name, page=page)
        response = get_html(search_url)
        if response is None:
            break

        soup = BeautifulSoup(response.text, 'lxml')
        
        # å®šä½æ‰€æœ‰ç›¸å†Œçš„å®¹å™¨ div.mbphoto2
        album_containers = soup.select('#container.photosgrid div.mbphoto2')
        
        if not album_containers:
            break

        for container in album_containers:
            # 1. æå–é“¾æ¥ (a æ ‡ç­¾)
            a_tag = container.find('a', id=re.compile(r'^ah\d+'))
            # 2. æå–æ‰€å±é›†åˆæ ‡é¢˜ (div.mbtitphoto2 æ ‡ç­¾)
            title_tag = container.find('div', class_='mbtitphoto2')
            
            if a_tag and title_tag:
                href = a_tag.get('href')
                album_title = title_tag.get_text().strip()
                
                if href and href.startswith('/gallery/'):
                    full_url = BASE_URL + href
                    if full_url not in album_data:
                        album_data[full_url] = album_title
        
        page += 1
    
    return [(url, title) for url, title in album_data.items()]


# --- ç¬¬äºŒéƒ¨åˆ†ï¼šè®¿é—®è¯¦æƒ…é¡µï¼Œè·å–å›¾ç‰‡ä¿¡æ¯ ---

def process_album_page(album_url, album_collection_name):
    """
    è®¿é—®å›¾ç‰‡è¯¦æƒ…é¡µï¼Œè·å–å›¾ç‰‡ä¿¡æ¯ï¼Œå¹¶æ·»åŠ æ‰€å±é›†åˆå­—æ®µã€‚
    è¿”å›ï¼šè¯¥ç›¸å†Œæ‰€æœ‰å›¾ç‰‡çš„è¯¦ç»†ä¿¡æ¯åˆ—è¡¨ã€‚
    """
    image_data_list = []
    
    response = get_html(album_url)
    if response is None:
        return image_data_list

    soup = BeautifulSoup(response.text, 'lxml')
    
    img_tags = soup.select('div.gallerygrid img[id^="t"]')
    
    for img_tag in img_tags:
        src_url = img_tag.get('src')
        alt_text = img_tag.get('alt', '')
        
        if not src_url:
            continue
            
        # 1. è½¬æ¢ä¸ºå®Œæ•´å›¾ç‰‡URL (å»é™¤ _æ•°å­—xæ•°å­—)
        full_image_url = re.sub(r'(_\d+x\d+)\.', '.', src_url)
        
        # 2. æå–æ ‡é¢˜
        title = alt_text.replace('amateur photo', '').replace('porn photo', '').strip()
        
        # 3. æå–åç§° (å»é™¤æœ«å°¾çš„ -æ•°å­—px)
        name = re.sub(r'-\d+px$', '', title).strip() 

        image_data_list.append({
            'å›¾ç‰‡URL': full_image_url,
            'æ ‡é¢˜': title,
            'åç§°': name,
            'æ‰€å±é›†åˆ': album_collection_name, 
        })
        
    return image_data_list

# --- æ•°æ®æŒä¹…åŒ– ---
def save_to_csv(data_list, filename):
    """å°†æ•°æ®åˆ—è¡¨å†™å…¥CSVæ–‡ä»¶"""
    if not data_list:
        return
        
    fieldnames = ['å›¾ç‰‡URL', 'æ ‡é¢˜', 'åç§°', 'æ‰€å±é›†åˆ']
    file_exists = os.path.exists(filename)
    
    try:
        with open(filename, 'a', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            
            if not file_exists or os.stat(filename).st_size == 0:
                writer.writeheader() 
                
            writer.writerows(data_list)
            
        print(f"ğŸ’¾ æˆåŠŸå°† {len(data_list)} æ¡æ•°æ®è¿½åŠ å†™å…¥åˆ° CSV æ–‡ä»¶ã€‚")
    except IOError as e:
        print(f"  [ERROR] å†™å…¥CSVæ–‡ä»¶å¤±è´¥: {e}")

# --- ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¤šçº¿ç¨‹ä¸‹è½½ (åˆ†æ–‡ä»¶å¤¹å­˜å‚¨/æ–­ç‚¹ç»­ä¼ ) ---

def download_image(image_info):
    """
    æ ¹æ®å›¾ç‰‡ä¿¡æ¯ä¸‹è½½å›¾ç‰‡å¹¶ä¿å­˜åˆ°å¯¹åº”çš„å­æ–‡ä»¶å¤¹ä¸­ã€‚
    """
    url = image_info['å›¾ç‰‡URL']
    title = image_info['æ ‡é¢˜']
    collection_name = image_info['æ‰€å±é›†åˆ']
    
    ext_match = re.search(r'\.(\w+)$', url)
    extension = f".{ext_match.group(1)}" if ext_match else ".jpg" 

    safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
    
    # âš ï¸ å…³é”®ä¿®æ”¹ï¼šæ‰©å±•æ›¿æ¢è§„åˆ™ï¼Œå°† . å’Œ - ä¹Ÿæ›¿æ¢ä¸º _
    # åŸå§‹ç¦æ­¢å­—ç¬¦ï¼š[\\/:*?"<>|]
    # æ–°å¢æ›¿æ¢å­—ç¬¦ï¼š. å’Œ -
    safe_collection_name = re.sub(r'[\\/:*?"<>|.-]', '_', collection_name).strip() 
    
    sub_dir = os.path.join(IMAGE_DIR, safe_collection_name)
    os.makedirs(sub_dir, exist_ok=True) 

    filename = safe_title + extension
    filepath = os.path.join(sub_dir, filename) 

    if os.path.exists(filepath) and os.path.getsize(filepath) > 0:
        return f"Skipped (Exists in '{safe_collection_name}'): {filename}"
        
    try:
        response = requests.get(url, headers=HEADERS, stream=True, timeout=30)
        response.raise_for_status()

        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
                
        return f"Downloaded to '{safe_collection_name}': {filename}"
        
    except requests.exceptions.RequestException as e:
        return f"Error downloading {filename} to '{safe_collection_name}' from {url}: {e}"
        
def start_download_executor(all_data):
    """ä½¿ç”¨ThreadPoolExecutorå¯åŠ¨å¤šçº¿ç¨‹ä¸‹è½½"""
    if not all_data:
        print("æ²¡æœ‰å›¾ç‰‡æ•°æ®å¯ä¾›ä¸‹è½½ã€‚")
        return
        
    MAX_WORKERS = 10 
    success_count = 0
    error_count = 0
    total_tasks = len(all_data)
    
    print(f"\nâš¡ å¯åŠ¨ {total_tasks} ä¸ªå¤šçº¿ç¨‹ä¸‹è½½ä»»åŠ¡...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_info = {executor.submit(download_image, item): item for item in all_data}
        
        for future in as_completed(future_to_info):
            try:
                result = future.result()
                if result.startswith("Downloaded"):
                    success_count += 1
                elif result.startswith("Error"):
                    error_count += 1
            except Exception as exc:
                print(f"  [EXCEPTION] ä»»åŠ¡æ‰§è¡Œæ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
                error_count += 1
                
    print(f"ğŸ‰ æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆï¼ æˆåŠŸ: {success_count}ï¼Œ å¤±è´¥/è·³è¿‡: {len(all_data) - success_count}ï¼Œ é”™è¯¯: {error_count}")


# --- ä¸»é€»è¾‘ (æ–°å¢ä¸‹è½½å¼€å…³åˆ¤æ–­) ---
def main():
    person_tags = read_person_tags(PERSON_TAGS_FILE)
    if not person_tags:
        return
        
    all_unique_data_dict = {}

    for i, tag in enumerate(person_tags):
        person_name = tag.strip()
        print(f"\n=======================================================")
        print(f"ğŸš€ [ä»»åŠ¡ {i+1}/{len(person_tags)}] å¼€å§‹å¤„ç†äººç‰©æ ‡ç­¾: '{person_name}'")
        print(f"=======================================================")

        album_links_and_titles = extract_album_links(person_name)
        if not album_links_and_titles:
            print(f"âš ï¸ æœªæ‰¾åˆ°äººç‰© '{person_name}' çš„ä»»ä½•ç›¸å†Œï¼Œè·³è¿‡ã€‚")
            continue
        print(f"  æ‰¾åˆ° {len(album_links_and_titles)} ä¸ªç›¸å†Œã€‚")

        for j, (url, title) in enumerate(album_links_and_titles):
            data_list = process_album_page(url, title) 
            
            for item in data_list:
                all_unique_data_dict[item['å›¾ç‰‡URL']] = item
            
        print(f"ğŸŒŸ äººç‰© '{person_name}' çš„æ•°æ®æ”¶é›†å®Œæˆï¼Œå½“å‰æ€»è®¡ {len(all_unique_data_dict)} æ¡ç‹¬ç‰¹å›¾ç‰‡æ•°æ®ã€‚")
        
    overall_unique_data = list(all_unique_data_dict.values())
        
    if not overall_unique_data:
        print("\næ‰€æœ‰äººç‰©éƒ½æ²¡æœ‰æå–åˆ°æ•°æ®ã€‚")
        return
        
    print(f"\n=======================================================")
    print(f"ğŸ“Š æœ€ç»ˆæ€»è®¡ï¼š {len(overall_unique_data)} æ¡å›¾ç‰‡æ•°æ®å‡†å¤‡å†™å…¥ã€‚")
    print(f"=======================================================")

    # D. å†™å…¥ CSV
    save_to_csv(overall_unique_data, CSV_PATH)
    
    # E. å¯åŠ¨å¤šçº¿ç¨‹ä¸‹è½½ (æ–°å¢å¼€å…³åˆ¤æ–­)
    if ENABLE_DOWNLOAD:
        print("\nğŸ“¥ é…ç½®ï¼šä¸‹è½½åŠŸèƒ½å·²å¯ç”¨ã€‚")
        start_download_executor(overall_unique_data)
    else:
        print("\nâ¸ï¸ é…ç½®ï¼šä¸‹è½½åŠŸèƒ½å·²ç¦ç”¨ã€‚å›¾ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ° CSVï¼Œè¯·ç¨åè¿è¡Œç‹¬ç«‹ä¸‹è½½è„šæœ¬ã€‚")

if __name__ == '__main__':
    main()