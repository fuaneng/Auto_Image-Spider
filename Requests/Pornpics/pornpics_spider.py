import os
import re
import csv
import time
import requests
import urllib3
import redis
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from bs4 import BeautifulSoup

# --- é…ç½®å¸¸é‡ ---
BASE_SEARCH_URL = "https://www.pornpics.com/search/srch.php"
BASE_GALLERY_URL = "https://www.pornpics.com"
MAX_WORKERS_CRAWL = 5  # çˆ¬å–çº¿ç¨‹æ•°
MAX_WORKERS_DOWNLOAD = 10 # ä¸‹è½½çº¿ç¨‹æ•°

# æ–‡ä»¶è·¯å¾„é…ç½®
TAG_FILE_PATH = r"R:\py\Auto_Image-Spider\Requests\Pornpics\äººåtag.txt"
CSV_DIR_PATH = r"R:\py\Auto_Image-Spider\Requests\Pornpics"
CSV_FILENAME = "all_images_data.csv"
DOWNLOAD_DIR_PATH = r"R:\py\Auto_Image-Spider\Requests\Pornpics\images"

# Redis é…ç½®
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_KEY = 'pornpics_image_url_set' # ä½¿ç”¨ å›¾ç‰‡ URL ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦

class PornpicsSpider:
    """
    Pornpics çˆ¬è™«ç±»ï¼Œé›†æˆäº†æœç´¢ã€è¯¦æƒ…é¡µè§£æã€CSV å­˜å‚¨ã€Redis/å†…å­˜å»é‡å’Œå¼‚æ­¥ä¸‹è½½åŠŸèƒ½ã€‚
    """
    def __init__(self, csv_dir_path, csv_filename, redis_host=REDIS_HOST, redis_port=REDIS_PORT):
        """
        åˆå§‹åŒ–çˆ¬è™«å®ä¾‹ï¼Œé›†æˆ Redis/å†…å­˜å»é‡é€»è¾‘ã€‚
        """
        self.csv_dir_path = csv_dir_path
        self.csv_path = os.path.join(self.csv_dir_path, csv_filename)
        self.csv_lock = Lock() # ç”¨äº CSV å†™å…¥çš„çº¿ç¨‹é”
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.pornpics.com/' # æœ‰æ—¶ Referer æ˜¯å¿…é¡»çš„
        }
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        self._setup_redis_duplication(redis_host, redis_port)
        self._ensure_csv_headers()

    def _setup_redis_duplication(self, redis_host, redis_port):
        """ åˆå§‹åŒ– Redis è¿æ¥æˆ–å›é€€åˆ°å†…å­˜å»é‡ """
        try:
            # å°è¯•è¿æ¥ Redis
            self.redis = redis.StrictRedis(host=redis_host, port=redis_port, decode_responses=True, socket_connect_timeout=5)
            # å°è¯•æ‰§è¡Œä¸€æ¬¡ ping æ¥éªŒè¯è¿æ¥
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸï¼Œä½¿ç”¨ Redis é›†åˆè¿›è¡Œå»é‡ã€‚")
        except redis.exceptions.ConnectionError as e:
            print(f"âš ï¸ Redis è¿æ¥å¤±è´¥ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            # å†…å­˜å»é‡é›†åˆï¼Œç”¨äºåœ¨å½“å‰ç¨‹åºç”Ÿå‘½å‘¨æœŸå†…çš„å»é‡
            self.visited_urls = set()
        except Exception as e:
            print(f"âš ï¸ Redis åˆå§‹åŒ–é‡åˆ°å…¶ä»–é”™è¯¯ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_urls = set()

    def _ensure_csv_headers(self):
        """ ç¡®ä¿ CSV æ–‡ä»¶åŠå…¶è¡¨å¤´å­˜åœ¨ """
        fieldnames = ['æ ‡é¢˜', 'å›¾ç‰‡åç§°', 'å›¾ç‰‡URL', 'æ‰€å±ç›¸å†Œ', 'äººåTagæ ‡ç­¾']
        if not os.path.exists(self.csv_path):
            os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)
            with open(self.csv_path, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
            print(f"ğŸ“„ å·²åˆ›å»º CSV æ–‡ä»¶ï¼š{self.csv_path} å¹¶å†™å…¥è¡¨å¤´ã€‚")

    def _is_url_visited(self, image_url):
        """ æ£€æŸ¥ URL æ˜¯å¦å·²è¢«è®¿é—®ï¼ˆå»é‡ï¼‰ """
        if self.redis:
            # ä½¿ç”¨ Redis çš„ sadd æ–¹æ³•ï¼Œå¦‚æœå…ƒç´ å·²å­˜åœ¨ï¼Œè¿”å› 0
            return self.redis.sadd(REDIS_KEY, image_url) == 0
        else:
            # å†…å­˜å»é‡
            if image_url in self.visited_urls:
                return True
            self.visited_urls.add(image_url)
            return False

    def _read_tags(self, file_path):
        """ ä»æ–‡ä»¶è¯»å–äººå Tag åˆ—è¡¨ """
        if not os.path.exists(file_path):
            print(f"âŒ é”™è¯¯ï¼šTag æ–‡ä»¶ä¸å­˜åœ¨äº {file_path}")
            return []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                # è¿‡æ»¤ç©ºè¡Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
                return [tag.strip() for tag in f if tag.strip()]
        except Exception as e:
            print(f"âŒ è¯»å– Tag æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []

    # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šæœç´¢ç›¸å†Œåˆ—è¡¨ ---
    def _fetch_gallery_list(self, tag):
        """ æœç´¢ç‰¹å®š Tag çš„æ‰€æœ‰ç›¸å†Œï¼Œå¹¶è¿”å›ç›¸å†ŒURLå’Œæ ‡é¢˜åˆ—è¡¨ """
        galleries = []
        offset = 0
        tag_encoded = tag.replace(' ', '+') # æ›¿æ¢ç©ºæ ¼ä¸º '+'

        while True:
            search_params = {
                'q': tag_encoded,
                'lang': 'zh',
                'limit': 20,
                'offset': offset
            }
            search_url = f"{BASE_SEARCH_URL}?q={tag_encoded}&lang=zh&limit=20&offset={offset}"
            print(f"ğŸ” æ­£åœ¨æœç´¢ Tag: {tag} - é¡µç  Offset: {offset}")

            try:
                response = requests.get(search_url, headers=self.headers, verify=False, timeout=15)
                response.raise_for_status()
                data = response.json()
            except requests.exceptions.RequestException as e:
                print(f"âŒ æœç´¢è¯·æ±‚å¤±è´¥: {search_url} - é”™è¯¯: {e}")
                break
            except ValueError:
                print(f"âŒ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„ JSON: {search_url}")
                break
            
            # é€’è¿›é¡µç å¦‚æœæ²¡æœ‰ç›¸å†ŒåŠ è½½ï¼Œè¿™è¡¨ç¤ºæ²¡æœ‰æ›´å¤šé¡µç 
            if not data:
                print(f"âœ… Tag: {tag} - ç›¸å†Œåˆ—è¡¨çˆ¬å–å®Œæˆã€‚")
                break

            for item in data:
                gallery_url = item.get('g_url')
                gallery_title = item.get('desc')
                if gallery_url and gallery_title:
                    # ç¡®ä¿æ˜¯å®Œæ•´çš„ URL
                    if not gallery_url.startswith('http'):
                        gallery_url = BASE_GALLERY_URL + gallery_url
                    galleries.append({
                        'gallery_url': gallery_url,
                        'gallery_title': gallery_title,
                        'tag': tag
                    })
            
            offset += 20
            time.sleep(1) # ç¤¼è²Œæ€§ç­‰å¾…

        return galleries

    # --- ç¬¬äºŒéƒ¨åˆ†ï¼šè®¿é—®è¯¦æƒ…é¡µï¼Œè·å–å›¾ç‰‡ä¿¡æ¯å¹¶å†™å…¥è¡¨æ ¼ ---
    def _parse_gallery_page(self, gallery_info):
        """ è®¿é—®å•ä¸ªç›¸å†Œè¯¦æƒ…é¡µï¼Œæå–å›¾ç‰‡ä¿¡æ¯å¹¶å†™å…¥ CSV """
        gallery_url = gallery_info['gallery_url']
        gallery_title = gallery_info['gallery_title']
        tag = gallery_info['tag']
        
        print(f"ğŸ–¼ï¸ æ­£åœ¨è§£æç›¸å†Œ: {gallery_title} ({gallery_url})")

        try:
            response = requests.get(gallery_url, headers=self.headers, verify=False, timeout=15)
            response.raise_for_status()
            response.encoding = 'utf-8' # ç¡®ä¿ä¸­æ–‡æ ‡é¢˜æ­£ç¡®è§£æ
            soup = BeautifulSoup(response.text, 'html.parser')
        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯¦æƒ…é¡µè¯·æ±‚å¤±è´¥: {gallery_url} - é”™è¯¯: {e}")
            return
        
        # å®šä½åˆ°åŒ…å«å›¾ç‰‡åˆ—è¡¨çš„ ul å…ƒç´ 
        tiles_ul = soup.find('ul', id='tiles')
        if not tiles_ul:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç›¸å†Œå›¾ç‰‡åˆ—è¡¨: {gallery_url}")
            return

        image_data_list = []
        # æŸ¥æ‰¾æ‰€æœ‰ li.thumbwook ä¸‹çš„ a.rel-link å…ƒç´ 
        for a_tag in tiles_ul.select('li.thumbwook a.rel-link'):
            image_url = a_tag.get('href') # å›¾ç‰‡åŸå›¾ URL
            img_tag = a_tag.find('img')
            
            if image_url and img_tag:
                image_title = img_tag.get('alt', '').strip() # å›¾ç‰‡æ ‡é¢˜
                
                # ä»å›¾ç‰‡ URL ä¸­æå–å›¾ç‰‡åç§°
                match = re.search(r'[^/]+$', image_url)
                if match:
                    full_filename = match.group(0)
                    # å»æ‰æ‰©å±•åå³ä¸ºåç§°
                    image_name, _ = os.path.splitext(full_filename) 
                else:
                    image_name = 'unknown' # æ— æ³•æå–æ—¶ä½¿ç”¨é»˜è®¤å€¼

                data_row = {
                    'æ ‡é¢˜': image_title,
                    'å›¾ç‰‡åç§°': image_name,
                    'å›¾ç‰‡URL': image_url,
                    'æ‰€å±ç›¸å†Œ': gallery_title,
                    'äººåTagæ ‡ç­¾': tag
                }

                # æ£€æŸ¥ URL æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰
                if not self._is_url_visited(image_url):
                    image_data_list.append(data_row)
                    print(f"   [+] æ”¶é›†å›¾ç‰‡: {image_name}")
                else:
                    print(f"   [-] è·³è¿‡é‡å¤å›¾ç‰‡: {image_name}")


        # æ‰¹é‡å†™å…¥ CSV
        if image_data_list:
            self._write_to_csv(image_data_list)
            print(f"âœ… ç›¸å†Œ {gallery_title} çš„ {len(image_data_list)} å¼ å›¾ç‰‡ä¿¡æ¯å·²å†™å…¥ CSVã€‚")
        else:
            print(f"â„¹ï¸ ç›¸å†Œ {gallery_title} ä¸­æ²¡æœ‰æ–°çš„å›¾ç‰‡æ•°æ®å†™å…¥ã€‚")

    def _write_to_csv(self, data_list):
        """ çº¿ç¨‹å®‰å…¨åœ°å°†æ•°æ®å†™å…¥ CSV æ–‡ä»¶ """
        fieldnames = ['æ ‡é¢˜', 'å›¾ç‰‡åç§°', 'å›¾ç‰‡URL', 'æ‰€å±ç›¸å†Œ', 'äººåTagæ ‡ç­¾']
        with self.csv_lock: # ä½¿ç”¨çº¿ç¨‹é”
            with open(self.csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writerows(data_list)

    def start_crawl(self, download_enabled=False):
        """ 
        å¯åŠ¨çˆ¬è™«ä»»åŠ¡ï¼Œæ§åˆ¶æ•°æ®æ”¶é›†å’Œå¯é€‰çš„ä¸‹è½½ã€‚
        :param download_enabled: bool, æ˜¯å¦åœ¨æ•°æ®æ”¶é›†åç«‹å³å¯åŠ¨ä¸‹è½½ã€‚
        """
        print("\n--- ğŸŒ çˆ¬è™«æ•°æ®æ”¶é›†é˜¶æ®µå¼€å§‹ ---")
        tags = self._read_tags(TAG_FILE_PATH)
        if not tags:
            print("âŒ çˆ¬è™«ç»ˆæ­¢ï¼šæœªæ‰¾åˆ°æˆ–æ— æ³•è¯»å–äººå Tagã€‚")
            return

        all_galleries = []
        # 1. è·å–æ‰€æœ‰ Tag çš„ç›¸å†Œåˆ—è¡¨ (åŒæ­¥æ“ä½œï¼Œé¿å…å¹¶å‘å¼•èµ· offset æ··ä¹±)
        for tag in tags:
            galleries = self._fetch_gallery_list(tag)
            all_galleries.extend(galleries)
            time.sleep(2) # æœç´¢ Tag ä¹‹é—´ç¨ä½œåœé¡¿

        if not all_galleries:
            print("âŒ çˆ¬è™«ç»ˆæ­¢ï¼šæœªæ‰¾åˆ°ä»»ä½•ç›¸å†Œã€‚")
            return

        print(f"âœ… æ€»å…±æ‰¾åˆ° {len(all_galleries)} ä¸ªç›¸å†Œï¼Œå¯åŠ¨å¹¶å‘è§£æã€‚")

        # 2. å¹¶å‘è®¿é—®ç›¸å†Œè¯¦æƒ…é¡µå¹¶å†™å…¥ CSV
        with ThreadPoolExecutor(max_workers=MAX_WORKERS_CRAWL) as executor:
            executor.map(self._parse_gallery_page, all_galleries)

        print("\n--- âœ… çˆ¬è™«æ•°æ®æ”¶é›†é˜¶æ®µå®Œæˆ ---")

        if download_enabled:
            print("\n--- â¬‡ï¸ ç«‹å³å¯åŠ¨å›¾ç‰‡ä¸‹è½½ä»»åŠ¡ ---")
            self.start_download()

    # --- ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¯åŠ¨ä¸‹è½½å›¾ç‰‡ ---
    def start_download(self):
        """ ä» CSV æ–‡ä»¶è¯»å–æ•°æ®ï¼Œå¯åŠ¨è§£è€¦å¼å¼‚æ­¥å¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡ """
        if not os.path.exists(self.csv_path):
            print(f"âŒ ä¸‹è½½ç»ˆæ­¢ï¼šCSV æ–‡ä»¶ä¸å­˜åœ¨äº {self.csv_path}")
            return

        print(f"â¬‡ï¸ æ­£åœ¨ä» {self.csv_path} è¯»å–ä¸‹è½½ä»»åŠ¡...")
        
        download_tasks = []
        try:
            with open(self.csv_path, 'r', newline='', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    # å‡†å¤‡ä¸‹è½½æ‰€éœ€ä¿¡æ¯
                    download_tasks.append({
                        'image_url': row['å›¾ç‰‡URL'],
                        'album_name': row['æ‰€å±ç›¸å†Œ'],
                        'filename': row['å›¾ç‰‡åç§°'],
                        'tag': row['äººåTagæ ‡ç­¾']
                    })
        except Exception as e:
            print(f"âŒ è¯»å– CSV æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return

        if not download_tasks:
            print("â„¹ï¸ CSV æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸‹è½½ä»»åŠ¡ã€‚")
            return

        print(f"ğŸš€ å¯åŠ¨ {len(download_tasks)} ä¸ªå›¾ç‰‡çš„ä¸‹è½½ä»»åŠ¡ï¼Œä½¿ç”¨ {MAX_WORKERS_DOWNLOAD} çº¿ç¨‹ã€‚")

        # ä½¿ç”¨å¤šçº¿ç¨‹æ‰§è¡Œä¸‹è½½ä»»åŠ¡
        with ThreadPoolExecutor(max_workers=MAX_WORKERS_DOWNLOAD) as executor:
            executor.map(self._download_image_task, download_tasks)

        print("\n--- âœ… æ‰€æœ‰å›¾ç‰‡ä¸‹è½½ä»»åŠ¡å®Œæˆ ---")


    def _download_image_task(self, task):
        """ å•ä¸ªå›¾ç‰‡çš„ä¸‹è½½é€»è¾‘ """
        image_url = task['image_url']
        album_name = task['album_name']
        filename = task['filename']
        
        # æå–æ–‡ä»¶æ‰©å±•åï¼Œç”¨äºä¿ç•™åŸå§‹æ ¼å¼
        ext = os.path.splitext(os.path.basename(image_url))[-1]
        
        # æ„å»ºå­æ–‡ä»¶å¤¹è·¯å¾„ (ä½¿ç”¨ç›¸å†Œå­—æ®µä½œä¸ºå­æ–‡ä»¶å¤¹å)
        # æ¸…ç†ç›¸å†Œåä¸­çš„éæ³•å­—ç¬¦ï¼Œä»¥é˜²åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥
        safe_album_name = re.sub(r'[\\/:*?"<>|]', '_', album_name) 
        sub_dir = os.path.join(DOWNLOAD_DIR_PATH, safe_album_name)
        os.makedirs(sub_dir, exist_ok=True)
        
        # å®Œæ•´çš„æ–‡ä»¶è·¯å¾„ (ä½¿ç”¨åç§°å­—æ®µä½œä¸ºæ–‡ä»¶åï¼Œä¿ç•™æ‰©å±•å)
        file_path = os.path.join(sub_dir, f"{filename}{ext}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆäºŒæ¬¡å»é‡/æ–­ç‚¹ç»­ä¼ ç®€å•å®ç°ï¼‰
        if os.path.exists(file_path):
            # print(f"   [SKIP] æ–‡ä»¶å·²å­˜åœ¨: {file_path}")
            return

        try:
            # æµå¼ä¸‹è½½å›¾ç‰‡
            response = requests.get(image_url, headers=self.headers, verify=False, stream=True, timeout=30)
            response.raise_for_status()

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            print(f"   [SUCCESS] ä¸‹è½½å®Œæˆ: {file_path}")

        except requests.exceptions.RequestException as e:
            print(f"   [ERROR] ä¸‹è½½å¤±è´¥: {image_url} - é”™è¯¯: {e}")
        except Exception as e:
            print(f"   [ERROR] å¤„ç†ä¸‹è½½æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


if __name__ == '__main__':
    # ç¡®ä¿ä¸‹è½½ç›®å½•å­˜åœ¨
    os.makedirs(DOWNLOAD_DIR_PATH, exist_ok=True)
    
    # å®ä¾‹åŒ–çˆ¬è™«
    spider = PornpicsSpider(
        csv_dir_path=CSV_DIR_PATH, 
        csv_filename=CSV_FILENAME
    )

    # --- è¿è¡Œæ¨¡å¼ç¤ºä¾‹ ---
    
    # ç¤ºä¾‹ 1: ä»…çˆ¬å–æ•°æ®åˆ° CSVï¼Œä¸ä¸‹è½½å›¾ç‰‡
    print("\n--- æ¨¡å¼ä¸€ï¼šä»…çˆ¬å–æ•°æ®åˆ° CSV (download_enabled=False) ---")
    spider.start_crawl(download_enabled=False)
    
    # # ç¤ºä¾‹ 2: çˆ¬å–æ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡ (å¦‚æœéœ€è¦ï¼Œè¯·å–æ¶ˆä¸‹ä¸€è¡Œçš„æ³¨é‡Š)
    # print("\n--- æ¨¡å¼äºŒï¼šçˆ¬å–æ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡ (download_enabled=True) ---")
    # spider.start_crawl(download_enabled=True) 

    # ç¤ºä¾‹ 3: ä¹Ÿå¯ä»¥åœ¨æ•°æ®æ”¶é›†å®Œæˆåå•ç‹¬å¯åŠ¨ä¸‹è½½ä»»åŠ¡
    # print("\n--- æ¨¡å¼ä¸‰ï¼šå•ç‹¬å¯åŠ¨ä¸‹è½½ä»»åŠ¡ ---")
    # spider.start_download()