# Wallhalla 高级图片爬虫

这是一个针对 Wallhalla.com 网站的 Python 爬虫项目，旨在自动搜索、下载高清壁纸并将其分类保存。

本项目的核心特点是**通过模拟真实用户行为来应对网站的反爬虫机制**。对于那些无法通过直接请求下载链接、必须依赖用户交互（如点击）才能获取资源的网站，本项目提供了一个完整且高效的解决方案。

## 核心挑战与反爬策略分析

许多现代网站为了防止被自动化程序抓取，采取了复杂的反爬虫措施。本项目针对的网站就存在以下典型挑战：

* **动态内容加载**：搜索结果通过“无限滚动”的方式动态加载，无法一次性获取所有内容。
* **隐藏的下载链接**：无法从搜索结果页的 HTML 中直接构造出原始图片的下载链接。
* **基于用户交互的下载**：真正的下载行为必须在图片的详情页通过点击一个特定的“下载”按钮来触发。服务器很可能会验证请求是否源于一次真实的用户点击事件，而不是一个直接的 URL 访问。

### 我们的解决方案：模拟真实用户点击

为了克服这些障碍，本爬虫不依赖传统的 HTTP 请求库（如 `requests`），而是完全基于 `Selenium WebDriver` 来驱动一个真实的浏览器实例，精细地模仿人类用户的操作流程：

1.  **自动化搜索**：程序自动在首页输入指定的关键词并执行搜索。
2.  **模拟滚动**：在搜索结果页，程序会自动向下滚动页面，直到所有图片都加载完毕，确保数据完整性。
3.  **在新标签页中导航**：程序会获取每个图片详情页的链接，然后像用户一样 **“在新标签页中打开链接”**。这确保了主搜索页面的状态得以保留，操作环境也更接近真实使用场景。
4.  **定位并点击下载按钮**：在详情页中，程序会精确定位到真正的下载按钮元素，并执行 `click()` 操作。
5.  **触发浏览器原生下载**：这次点击会触发浏览器自身的下载管理器来处理文件下载。我们将下载任务完全交给浏览器，这不仅成功绕过了反爬验证，还非常稳定和高效。
6.  **动态路径管理**：通过配置 `Chrome Options`，我们可以预设好下载路径，使得每个标签的图片都能自动保存到对应的子文件夹中，无需任何人工干预。

通过这一系列操作，爬虫的行为在服务器看来与一个普通用户几乎没有区别，从而成功绕过了反爬限制。

## 主要功能

* **关键词批量处理**：从 `txt` 文件中读取一个标签列表，并为每个标签执行一次完整的抓取任务。
* **自动化文件整理**：为每个搜索标签创建独立的文件夹，并将下载的图片自动保存到其中。
* **增量爬取与去重**：利用 Redis (或内存 set) 对图片的下载链接进行 MD5 哈希计算，避免重复下载，支持断点续传。
* **详细日志记录**：将所有成功抓取的图片元数据（标题、文件名、URL、标签）保存到一个统一的 CSV 文件中，便于管理和追溯。
* **性能优化**：在浏览器中禁用图片加载，大幅减少了页面加载时间和内存占用，显著提升了爬取效率。

## 环境准备

在运行此脚本之前，请确保你已安装以下软件和库：

1.  **Python 3.x**
2.  **Google Chrome 浏览器**
3.  **ChromeDriver**：
    * **重要**：ChromeDriver 的版本必须与你安装的 Google Chrome 浏览器版本完全匹配。
    * 下载地址：[https://googlechromelabs.github.io/chrome-for-testing/](https://googlechromelabs.github.io/chrome-for-testing/)
4.  **Redis (可选)**：
    * 如果需要持久化的去重记录，建议安装并运行 Redis。否则，程序将使用内存进行临时去重。
5.  **Python 依赖库**：
    ```bash
    pip install selenium redis
    ```

## 如何使用

1.  **克隆或下载项目**：
    将代码保存为 `wallhalla_spider.py`。

2.  **准备标签文件**：
    创建一个 `txt` 文件（例如 `tag_list.txt`），每行包含一个你想要搜索的关键词。例如：
    ```txt
    nature
    landscape
    city
    abstract
    dog
    ```

3.  **配置脚本参数**：
    打开 `wallhalla_spider.py` 文件，找到文件底部的 `if __name__ == '__main__':` 部分，根据你的环境修改以下四个变量：

    ```python
    if __name__ == '__main__':
        # --- 请在这里配置你的路径 ---
        
        # 1. ChromeDriver 的完整路径
        CHROME_DRIVER_PATH = r'C:\Program Files\Google\chromedriver-win32\chromedriver.exe'
        
        # 2. 所有数据（图片和CSV）的根保存目录
        SAVE_DIRECTORY = r"\\10.58.134.120\aigc2\01_数据\爬虫数据\wallhalla" 
        
        # 3. 你的标签文件路径
        TAG_FILE_PATH = r'D:\work\爬虫\ram_tag_list.txt'
    ```

4.  **运行爬虫**：
    打开终端或命令行，切换到脚本所在目录，然后执行：
    ```bash
    python wallhalla_spider.py
    ```

程序将开始自动执行，你可以看到控制台输出详细的执行过程。

## 代码结构说明

`WallhallaSpider` 类包含了爬虫的所有逻辑：

* `__init__(...)`：初始化所有配置，包括路径、Redis 连接等。
* `setup_driver(...)`：**核心方法之一**。负责为每个标签创建独立的浏览器实例，并动态配置下载路径和禁用图片加载。
* `is_duplicate(...)`：处理去重逻辑。
* `write_to_csv(...)`：将图片信息写入 CSV 文件。
* `perform_search(...)`：模拟搜索操作，并判断搜索结果是否存在。
* `get_images_and_download(...)`：**核心方法之二**。负责滚动页面、获取所有详情页链接，并在新标签页中逐一打开、点击下载。
* `main()`：主执行函数，负责读取标签列表，并循环调用上述方法来完成整个爬取任务。
* `close()`：安全地关闭浏览器实例，释放资源。

---