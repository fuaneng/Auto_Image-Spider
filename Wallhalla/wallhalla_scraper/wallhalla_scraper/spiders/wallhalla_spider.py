import scrapy
import os
from urllib.parse import urljoin
# å‡è®¾ä½ çš„ items.py è·¯å¾„æ˜¯æ­£ç¡®çš„
from ..items import WallhallaItem 

class WallhallaSpider(scrapy.Spider):
    """
    WallhallaSpider: ç”¨äºçˆ¬å– wallhalla.com ç½‘ç«™çš„å£çº¸ä¿¡æ¯ã€‚
    é€šè¿‡æ¨¡æ‹Ÿè®¿é—®é¦–é¡µå¹¶å¸¦ Referer å‘èµ·æœç´¢è¯·æ±‚ï¼Œè§£å†³ç½‘ç«™é‡å®šå‘å’Œåˆå§‹è¯·æ±‚å»é‡é—®é¢˜ã€‚
    """
    name = 'wallhalla'
    # å…è®¸çš„åŸŸåå¿…é¡»åŒ…å«å¸¦ www. å’Œä¸å¸¦ www. çš„ç‰ˆæœ¬
    allowed_domains = ['www.wallhalla.com', 'wallhalla.com'] 
    # base_url è®¾ä¸ºç½‘ç«™æœ€ç»ˆé‡å®šå‘çš„ç›®æ ‡åŸŸå
    base_url = 'https://wallhalla.com'  
    
    # ---------------------- æ­¥éª¤ 1: è¯·æ±‚é¦–é¡µ ----------------------
    def start_requests(self):
        """
        è¯»å–æ ‡ç­¾æ–‡ä»¶ï¼Œå¹¶ä¸ºæ¯ä¸ªæ ‡ç­¾ç”Ÿæˆä¸€ä¸ªå¯¹ç½‘ç«™é¦–é¡µçš„è¯·æ±‚ã€‚
        """
        tag_file_path = self.settings.get('TAG_FILE_PATH')
        
        tags = [] 
        
        if not tag_file_path or not os.path.exists(tag_file_path):
            self.logger.error(f"âŒ æ‰¾ä¸åˆ°æ ‡ç­¾æ–‡ä»¶: {tag_file_path}")
            return
        
        try:
            with open(tag_file_path, 'r', encoding='utf-8') as f:
                tags = [line.strip() for line in f if line.strip()]
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–æ ‡ç­¾æ–‡ä»¶å‡ºé”™: {e}")
            return

        if not tags:
            self.logger.warning("âš ï¸ æ ‡ç­¾åˆ—è¡¨ä¸ºç©ºï¼Œçˆ¬è™«å·²é€€å‡ºã€‚è¯·æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶å†…å®¹ã€‚")
            return

        self.logger.info(f"--- å‘ç° {len(tags)} ä¸ªæ ‡ç­¾ï¼Œå¼€å§‹ç”Ÿæˆåˆå§‹è¯·æ±‚ ---")

        for tag in tags:
            # å…ˆè¯·æ±‚é¦–é¡µï¼Œå¹¶å°† tag ä¼ é€’ç»™ parse_home_page
            yield scrapy.Request(
                url=self.base_url, 
                callback=self.parse_home_page,
                meta={'tag': tag},
                # âœ¨ å…³é”®ä¿®å¤ï¼šç¦ç”¨å»é‡ï¼Œç¡®ä¿æ¯ä¸ªæ ‡ç­¾çš„åˆå§‹è¯·æ±‚éƒ½èƒ½è¢«å‘é€
                dont_filter=True 
            )

    # ---------------------- æ­¥éª¤ 2: æ¨¡æ‹Ÿæœç´¢æäº¤ ----------------------
    def parse_home_page(self, response):
        """
        å¤„ç†é¦–é¡µå“åº”ï¼Œæ¨¡æ‹Ÿè¡¨å•æäº¤ï¼Œå‘é€å¸¦æœ‰ Referer çš„ GET æœç´¢è¯·æ±‚ã€‚
        """
        tag = response.meta['tag']
        
        self.logger.info(f"ğŸ¡ æˆåŠŸè®¿é—®é¦–é¡µï¼Œå¼€å§‹æ¨¡æ‹Ÿæœç´¢ '{tag}'...")

        # æ„é€ æœç´¢ URL (GET è¯·æ±‚å‚æ•°: /search?q={tag})
        search_url = urljoin(response.url, f'/search?q={tag}')
        
        # æ¨¡æ‹Ÿæµè§ˆå™¨æäº¤è¡¨å•ï¼Œå¹¶è®¾ç½®æ­£ç¡®çš„ Referer å’Œç¦ç”¨è‡ªåŠ¨é‡å®šå‘
        yield scrapy.Request(
            url=search_url,
            callback=self.parse_search_page,
            meta={
                'tag': tag,
                'dont_redirect': True,              # é˜»æ­¢ Scrapy è‡ªåŠ¨è·Ÿéšé‡å®šå‘
                'handle_httpstatus_list': [301, 302] # è®© Scrapy å¤„ç† 301/302 å“åº”
            },
            headers={
                'Referer': response.url # è®¾ç½® Referer å¤´ï¼Œæ¨¡æ‹Ÿè¯·æ±‚æ¥è‡ªé¦–é¡µ
            },
        )

    # ---------------------- æ­¥éª¤ 3: è§£ææœç´¢ç»“æœé¡µ ----------------------
    def parse_search_page(self, response):
        """
        è§£ææœç´¢ç»“æœé¡µé¢ï¼Œæå–è¯¦æƒ…é¡µé“¾æ¥ã€‚
        """
        tag = response.meta['tag']

        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å®šå‘ï¼ˆ3xx çŠ¶æ€ç ï¼‰
        if response.status in [301, 302]:
            location = response.headers.get('Location').decode('utf-8')
            self.logger.info(f"ğŸ”„ æ•è·åˆ°é‡å®šå‘ ({response.status})ï¼Œç›®æ ‡åœ°å€æ˜¯: {location}")
            
            # åªæœ‰å½“é‡å®šå‘ç›®æ ‡åŒ…å«æœç´¢è·¯å¾„æ—¶ï¼Œæˆ‘ä»¬æ‰è·Ÿéš
            if '/search?q=' in location:
                # é‡æ–°å‘é€è¯·æ±‚ï¼Œè®© Scrapy è‡ªåŠ¨å¤„ç†è¿™æ¬¡é‡å®šå‘
                yield scrapy.Request(
                    url=location,
                    callback=self.parse_search_page,
                    meta={'tag': tag}
                )
            else:
                self.logger.error(f"âŒ æœç´¢ '{tag}' å¤±è´¥ï¼Œç½‘ç«™é‡å®šå‘åˆ°é¦–é¡µï¼Œæœç´¢å‚æ•°ä¸¢å¤±ã€‚URL: {location}")
            return

        # åªæœ‰çŠ¶æ€ç ä¸º 200 æ—¶æ‰è¿›è¡Œè§£æ
        if response.status != 200:
             self.logger.error(f"âŒ æœç´¢ '{tag}' å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}. é¡µé¢URL: {response.url}")
             return
            
        # ç¡®ä¿æˆ‘ä»¬ä½äºæœç´¢é¡µ
        if '/search' not in response.url:
            self.logger.warning(f"âš ï¸ æœç´¢ '{tag}' ç–‘ä¼¼å¤±è´¥ï¼Œå½“å‰é¡µé¢ä¸æ˜¯æœç´¢é¡µ: {response.url}")
            return

        self.logger.info(f"ğŸ” æ­£åœ¨è§£ææ ‡ç­¾ '{tag}' çš„æœç´¢ç»“æœé¡µ: {response.url}")

        # ä½¿ç”¨ CSS é€‰æ‹©å™¨å®šä½æ‰€æœ‰å›¾ç‰‡å¡ç‰‡é“¾æ¥
        card_links = response.css('div.wallpapers-wrap a.wp-item::attr(href)').getall()
        
        if not card_links:
            # å¼ºåŒ–è¯Šæ–­ï¼šæ£€æŸ¥é¡µé¢ä¸Šæ˜¯å¦æœ‰ 'æ²¡æœ‰ç»“æœ' çš„æç¤º
            no_results_text = response.css('.text-wh-550::text').getall()
            
            is_no_results = False
            for text in no_results_text:
                if 'no results' in text.lower() or 'not found' in text.lower():
                    is_no_results = True
                    break
            
            if is_no_results:
                self.logger.warning(f"âš ï¸ æ ‡ç­¾ '{tag}' (URL: {response.url}) **ç¡®è®¤æ²¡æœ‰æœç´¢ç»“æœ**ã€‚")
            else:
                self.logger.error(f"âŒ æ ‡ç­¾ '{tag}' (URL: {response.url}) æœªæ‰¾åˆ°å›¾ç‰‡å¡ç‰‡ã€‚**è¯·æ£€æŸ¥é€‰æ‹©å™¨æ˜¯å¦å¤±æ•ˆ**ã€‚")
                
            return # æ²¡æœ‰å›¾ç‰‡ï¼Œè·³è¿‡è¯¥æ ‡ç­¾ã€‚

        self.logger.info(f"ğŸ–¼ï¸ æ ‡ç­¾ '{tag}' æ‰¾åˆ° {len(card_links)} ä¸ªè¯¦æƒ…é¡µé“¾æ¥ã€‚")

        for link in card_links:
            detail_url = urljoin(self.base_url, link)
            
            # å‘é€å¯¹è¯¦æƒ…é¡µçš„è¯·æ±‚
            yield scrapy.Request(
                url=detail_url,
                callback=self.parse_detail_page,
                meta={'tag': tag}
            )
            
        # TODO: ç¿»é¡µ/æ— é™æ»šåŠ¨åŠ è½½é€»è¾‘å¯ä»¥åœ¨æ­¤æ·»åŠ 

    # ---------------------- æ­¥éª¤ 4: è§£æè¯¦æƒ…é¡µ ----------------------
    def parse_detail_page(self, response):
        """
        è§£æè¯¦æƒ…é¡µï¼Œæå–å›¾ç‰‡æ ‡é¢˜å’Œä¸‹è½½é“¾æ¥ï¼Œå¹¶ç”Ÿæˆ Itemã€‚
        """
        tag = response.meta['tag']
        
        # 1. æå–æ ‡é¢˜
        title = response.css('h1.text-2xl::text').get(default='N/A').strip()

        # 2. æå–ä¸‹è½½é“¾æ¥
        download_relative_url = response.css('a[href*="/variant/original?dl=true"]::attr(href)').get()
        
        if not download_relative_url:
            self.logger.warning(f"âŒ è¯¦æƒ…é¡µ {response.url} æœªæ‰¾åˆ°ä¸‹è½½é“¾æ¥ã€‚")
            return
        
        download_url = urljoin(self.base_url, download_relative_url)
        
        # 3. æ„é€ æ–‡ä»¶åå’Œ ID
        try:
             # ä» URL ä¸­æå– ID: /wallpaper/41 -> 41
             wallpaper_id = response.url.split('/wallpaper/')[1].split('/')[0]
             image_name = f"{wallpaper_id}.jpg"
        except IndexError:
             self.logger.warning(f"âŒ æ— æ³•ä» URL æå– ID: {response.url}")
             image_name = f"unknown_{tag}.jpg"

        
        # 4. åˆ›å»º Item
        item = WallhallaItem()
        item['title'] = title
        item['tag'] = tag
        item['detail_url'] = response.url
        item['image_name'] = image_name
        item['file_urls'] = [download_url] # FilesPipeline ä½¿ç”¨

        self.logger.info(f"âœ”ï¸ æå– Item: {title}, URL: {download_url}")
        
        yield item