import os
import csv
import time
import redis
import urllib3
import requests
from threading import Lock
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options


# === å¸¸é‡é…ç½® ===
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_KEY = 'imgur_image_url_set'

TAG_FILE_PATH = r"D:\myproject\Code\çˆ¬è™«\çˆ¬è™«æ•°æ®\libreshot\ram_tag_list_å¤‡ä»½.txt"
CSV_DIR_PATH = r"D:\myproject\Code\çˆ¬è™«\çˆ¬è™«æ•°æ®\imgur"
CLIENT_ID = "d70305e7c3ac5c6"
CHROME_DRIVER_PATH = r'C:\Program Files\Google\chromedriver-win64\chromedriver.exe'


# === æ ¸å¿ƒçˆ¬è™«ç±» ===
class ImgurCrawler:
    def __init__(self, csv_dir_path, csv_filename, redis_host=REDIS_HOST, redis_port=REDIS_PORT):
        self.csv_dir_path = csv_dir_path
        self.csv_path = os.path.join(self.csv_dir_path, csv_filename)
        self.csv_lock = Lock()

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json'
        }
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

        # Redis åˆå§‹åŒ–
        try:
            self.redis = redis.StrictRedis(host=redis_host, port=redis_port, decode_responses=True)
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸï¼Œä½¿ç”¨ Redis å»é‡ã€‚")
        except redis.exceptions.ConnectionError as e:
            print(f"âš ï¸ Redis è¿æ¥å¤±è´¥ ({e})ï¼Œæ”¹ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_urls = set()

    def is_duplicated(self, url):
        """å»é‡é€»è¾‘"""
        if self.redis:
            if self.redis.sismember(REDIS_KEY, url):
                return True
            else:
                self.redis.sadd(REDIS_KEY, url)
                return False
        else:
            if url in self.visited_urls:
                return True
            self.visited_urls.add(url)
            return False

    def write_to_csv(self, title, name, url, tag):
        """å†™å…¥ CSV"""
        try:
            with self.csv_lock:
                os.makedirs(os.path.dirname(self.csv_path), exist_ok=True)
                is_file_empty = not os.path.exists(self.csv_path) or os.stat(self.csv_path).st_size == 0

                with open(self.csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    if is_file_empty:
                        writer.writerow(['Title', 'ImageName', 'URL', 'TAG'])
                    writer.writerow([title, name, url, tag])
        except Exception as e:
            print(f"[{tag}] [âœ—] å†™å…¥ CSV å‡ºé”™: {e}")

    def fetch_tag_page(self, tag, page):
        """è¯·æ±‚å•é¡µ"""
        api_url = f"https://api.imgur.com/post/v1/posts/t/{tag}?client_id={CLIENT_ID}&filter%5Bwindow%5D=week&include=adtiles%2Cadconfig%2Ccover&location=desktoptag&page={page}&sort=-viral"
        try:
            response = requests.get(api_url, headers=self.headers, timeout=10, verify=False)
            if response.status_code == 404:
                return None
            data = response.json()
            return data.get("posts", [])
        except Exception as e:
            print(f"[{tag}] [âœ—] ç¬¬ {page} é¡µè¯·æ±‚å‡ºé”™: {e}")
            return None

    def parse_and_save(self, posts, tag):
        """è§£æ JSON"""
        for post in posts:
            try:
                mime_type = post.get("cover", {}).get("mime_type", "")
                if not mime_type.startswith("image/"):
                    continue  # éå›¾ç‰‡è·³è¿‡

                img_url = post["cover"]["url"]
                title = post.get("title", "").strip()
                image_name = os.path.basename(img_url)

                if not self.is_duplicated(img_url):
                    self.write_to_csv(title, image_name, img_url, tag)
                    print(f"[{tag}] âœ… {title} | {img_url}")
                else:
                    print(f"[{tag}] âš™ï¸ å·²å­˜åœ¨: {img_url}")
            except Exception as e:
                print(f"[{tag}] [âœ—] è§£æå‡ºé”™: {e}")

    def crawl_tag(self, tag):
        """åˆ†é¡µçˆ¬å–"""
        print(f"ğŸš€ å¼€å§‹æŠ“å–æ ‡ç­¾: {tag}")
        page = 1
        while True:
            posts = self.fetch_tag_page(tag, page)
            if not posts:
                print(f"[{tag}] ğŸ›‘ æ— æ›´å¤šæ•°æ®ï¼Œç»“æŸã€‚")
                break
            self.parse_and_save(posts, tag)
            page += 1
            time.sleep(1)

    def run(self, tags, max_workers=20):    # å¼€å¯ 20 ä¸ªçº¿ç¨‹
        """å¤šçº¿ç¨‹è¿è¡Œ"""
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(self.crawl_tag, tag.strip()) for tag in tags if tag.strip()]
            for f in as_completed(futures):
                f.result()


# === å¯åŠ¨ Selenium æ‰“å¼€ä¸»é¡µ ===
def init_selenium():
    options = Options()
    options.add_argument("--headless")  # æ— å¤´æ¨¡å¼
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--log-level=3")

    service = Service(CHROME_DRIVER_PATH)
    driver = webdriver.Chrome(service=service, options=options)

    driver.get("https://imgur.com/")
    print("ğŸŒ Selenium å·²æˆåŠŸè®¿é—® Imgur ä¸»é¡µ")
    time.sleep(2)
    driver.quit()


# === ä¸»å‡½æ•°å…¥å£ ===
if __name__ == "__main__":
    # 1ï¸âƒ£ å¯åŠ¨æµè§ˆå™¨
    init_selenium()

    # 2ï¸âƒ£ åŠ è½½æ ‡ç­¾æ–‡ä»¶
    with open(TAG_FILE_PATH, 'r', encoding='utf-8') as f:
        tag_list = [line.strip() for line in f.readlines() if line.strip()]

    # 3ï¸âƒ£ å¯åŠ¨çˆ¬è™«ä»»åŠ¡
    crawler = ImgurCrawler(csv_dir_path=CSV_DIR_PATH, csv_filename="imgur_data.csv")
    crawler.run(tag_list, max_workers=20)   # å¼€å¯ 20 ä¸ªçº¿ç¨‹

    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆï¼")
