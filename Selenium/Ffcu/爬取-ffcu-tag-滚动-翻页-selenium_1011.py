from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import os, time, hashlib, redis, csv, threading, random, re, urllib.parse


class ffcu:
    def __init__(self, chrome_driver_path):
        service = Service(executable_path=chrome_driver_path)
        options = Options()
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument(
            'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        # options.add_argument('--headless')  # è°ƒè¯•å®Œå¯ä»¥å¼€å¯
        options.add_argument("--window-size=1920,1080")
        self.driver = webdriver.Chrome(service=service, options=options)

        self.base_url = 'https://ffcu.io/'
        self.csv_lock = threading.Lock()
        self.redis_key = 'image_md5_set_ffcu'
        self.main_container_selector = 'div.blog_post_preview.post-listing.image-post-format.one-image-format'

        try:
            self.redis = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸã€‚")
        except Exception:
            print("âš ï¸ Redis ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_md5 = set()

    # ---------------------- æå–å½“å‰é¡µå›¾ç‰‡ (ä¿®å¤åˆ†æ­¥æ»šåŠ¨å’Œæ‰“å°) ----------------------
    def get_images(self, tag, csv_path):
        print(f"--- æ­£åœ¨è§£æã€{tag}ã€‘çš„å›¾ç‰‡åˆ—è¡¨...")

        wait = WebDriverWait(self.driver, 30)
        try:
            # ç­‰å¾…ç¬¬ä¸€ä¸ªå›¾ç‰‡å®¹å™¨åŠ è½½
            wait.until(EC.presence_of_element_located((
                By.CSS_SELECTOR, self.main_container_selector)))
            print("[âˆš] ä¸»å›¾ç‰‡å®¹å™¨åŠ è½½å®Œæˆã€‚")
        except TimeoutException:
            print("[âœ—] é¡µé¢åŠ è½½è¶…æ—¶ã€‚")
            return

        # === æ ¸å¿ƒä¿®å¤åŒºåŸŸï¼šåˆ†æ­¥å¹³æ»‘æ»šåŠ¨åŠ è½½ ===
        SCROLL_HEIGHT = 1080  # æ¯æ¬¡æ»šåŠ¨çš„åƒç´ è·ç¦»
        SCROLL_PAUSE_TIME = 2.0  # æ¯æ¬¡æ»šåŠ¨åçš„æš‚åœæ—¶é—´
        
        current_scroll_position = 0
        total_scrolls = 0
        window_height = self.driver.execute_script("return window.innerHeight;") # è·å–æµè§ˆå™¨è§†å£é«˜åº¦
        
        print(f"ğŸš€ å¼€å§‹åˆ†æ­¥æ»šåŠ¨åŠ è½½ (æ¯æ­¥ {SCROLL_HEIGHT}pxï¼Œæš‚åœ {SCROLL_PAUSE_TIME}ç§’)...")

        while True:
            total_scrolls += 1
            
            # 1. è®¡ç®—æ–°çš„æ»šåŠ¨ä½ç½®å¹¶æ‰§è¡Œæ»šåŠ¨
            new_scroll_position = current_scroll_position + SCROLL_HEIGHT
            self.driver.execute_script(f"window.scrollTo(0, {new_scroll_position});")
            
            # 2. æš‚åœï¼Œç­‰å¾…å†…å®¹åŠ è½½
            time.sleep(SCROLL_PAUSE_TIME + random.random() * 0.5)
            
            # 3. æ›´æ–°å½“å‰æ»šåŠ¨ä½ç½®å’Œé¡µé¢æ€»é«˜åº¦
            current_scroll_position = self.driver.execute_script("return window.pageYOffset;")
            page_total_height = self.driver.execute_script("return document.body.scrollHeight")

            # 4. æ£€æŸ¥æ˜¯å¦åˆ°è¾¾åº•éƒ¨
            # å¦‚æœå½“å‰æ»šåŠ¨ä½ç½®åŠ ä¸Šæµè§ˆå™¨è§†å£é«˜åº¦çº¦ç­‰äºé¡µé¢æ€»é«˜åº¦ï¼Œåˆ™è®¤ä¸ºåˆ°è¾¾åº•éƒ¨ã€‚
            if (current_scroll_position + window_height) >= page_total_height:
                print(f"âœ… æ»šåŠ¨åˆ°åº•éƒ¨ã€‚æ€»å…±æ»šåŠ¨äº† {total_scrolls} æ¬¡ã€‚")
                break
                
            # å¦‚æœå®é™…æ»šåŠ¨ä½ç½®æ²¡æœ‰è¾¾åˆ°é¢„æœŸçš„ new_scroll_positionï¼Œé€šå¸¸æ„å‘³ç€åˆ°è¾¾äº†é¡µé¢åº•éƒ¨
            if current_scroll_position < new_scroll_position:
                 print(f"âœ… æ»šåŠ¨åˆ°åº•éƒ¨ (æ»šåŠ¨ä½ç½®ç¨³å®š)ã€‚æ€»å…±æ»šåŠ¨äº† {total_scrolls} æ¬¡ã€‚")
                 break

        # === æ»šåŠ¨åŠ è½½ç»“æŸï¼Œå¼€å§‹æå– ===
        
        # å†æ¬¡ç¡®è®¤æ‰€æœ‰å®¹å™¨éƒ½å·²åŠ è½½ï¼Œå¹¶è®¡ç®—æœ€ç»ˆæ•°é‡
        post_containers = self.driver.find_elements(By.CSS_SELECTOR, self.main_container_selector)
        last_count = len(post_containers)
        print(f"ğŸ–¼ï¸ å…±æ£€æµ‹åˆ° {last_count} ä¸ªå›¾ç‰‡å®¹å™¨ã€‚")

        # --- éå†æå– ---
        successful_writes = 0
        
        for idx, card_ele in enumerate(post_containers):
            try:
                # ä½¿ç”¨ By.TAG_NAME å®šä½ img å…ƒç´ ï¼ˆä¸å—ç±»åå·®å¼‚å½±å“ï¼‰
                img_ele = card_ele.find_element(By.TAG_NAME, 'img')
                
                # æ‰©å±•å›¾ç‰‡ URL å±æ€§è·å–
                image_url = (img_ele.get_attribute('src')
                             or img_ele.get_attribute('data-src')       
                             or img_ele.get_attribute('data-original')
                             or img_ele.get_attribute('data-lazy-src')
                             or "")
                                 
                if not image_url:
                    print(f"[è·³è¿‡] å®¹å™¨åºå· {idx}ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆå›¾ç‰‡é“¾æ¥ã€‚")
                    continue

                # --- æå–æ ‡é¢˜ ---
                try:
                    title_ele = card_ele.find_element(By.CSS_SELECTOR, 'div.blog_post_title h2 a')
                    title = title_ele.text.strip()
                except NoSuchElementException:
                    title = "æœªå‘½åå›¾ç‰‡"

                # --- æ¸…ç†å›¾ç‰‡ URL ---
                image_url_cleaned = re.sub(r'-\d+x\d+(?=\.\w+$)', '', image_url)

                # --- å»é‡åŠæ‰“å°ä¿®æ”¹ ---
                md5_hash = hashlib.md5(image_url_cleaned.encode('utf-8')).hexdigest()
                if self.is_duplicate(md5_hash):
                    # ğŸ“¢ å‘ç°é‡å¤é¡¹æ—¶ï¼Œæ‰“å°è·³è¿‡ä¿¡æ¯
                    print(f"[é‡å¤] å®¹å™¨åºå· {idx}ï¼šã€{title}ã€‘URL {image_url_cleaned} å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                    continue

                # --- å†™å…¥ CSV ---
                image_name = os.path.basename(image_url_cleaned)
                self.write_to_csv(title, image_name, image_url_cleaned, csv_path, tag)
                
                # ğŸ“¢ æˆåŠŸå†™å…¥æ—¶ï¼Œæ‰“å°æˆåŠŸä¿¡æ¯
                print(f"âœ”ï¸ å†™å…¥æˆåŠŸï¼ã€{title}ã€‘")
                successful_writes += 1

            except NoSuchElementException:
                continue 
            except Exception as e:
                print(f"[âœ—] å®¹å™¨åºå· {idx} æå–å›¾ç‰‡ä¿¡æ¯å¤±è´¥: {e}")
                
        print(f"âœ… ã€{tag}ã€‘æœ¬é¡µå…±æ£€æµ‹ {len(post_containers)} ä¸ªå®¹å™¨ï¼ŒæˆåŠŸå†™å…¥ {successful_writes} æ¡è®°å½•ã€‚")


    # ---------------------- å»é‡é€»è¾‘ ----------------------
    def is_duplicate(self, md5_hash):
        if self.redis:
            if self.redis.sismember(self.redis_key, md5_hash):
                return True
            self.redis.sadd(self.redis_key, md5_hash)
        else:
            if md5_hash in self.visited_md5:
                return True
            self.visited_md5.add(md5_hash)
        return False

    # ---------------------- å†™å…¥ CSV ----------------------
    def write_to_csv(self, title, name, url, csv_path, tag):
        try:
            with self.csv_lock:
                # ä¼˜åŒ–å†™å…¥é€»è¾‘ï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œæ˜¯å¦ä¸ºç©º
                is_file_empty = not os.path.exists(csv_path) or os.stat(csv_path).st_size == 0
                
                with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    if is_file_empty:
                        writer.writerow(['Title', 'ImageName', 'URL', 'Tag'])
                    writer.writerow([title, name, url, tag])
        except Exception as e:
            print(f"[âœ—] å†™å…¥ CSV å‡ºé”™: {e}")

    # ---------------------- ç¿»é¡µé€»è¾‘ ----------------------
    def crawl_page(self, tag, csv_path):
        page_num = 1
        wait = WebDriverWait(self.driver, 15)

        while True:
            print(f"\n=== æ­£åœ¨çˆ¬å–ã€{tag}ã€‘ç¬¬ {page_num} é¡µ ===")
            self.get_images(tag, csv_path)

            try:
                # å°è¯•å®šä½â€œNextâ€æŒ‰é’®
                next_btn = self.driver.find_element(By.XPATH, "//li[@class='next_page']/a[contains(text(),'Next')]")
                next_url = next_btn.get_attribute("href")
                
                if not next_url:
                    print(f"[å®Œæˆ] ã€{tag}ã€‘å…±çˆ¬å– {page_num} é¡µã€‚")
                    break

                self.driver.get(next_url)
                time.sleep(random.uniform(3, 5))
                
                # ç­‰å¾…ä¸‹ä¸€é¡µå›¾ç‰‡å®¹å™¨åŠ è½½
                wait.until(EC.presence_of_element_located(
                    (By.CSS_SELECTOR, self.main_container_selector)))
                
                page_num += 1
            except NoSuchElementException:
                # æœªæ‰¾åˆ° Next æŒ‰é’®ï¼Œè®¤ä¸ºåˆ°è¾¾æœ€åä¸€é¡µ
                print(f"[å®Œæˆ] ã€{tag}ã€‘å…±çˆ¬å– {page_num} é¡µã€‚")
                break
            except Exception as e:
                print(f"[âœ—] ç¿»é¡µè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                print(f"[å®Œæˆ] ã€{tag}ã€‘å…±çˆ¬å– {page_num} é¡µã€‚")
                break

    # ---------------------- ä¸»å‡½æ•° ----------------------
    def main(self):
        save_dir = r'D:\work\çˆ¬è™«\çˆ¬è™«æ•°æ®\ffcu'
        tag_file_path = r'D:\work\çˆ¬è™«\ram_tag_list.txt'
        os.makedirs(save_dir, exist_ok=True)
        csv_path = os.path.join(save_dir, 'all_records_ffcu_01.csv')

        try:
            with open(tag_file_path, 'r', encoding='utf-8') as f:
                tags = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"[é”™è¯¯] æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {tag_file_path}")
            return

        print(f"--- å‘ç° {len(tags)} ä¸ªæ ‡ç­¾ ---")

        for tag in tags:
            try:
                encoded_tag = urllib.parse.quote_plus(tag)
                search_url = f"{self.base_url}?s={encoded_tag}"
                print(f"\n--- å¼€å§‹å¤„ç†ï¼šã€{tag}ã€‘ ---\nURL: {search_url}")
                self.driver.get(search_url)
                time.sleep(random.uniform(3, 5))

                try:
                    # ç¡®ä¿æœç´¢ç»“æœé¡µæœ‰å›¾ç‰‡å®¹å™¨
                    WebDriverWait(self.driver, 15).until(
                        lambda d: d.find_elements(By.CSS_SELECTOR, self.main_container_selector))
                except TimeoutException:
                    print(f"[è·³è¿‡] {tag} é¡µé¢æœªåŠ è½½å‡ºå›¾ç‰‡å®¹å™¨ã€‚")
                    continue

                self.crawl_page(tag, csv_path)
            except Exception as e:
                print(f"[âœ—] å¤„ç†æ ‡ç­¾ {tag} å‡ºé”™: {e}")

        # driver.quit() ä¼šåœ¨ finally å—ä¸­å¤„ç†

if __name__ == '__main__':
    # âš ï¸ è¯·ç¡®è®¤ä½ çš„é©±åŠ¨è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼
    chrome_driver_path = r'C:\Program Files\Google\1\chromedriver-win32\chromedriver.exe'
    
    spider = None # åˆå§‹åŒ–ä¸º None
    try:
        spider = ffcu(chrome_driver_path)
        spider.main()
    except Exception as main_e:
        print(f"ä¸»ç¨‹åºè¿è¡Œå‡ºé”™: {main_e}")
    finally:
        # ç¡®ä¿æ— è®ºç¨‹åºæ˜¯å¦å‡ºé”™ï¼ŒWebDriver éƒ½ä¼šå…³é—­
        if spider and spider.driver:
            print("\næ­£åœ¨å…³é—­æµè§ˆå™¨...")
            spider.driver.quit()