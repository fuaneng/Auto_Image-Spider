import os
import csv
import time
import redis
import urllib3
import undetected_chromedriver as uc 
from threading import Lock
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from requests.exceptions import RequestException
import random # å¼•å…¥ random ç”¨äºéšæœºç­‰å¾…

# å¿½ç•¥ Requests çš„ SSL è­¦å‘Š (è™½ç„¶è¿™ä¸ªç‰ˆæœ¬æ²¡æœ‰ç”¨åˆ° Requestsï¼Œä½†ä¿ç•™è¿™ä¸ªä¹ æƒ¯)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é…ç½®å¸¸é‡ ---
REDIS_HOST = 'localhost'
REDIS_PORT = 6379
REDIS_KEY = 'piqsels_image_url_set' 
TAG_FILE_PATH = r"R:\py\Auto_Image-Spider\Requests\Piqsels\ram_tag_list.txt"
CSV_DIR_PATH = r"R:\py\Auto_Image-Spider\Requests\Piqsels"
CSV_FILENAME = "piqsels_data.csv"
BASE_URL_TEMPLATE = "https://www.piqsels.com/en/search?q={tag}&page={page}"

# !! æ‚¨çš„è‡ªå®šä¹‰é©±åŠ¨è·¯å¾„ !!
CUSTOM_DRIVER_PATH = r"C:\Program Files\Google\chromedriver-win64\chromedriver.exe"

class PiqselsImageCrawler:
    """
    é’ˆå¯¹ piqsels.com çš„å•çº¿ç¨‹ä¸²è¡Œçˆ¬è™«ï¼Œé›†æˆ Selenium å¤„ç†åçˆ¬éªŒè¯ï¼Œ
    å¹¶ä½¿ç”¨ Redis/å†…å­˜è¿›è¡Œå»é‡ã€‚
    """

    def __init__(self, csv_dir_path, csv_filename, driver_path, redis_host=REDIS_HOST, redis_port=REDIS_PORT):
        
        self.csv_dir_path = csv_dir_path
        self.csv_path = os.path.join(self.csv_dir_path, csv_filename) 
        self.csv_lock = Lock() 
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image:apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7'
        }
        
        # --- Redis/å†…å­˜ å»é‡åˆå§‹åŒ–é€»è¾‘ ---
        self.redis = None
        self.visited_urls = set()
        self._init_deduplication(redis_host, redis_port)

        # --- Selenium åˆå§‹åŒ– (ä½¿ç”¨æŒ‡å®šè·¯å¾„) ---
        self.driver = None
        print("â³ æ­£åœ¨åˆå§‹åŒ– Selenium æµè§ˆå™¨...")
        try:
            self.driver = uc.Chrome(
                headless=False, 
                use_subprocess=True,
                driver_executable_path=driver_path 
            )
            # è®¾ç½®éšå¼ç­‰å¾…ï¼Œä½œä¸ºé¢å¤–çš„ç¼“å†²æœºåˆ¶
            self.driver.implicitly_wait(2) 
            print("âœ… Selenium æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸã€‚")
        except Exception as e:
            print(f"âŒ Selenium æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.driver = None


    def _init_deduplication(self, redis_host, redis_port):
        """åˆå§‹åŒ– Redis æˆ–å†…å­˜å»é‡æœºåˆ¶ã€‚"""
        try:
            self.redis = redis.StrictRedis(host=redis_host, port=redis_port, decode_responses=True)
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸï¼Œä½¿ç”¨ Redis é›†åˆè¿›è¡Œå»é‡ã€‚")
        except redis.exceptions.ConnectionError:
            print("âš ï¸ Redis è¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚") 
            self.redis = None
            self.visited_urls = set()
        except Exception as e:
            print(f"âš ï¸ Redis åˆå§‹åŒ–é‡åˆ°å…¶ä»–é”™è¯¯ ({e})ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_urls = set()


    def _is_url_visited(self, url):
        """æ£€æŸ¥ URL æ˜¯å¦å·²è¢«è®¿é—®ï¼ˆå³å·²ä¿å­˜ï¼‰ã€‚"""
        unique_id = os.path.basename(url) 
        
        if self.redis:
            return not self.redis.sadd(REDIS_KEY, unique_id)
        else:
            if unique_id in self.visited_urls:
                return True
            self.visited_urls.add(unique_id)
            return False


    def write_to_csv(self, title, name, url, tag):
        """
        å†™å…¥ CSV æ–¹æ³•ã€‚
        """
        csv_path = self.csv_path
        try:
            # å•çº¿ç¨‹ä¸éœ€è¦ Lockï¼Œä½†ä¿ç•™ structure ä»¥é˜²æœªæ¥æ›´æ”¹
            os.makedirs(os.path.dirname(csv_path), exist_ok=True)
            is_file_empty = not os.path.exists(csv_path) or os.stat(csv_path).st_size == 0
            
            with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                
                if is_file_empty:
                    writer.writerow(['Title', 'ImageName', 'URL', 'TAG'])
                    
                writer.writerow([title, name, url, tag])
            print(f"[{tag}] [âœ“] æˆåŠŸå†™å…¥ CSV: {name}")
        except Exception as e:
            print(f"[{tag}] [âœ—] å†™å…¥ CSV å‡ºé”™: {e}")


    def _get_page_source(self, url, tag, page_num):
        """
        ä½¿ç”¨ Selenium é©±åŠ¨æµè§ˆå™¨ï¼Œå¤„ç† Cloudflare éªŒè¯å¹¶è·å–é¡µé¢æºä»£ç ã€‚
        è¿”å›ï¼šHTMLå†…å®¹ï¼ˆæˆåŠŸï¼‰ï¼ŒNoneï¼ˆå¯é‡è¯•å¤±è´¥ï¼Œå¦‚ç½‘ç»œè¶…æ—¶/éªŒè¯å¡ä½ï¼‰ï¼Œ""ï¼ˆä¸å¯é‡è¯•å¤±è´¥ï¼Œå¦‚ No resultsï¼‰
        """
        if not self.driver:
            return None 

        print(f"[{tag}] å°è¯•è®¿é—® URL: {url}")
        self.driver.get(url)

        try:
            # 1. ç­‰å¾… ID ä¸º "main" çš„ä¸»å†…å®¹åŒºåŸŸåŠ è½½å‡ºæ¥ (æœ€é•¿ 60 ç§’ç”¨äºæ‰‹åŠ¨ Cloudflare)
            WebDriverWait(self.driver, 60).until(
                EC.presence_of_element_located((By.ID, "main"))
            )
            
            # --- æ£€æŸ¥ â€œNo resultsâ€ å…ƒç´  (å¿«é€Ÿè·³è¿‡é€»è¾‘) ---
            try:
                # å°è¯•æ‰¾åˆ° <span class="notfound">No results</span> å…ƒç´ 
                no_results_element = self.driver.find_element(By.CLASS_NAME, "notfound")
                if no_results_element and no_results_element.text.strip().lower() == "no results":
                    print(f"[{tag}] [Page {page_num}] âŒ æ£€æµ‹åˆ° 'No results'ï¼Œ**å¿«é€Ÿè·³è¿‡å½“å‰æ ‡ç­¾ï¼Œæ— éœ€é‡è¯•**ã€‚")
                    return "" # è¿”å› "" ä½œä¸ºç‰¹æ®Šæ ‡è®°
            except Exception:
                pass
            
            # 2. ç»§ç»­ç­‰å¾…å›¾ç‰‡åˆ—è¡¨ (ç­‰å¾…ç¬¬ä¸€ä¸ªå›¾ç‰‡å…ƒç´ å‡ºç°)
            try:
                WebDriverWait(self.driver, 5).until( 
                    EC.presence_of_element_located((By.CSS_SELECTOR, "#flow li.item"))
                )
            except Exception:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ Cloudflare å†æ¬¡è§¦å‘
                page_text = self.driver.page_source
                if "Just a moment" in page_text or "captcha" in page_text:
                    print(f"[{tag}] âš ï¸ æ£€æµ‹åˆ° Cloudflare å†æ¬¡è§¦å‘éªŒè¯ï¼Œè¯·æ‰‹åŠ¨å¤„ç†æµè§ˆå™¨çª—å£ã€‚")
                    return None # è¿”å› None å…è®¸é‡è¯•ï¼Œç»™ç”¨æˆ·æ—¶é—´å»å¤„ç†æµè§ˆå™¨çª—å£

                print(f"[{tag}] âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶ï¼Œæœªæ‰¾åˆ°å›¾ç‰‡å…ƒç´ æˆ– Cloudflare å†æ¬¡è§¦å‘ã€‚")
                return None
            
            print(f"[{tag}] âœ… é¡µé¢åŠ è½½æˆåŠŸæˆ–å·²é€šè¿‡éªŒè¯ã€‚")
            
            # 3. æ£€æŸ¥ 404 
            page_text = self.driver.page_source
            if "page not found" in page_text.lower() or self.driver.title.lower().startswith("404"):
                 print(f"[{tag}] âš ï¸ æ£€æµ‹åˆ°å¯èƒ½æ˜¯ 404 é¡µé¢ï¼Œåœæ­¢åˆ†é¡µã€‚")
                 return None 
            
            return self.driver.page_source

        except Exception as e:
            # é¡µé¢åŠ è½½è¶…æ—¶æˆ– Cloudflare éªŒè¯å¡ä½
            print(f"[{tag}] âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶æˆ–å‡ºç°å¼‚å¸¸ (Message: {e.msg if hasattr(e, 'msg') else e})ã€‚è¿”å› None å…è®¸é‡è¯•ã€‚")
            return None


    def _parse_page(self, html_content, tag):
        """
        è§£æ HTML å†…å®¹ï¼Œæå–å›¾ç‰‡ä¿¡æ¯ã€‚
        """
        if not html_content:
            return []

        soup = BeautifulSoup(html_content, 'html.parser')
        ul_flow = soup.find('ul', id='flow')
        
        if not ul_flow:
             # åœ¨ _get_page_source ä¸­å·²æ£€æŸ¥è¿‡ï¼Œè¿™é‡Œæ— éœ€é‡å¤æ‰“å°
             return []

        image_items = ul_flow.find_all('li', class_='item')
        
        if not image_items:
            # åœ¨ _get_page_source ä¸­å·²å¤„ç† No results/è¶…æ—¶ï¼Œè¿™é‡Œæ— éœ€é‡å¤æ‰“å°
            return [] 

        data_list = []
        for item in image_items:
            try:
                img_tag = item.find('img', class_='lazy')
                title = img_tag.get('alt', 'N/A').strip() if img_tag else 'N/A'

                license_a_tag = item.find('a', rel="license") 
                if license_a_tag:
                    about_url = license_a_tag.get('about')
                    if about_url and about_url.endswith('-thumbnail.jpg'):
                        full_url = about_url.replace('-thumbnail.jpg', '.jpg')
                        image_name = os.path.basename(full_url)
                    else:
                        continue 
                else:
                    continue 

                if self._is_url_visited(full_url):
                    print(f"[{tag}] [~] URL å·²å­˜åœ¨ï¼Œè·³è¿‡: {image_name}")
                    continue

                data_list.append({
                    'title': title, 'name': image_name, 'url': full_url, 'tag': tag
                })

            except Exception as e:
                print(f"[{tag}] [âœ—] è§£æå•ä¸ªå›¾ç‰‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                continue

        return data_list


    def start_crawl_for_tag(self, tag):
        """
        é’ˆå¯¹å•ä¸ªæ ‡ç­¾ï¼Œæ‰§è¡Œä¸²è¡Œåˆ†é¡µçˆ¬å–ã€‚åœ¨åŠ è½½å¤±è´¥æˆ–æ— æ•°æ®æ—¶å¿«é€Ÿè·³è¿‡ã€‚
        """
        if not self.driver:
            print(f"[{tag}] [âœ—] çˆ¬è™«æ— æ³•å¯åŠ¨ï¼ŒSelenium é©±åŠ¨ç¼ºå¤±ã€‚")
            return

        print(f"\n--- âš¡ï¸ å¼€å§‹çˆ¬å–æ ‡ç­¾: {tag} ---")
        page = 1
        max_retries = 3 

        while True:
            url = BASE_URL_TEMPLATE.format(tag=tag, page=page)
            print(f"[{tag}] [Page {page}] æ­£åœ¨å¤„ç†...")
            
            html_content = None
            retry_count = 0
            
            # --- é¡µé¢åŠ è½½ä¸é‡è¯•é€»è¾‘ ---
            while retry_count < max_retries:
                html_content = self._get_page_source(url, tag, page) 
                
                if html_content == "": 
                    # å‘ç° No resultsï¼Œæ— éœ€é‡è¯•
                    retry_count = max_retries 
                    break 
                
                if html_content is not None:
                    break # æˆåŠŸè·å– HTML
                
                print(f"[{tag}] [Page {page}] åŠ è½½å¤±è´¥ï¼Œé‡è¯• ({retry_count + 1}/{max_retries})...")
                retry_count += 1
                time.sleep(5) 

            # --- å¿«é€Ÿè·³è¿‡å’Œæœ€ç»ˆå¤±è´¥åˆ¤æ–­ ---
            if html_content is None or html_content == "": 
                print(f"[{tag}] [Page {page}] åœæ­¢åˆ†é¡µï¼Œå¿«é€Ÿè·³è¿‡å½“å‰æ ‡ç­¾ã€‚")
                break 

            # --- è§£ææ•°æ® ---
            image_data = self._parse_page(html_content, tag)

            if not image_data:
                # è§£æè¿”å›ç©ºåˆ—è¡¨ï¼ˆæœªæå–åˆ°ä»»ä½•æ–°æ•°æ® æˆ– é¡µé¢ä¸­æ— å›¾ç‰‡å…ƒç´ ï¼‰
                print(f"[{tag}] [Page {page}] **è§£æç»“æœä¸ºç©ºï¼Œåœæ­¢åˆ†é¡µ**ã€‚")
                break 
            
            # å†™å…¥ CSV
            for data in image_data:
                self.write_to_csv(data['title'], data['name'], data['url'], data['tag'])

            page += 1
            # éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼Œå»¶è¿ŸèŒƒå›´ 3 åˆ° 8 ç§’
            time.sleep(3 + random.uniform(2, 5)) 


    def run(self):
        """
        ä¸»æ‰§è¡Œæ–¹æ³•ï¼šè¯»å–æ ‡ç­¾æ–‡ä»¶ï¼Œå¹¶å¯¹æ¯ä¸ªæ ‡ç­¾å¯åŠ¨çˆ¬å–ã€‚
        """
        if not self.driver:
             print("âŒ çˆ¬è™«æ— æ³•å¯åŠ¨ï¼Œè¯·è§£å†³ Selenium é©±åŠ¨åˆå§‹åŒ–é”™è¯¯ã€‚")
             return

        tag_list = []
        try:
            with open(TAG_FILE_PATH, 'r', encoding='utf-8') as f:
                tag_list = [line.strip() for line in f if line.strip()]
            print(f"âœ… æˆåŠŸè¯»å– {len(tag_list)} ä¸ªæ ‡ç­¾ã€‚")
        except FileNotFoundError:
            print(f"âŒ æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°: {TAG_FILE_PATH}")
            return

        for tag in tag_list:
            self.start_crawl_for_tag(tag)
        
        if self.driver:
            self.driver.quit()
            print("\nğŸ‰ æ‰€æœ‰æ ‡ç­¾çˆ¬å–å®Œæˆï¼Œæµè§ˆå™¨å·²å…³é—­ã€‚")

if __name__ == '__main__':
    crawler = PiqselsImageCrawler(
        csv_dir_path=CSV_DIR_PATH, 
        csv_filename=CSV_FILENAME,
        driver_path=CUSTOM_DRIVER_PATH # ä¼ é€’æœ¬åœ°é©±åŠ¨è·¯å¾„
    )
    crawler.run()