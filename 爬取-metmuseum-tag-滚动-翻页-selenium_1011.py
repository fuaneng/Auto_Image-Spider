from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException
import os, time, hashlib, redis, csv, threading, random, re, urllib.parse, sys


class metmuseum:
    def __init__(self, chrome_driver_path, use_headless=True, redis_host='localhost', redis_port=6379):
        service = Service(executable_path=chrome_driver_path)
        options = Options()
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument(
            'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
        if use_headless:
            options.add_argument('--headless=new')  # æ–° headless æ¨¡å¼ï¼Œæˆ–ç”¨ '--headless' å–å†³äº chromedriver ç‰ˆæœ¬
        options.add_argument("--window-size=1920,1080")
        # æ¨èç¦ç”¨å›¾ç‰‡åŠ è½½ä»¥åŠ å¿«é¡µé¢è§£æï¼ˆå¦‚æœä½ éœ€è¦ä¸‹è½½å›¾ç‰‡åˆ™ä¸è¦ç¦ç”¨ï¼‰
        # prefs = {"profile.managed_default_content_settings.images": 2}
        # options.add_experimental_option("prefs", prefs)

        self.driver = webdriver.Chrome(service=service, options=options)
        self.base_url = 'https://www.metmuseum.org/'
        self.csv_lock = threading.Lock()
        self.redis_key = 'image_md5_set_metmuseum'
        # å…¨å±€ä¸€çº§å®¹å™¨ CSSï¼ˆä¿æŒä½ æä¾›çš„é€‰æ‹©å™¨ï¼‰
        self.main_container_selector = 'section.object-grid_grid__hKKqs figure.collection-object_collectionObject__SuPct'

        # Redis è¿æ¥ï¼ˆå¯é€‰ï¼‰
        try:
            self.redis = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸã€‚")
        except Exception:
            print("âš ï¸ Redis ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_md5 = set()

    # ---------------------- æå–å½“å‰é¡µå›¾ç‰‡ (ä¿®å¤åˆ†æ­¥æ»šåŠ¨å’Œæ‰“å°) ----------------------
    def get_images(self, tag, csv_path):
        print(f"--- æ­£åœ¨è§£æã€{tag}ã€‘çš„å›¾ç‰‡åˆ—è¡¨...")

        wait = WebDriverWait(self.driver, 30)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.main_container_selector)))
            print("[âˆš] ä¸»å›¾ç‰‡å®¹å™¨åŠ è½½å®Œæˆã€‚")
        except TimeoutException:
            print("[âœ—] é¡µé¢åŠ è½½è¶…æ—¶ï¼Œæ‰¾ä¸åˆ°ä¸»å›¾ç‰‡å®¹å™¨ã€‚")
            return

        # === åˆ†æ­¥å¹³æ»‘æ»šåŠ¨åŠ è½½ï¼ˆåœ¨ä½ åŸæ¥æ€è·¯ä¸Šå¢å¼ºåˆ¤æ–­ï¼‰ ===
        SCROLL_STEP = 1080  # æ¯æ¬¡æ»šåŠ¨åƒç´ 
        MAX_IDLE_ROUNDS = 2  # å½“å¤šæ¬¡æ»šåŠ¨é¡µé¢é«˜åº¦æ²¡æœ‰å˜åŒ–åˆ™åœæ­¢
        scroll_round = 0
        idle_rounds = 0
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        window_height = self.driver.execute_script("return window.innerHeight;")

        print(f"ğŸš€ å¼€å§‹åˆ†æ­¥æ»šåŠ¨åŠ è½½ (step={SCROLL_STEP}px)...")
        while True:
            scroll_round += 1
            # è®¡ç®—æ–°çš„ä½ç½®å¹¶æ»šåŠ¨
            new_pos = min(last_height, (scroll_round * SCROLL_STEP))
            self.driver.execute_script(f"window.scrollTo(0, window.pageYOffset + {SCROLL_STEP});")
            time.sleep(1.0 + random.random() * 1.0)  # éšæœºç­‰å¾…ï¼Œé™ä½è¢«é˜»æ–­çš„æ¦‚ç‡

            current_height = self.driver.execute_script("return document.body.scrollHeight")
            current_offset = self.driver.execute_script("return window.pageYOffset;")

            # æ‰“å°è¿›åº¦ï¼ˆç®€çŸ­ï¼‰
            # print(f"  scroll_round={scroll_round}, offset={current_offset}, pageHeight={current_height}")

            # å¦‚æœé¡µé¢é«˜åº¦å¢åŠ ï¼Œé‡ç½® idle è®¡æ•°
            if current_height > last_height:
                idle_rounds = 0
                last_height = current_height
            else:
                idle_rounds += 1

            # åˆ°è¾¾åº•éƒ¨åˆ¤æ–­ï¼šoffset + window_height >= total_height - small_margin
            if (current_offset + window_height) >= (current_height - 20):
                print(f"âœ… æ£€æµ‹åˆ°æ»šåŠ¨åˆ°åº•éƒ¨ (offset+win >= total). rounds={scroll_round}")
                break

            if idle_rounds >= MAX_IDLE_ROUNDS:
                print(f"âœ… åœæ­¢æ»šåŠ¨ï¼šå·²è¿ç»­ {idle_rounds} æ¬¡æ²¡æœ‰å‘ç°é¡µé¢é«˜åº¦å˜åŒ–ã€‚")
                break

            # å®‰å…¨ä¸Šé™ï¼Œé¿å…æ— é™å¾ªç¯
            if scroll_round >= 80:
                print("âš ï¸ è¾¾åˆ°æœ€å¤§æ»šåŠ¨æ¬¡æ•°ï¼Œåœæ­¢æ»šåŠ¨ä»¥é¿å…æ— é™å¾ªç¯ã€‚")
                break

        # === æ»šåŠ¨åŠ è½½ç»“æŸï¼Œå¼€å§‹æå– ===
        post_containers = self.driver.find_elements(By.CSS_SELECTOR, self.main_container_selector)
        last_count = len(post_containers)
        print(f"ğŸ–¼ï¸ å…±æ£€æµ‹åˆ° {last_count} ä¸ªå›¾ç‰‡å®¹å™¨ã€‚")

        successful_writes = 0
        for idx, card_ele in enumerate(post_containers):
            try:
                # å°è¯•å¤šç§æ–¹å¼è·å–å›¾ç‰‡å…ƒç´  / é“¾æ¥
                img_ele = None
                try:
                    img_ele = card_ele.find_element(By.TAG_NAME, 'img')
                except NoSuchElementException:
                    # å¦‚æœæ²¡æœ‰ imgï¼Œå¯å¯»æ‰¾ background-image æˆ– a>img ä¹‹ç±»
                    try:
                        img_ele = card_ele.find_element(By.CSS_SELECTOR, 'div img')
                    except NoSuchElementException:
                        img_ele = None

                if not img_ele:
                    print(f"[è·³è¿‡] å®¹å™¨åºå· {idx}ï¼šæœªæ‰¾åˆ° img å…ƒç´ ã€‚")
                    continue

                # å…ˆå°è¯• srcï¼Œå†å°è¯• data-src æˆ– srcset
                image_url = img_ele.get_attribute('src') or img_ele.get_attribute('data-src') or ''
                if not image_url:
                    srcset = img_ele.get_attribute('srcset') or ''
                    if srcset:
                        # ä» srcset ä¸­é€‰æœ€å¤§çš„ (é€šå¸¸æœ€åä¸€ä¸ª)
                        parts = [p.strip() for p in srcset.split(',') if p.strip()]
                        if parts:
                            last = parts[-1].split()[0]
                            image_url = last

                if not image_url:
                    print(f"[è·³è¿‡] å®¹å™¨åºå· {idx}ï¼šå›¾ç‰‡ URL ä¸ºç©ºã€‚")
                    continue

                # --- æå–æ ‡é¢˜ï¼ˆä¼˜å…ˆä» a.link æˆ– captionï¼‰ ---
                try:
                    title_ele = card_ele.find_element(By.CSS_SELECTOR, 'div.collection-object_title__1MnJJ a.collection-object_link__qM3YR')
                    title = title_ele.text.strip()
                except NoSuchElementException:
                    # å¤‡ç”¨ï¼š figcaption ç›´æ¥æ–‡æœ¬
                    try:
                        figcap = card_ele.find_element(By.TAG_NAME, 'figcaption')
                        title = figcap.text.splitlines()[0].strip() if figcap.text else "NAN"
                    except Exception:
                        title = "NAN"

                # --- æ¸…ç†å›¾ç‰‡ URL (æŠŠå¸¸è§çš„ç§»åŠ¨å°ºå¯¸æ›¿æ¢ä¸º original) ---
                # ä¾‹å¦‚: .../mobile-large/DP701752.jpg  -> .../original/DP701752.jpg
                image_url_cleaned = re.sub(r'/mobile-[^/]+/', '/original/', image_url, flags=re.IGNORECASE)
                # å¦å¤–ä¸€äº›è·¯å¾„å¯èƒ½æ˜¯ /web-large/ æˆ– /medium/ ï¼Œè¦†ç›–å¸¸è§æƒ…å†µ
                image_url_cleaned = re.sub(r'/(web|medium|small|thumb)-?large?/', '/original/', image_url_cleaned, flags=re.IGNORECASE)

                # è§„èŒƒåŒ–é“¾æ¥ï¼ˆè‹¥ä¸ºç›¸å¯¹é“¾æ¥ï¼‰
                if image_url_cleaned.startswith('/'):
                    image_url_cleaned = urllib.parse.urljoin(self.base_url, image_url_cleaned)

                # --- å»é‡ ---
                md5_hash = hashlib.md5(image_url_cleaned.encode('utf-8')).hexdigest()
                if self.is_duplicate(md5_hash):
                    print(f"[é‡å¤] å®¹å™¨åºå· {idx}ï¼šã€{title}ã€‘ URL å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                    continue

                # --- å†™å…¥ CSV ---
                image_name = os.path.basename(urllib.parse.urlparse(image_url_cleaned).path)
                self.write_to_csv(title, image_name, image_url_cleaned, csv_path, tag)
                print(f"âœ”ï¸ å†™å…¥æˆåŠŸï¼ã€{image_url_cleaned}ã€‘")
                successful_writes += 1

            except Exception as e:
                print(f"[âœ—] å®¹å™¨åºå· {idx} æå–å›¾ç‰‡ä¿¡æ¯å¤±è´¥: {e}")

        print(f"âœ… ã€{tag}ã€‘æœ¬é¡µå…±æ£€æµ‹ {len(post_containers)} ä¸ªå®¹å™¨ï¼ŒæˆåŠŸå†™å…¥ {successful_writes} æ¡è®°å½•ã€‚")

    # ---------------------- å»é‡é€»è¾‘ ----------------------
    def is_duplicate(self, md5_hash):
        if self.redis:
            try:
                if self.redis.sismember(self.redis_key, md5_hash):
                    return True
                self.redis.sadd(self.redis_key, md5_hash)
            except Exception as e:
                # è‹¥ Redis å‡ºé”™ï¼Œé€€å›åˆ°å†…å­˜é›†åˆï¼ˆç¡®ä¿è„šæœ¬ä¸å´©æºƒï¼‰
                print(f"âš ï¸ Redis æ“ä½œå‡ºé”™ï¼Œé€€å›åˆ°å†…å­˜å»é‡: {e}")
                if not hasattr(self, 'visited_md5'):
                    self.visited_md5 = set()
                if md5_hash in self.visited_md5:
                    return True
                self.visited_md5.add(md5_hash)
        else:
            if md5_hash in self.visited_md5:
                return True
            self.visited_md5.add(md5_hash)
        return False

    # ---------------------- å†™å…¥ CSV ----------------------
    def write_to_csv(self, title, name, url, csv_path, tag):
        try:
            with self.csv_lock:
                is_file_empty = not os.path.exists(csv_path) or os.stat(csv_path).st_size == 0
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(csv_path), exist_ok=True)
                with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    if is_file_empty:
                        writer.writerow(['Title', 'ImageName', 'URL', 'Tag'])
                    writer.writerow([title, name, url, tag])
        except Exception as e:
            print(f"[âœ—] å†™å…¥ CSV å‡ºé”™: {e}")

    # ---------------------- ç¿»é¡µé€»è¾‘ ----------------------
    def crawl_page(self, tag, csv_path):
        page_num = 1
        wait = WebDriverWait(self.driver, 15)

        while True:
            print(f"\n=== æ­£åœ¨çˆ¬å–ã€{tag}ã€‘ç¬¬ {page_num} é¡µ ===")
            self.get_images(tag, csv_path)

            try:
                # ç¿»é¡µï¼šå¯»æ‰¾ aria-label="Next Page" çš„æŒ‰é’®å¹¶ç‚¹å‡»ï¼ˆè¿™æ˜¯æŒ‰é’®ä¸æ˜¯ a.hrefï¼‰
                # å¤‡ç”¨é€‰æ‹©å™¨ï¼š button[aria-label*="Next"] æˆ– button.pagination-controls_paginationButton__JT_jI[aria-label="Next Page"]
                try:
                    next_btn = self.driver.find_element(By.CSS_SELECTOR, 'button[aria-label="Next Page"]')
                except NoSuchElementException:
                    # æœ‰äº›é¡µé¢ button ä½ç½®ä¸åŒï¼Œå°è¯•æ›´å®½æ¾çš„åŒ¹é…
                    try:
                        next_btn = self.driver.find_element(By.CSS_SELECTOR, 'button[aria-label*="Next"]')
                    except NoSuchElementException:
                        next_btn = None

                if not next_btn:
                    print(f"[å®Œæˆ] æœªæ‰¾åˆ° Next æŒ‰é’®ï¼Œè®¤ä¸ºå·²åˆ°æœ€åä¸€é¡µã€‚å…±çˆ¬å– {page_num} é¡µã€‚")
                    break

                # å¦‚æœæŒ‰é’®æ˜¯ç¦ç”¨çš„ï¼ˆç±»åæˆ– disabled å±æ€§ï¼‰ï¼Œåˆ™ç»“æŸ
                disabled = next_btn.get_attribute('disabled')
                if disabled:
                    print(f"[å®Œæˆ] Next æŒ‰é’®å·²ç¦ç”¨ï¼Œç»“æŸç¿»é¡µã€‚å…±çˆ¬å– {page_num} é¡µã€‚")
                    break

                # ç‚¹å‡»ä¸‹ä¸€é¡µ
                try:
                    self.driver.execute_script("arguments[0].scrollIntoView({block:'center'});", next_btn)
                    time.sleep(0.5)
                    next_btn.click()
                except ElementClickInterceptedException:
                    # å¦‚æœç‚¹å‡»è¢«æ‹¦æˆªï¼Œå°è¯• JS ç‚¹å‡»
                    self.driver.execute_script("arguments[0].click();", next_btn)

                # éšæœºç­‰å¾…å¹¶ç¡®ä¿æ–°é¡µé¢åŠ è½½å‡ºä¸»å®¹å™¨
                time.sleep(random.uniform(2.5, 4.5))
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.main_container_selector)))
                page_num += 1

            except TimeoutException:
                print(f"[âœ—] ç¿»é¡µåé¡µé¢åŠ è½½è¶…æ—¶ï¼Œå¯èƒ½æ²¡æœ‰æ–°å†…å®¹ã€‚å·²å®Œæˆ {page_num} é¡µã€‚")
                break
            except Exception as e:
                print(f"[âœ—] ç¿»é¡µè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                break

    # ---------------------- ä¸»å‡½æ•° ----------------------
    def main(self, save_dir, tag_file_path, csv_name='all_records_metmuseum_01.csv'):
        os.makedirs(save_dir, exist_ok=True)
        csv_path = os.path.join(save_dir, csv_name)

        try:
            with open(tag_file_path, 'r', encoding='utf-8') as f:
                tags = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"[é”™è¯¯] æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {tag_file_path}")
            return

        print(f"--- å‘ç° {len(tags)} ä¸ªæ ‡ç­¾ ---")

        for tag in tags:
            try:
                encoded_tag = urllib.parse.quote_plus(tag)
                search_url = f"{self.base_url}art/collection/search?q={encoded_tag}"
                print(f"\n--- å¼€å§‹å¤„ç†ï¼šã€{tag}ã€‘ ---\nURL: {search_url}")
                self.driver.get(search_url)
                time.sleep(random.uniform(2.5, 4.0))

                try:
                    WebDriverWait(self.driver, 15).until(
                        lambda d: d.find_elements(By.CSS_SELECTOR, self.main_container_selector))
                except TimeoutException:
                    print(f"[è·³è¿‡] {tag} é¡µé¢æœªåŠ è½½å‡ºå›¾ç‰‡å®¹å™¨ã€‚")
                    continue

                self.crawl_page(tag, csv_path)
            except Exception as e:
                print(f"[âœ—] å¤„ç†æ ‡ç­¾ {tag} å‡ºé”™: {e}")

    def quit(self):
        try:
            if self.driver:
                self.driver.quit()
        except Exception as e:
            print(f"å…³é—­æµè§ˆå™¨å‡ºé”™: {e}")


if __name__ == '__main__':
    # âš ï¸ è¯·ç¡®è®¤ä½ çš„é©±åŠ¨è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼
    chrome_driver_path = r'C:\Program Files\Google\1\chromedriver-win32\chromedriver.exe'

    # ä¿å­˜è·¯å¾„ä¸æ ‡ç­¾æ–‡ä»¶ï¼ˆç¤ºä¾‹ï¼‰
    save_dir = r'D:\work\çˆ¬è™«\çˆ¬è™«æ•°æ®\metmuseum'
    tag_file_path = r'D:\work\çˆ¬è™«\ram_tag_list.txt'

    spider = None
    try:
        spider = metmuseum(chrome_driver_path, use_headless=True)
        spider.main(save_dir=save_dir, tag_file_path=tag_file_path)
    except Exception as main_e:
        print(f"ä¸»ç¨‹åºè¿è¡Œå‡ºé”™: {main_e}")
    finally:
        if spider:
            print("\næ­£åœ¨å…³é—­æµè§ˆå™¨...")
            spider.quit()
