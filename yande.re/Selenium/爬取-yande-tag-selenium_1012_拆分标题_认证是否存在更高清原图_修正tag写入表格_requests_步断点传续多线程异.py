from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
# <--- æ–°å¢ï¼šå¯¼å…¥å¹¶å‘å¤„ç†åº“ --->
from concurrent.futures import ThreadPoolExecutor
import os, time, hashlib, redis, csv, threading, random, re, urllib.parse, requests
from datetime import datetime, timedelta


class yande_re:
    def __init__(self, chrome_driver_path, use_headless=True, redis_host='localhost', redis_port=6379, download_workers=10, parser_workers=3, tag_workers=3):
        self.chrome_driver_path = chrome_driver_path
        self.use_headless = use_headless
        self.base_url = 'https://yande.re/'
        self.csv_lock = threading.Lock()
        self.redis_key = 'image_md5_set_yande.re'
        self.image_save_dir = ''
        self.progress_file_path = ''  # å°†åœ¨mainæ–¹æ³•ä¸­è®¾ç½®
        self.set_lock = threading.Lock()
        self.parser_workers = parser_workers
        self.download_workers = download_workers
        self.tag_workers = tag_workers
        # ä»…åˆå§‹åŒ–ä¸‹è½½çº¿ç¨‹æ± ï¼Œè§£æçº¿ç¨‹æ± åœ¨æ¯ä¸ªtagä»»åŠ¡ä¸­ä¸´æ—¶åˆ›å»º
        self.download_executor = ThreadPoolExecutor(max_workers=self.download_workers, thread_name_prefix='Downloader')
        print(f"âœ… ä¸‹è½½çº¿ç¨‹æ± å·²å¯åŠ¨ï¼Œæœ€å¤§çº¿ç¨‹æ•°: {self.download_workers}")
        self.main_container_selector = 'div#post-list-posts li[id^="p"], div#content li[id^="p"]'
        try:
            self.redis = redis.Redis(host=redis_host, port=redis_port, db=0, decode_responses=True)
            self.redis.ping()
            print("âœ… Redis è¿æ¥æˆåŠŸã€‚")
        except Exception:
            print("âš ï¸ Redis ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å†…å­˜å»é‡ã€‚")
            self.redis = None
            self.visited_md5 = set()

    def resume_incomplete_downloads(self, csv_path):
        import pandas as pd
        if not os.path.exists(csv_path):
            return
        try:
            df = pd.read_csv(csv_path)
        except Exception as e:
            print(f"[æ–­ç‚¹è¡¥å…¨] è¯»å–CSVå¤±è´¥: {e}")
            return
        for _, row in df.iterrows():
            image_name = row.get('ImageName')
            week_label = row.get('WeekLabel')
            image_url = row.get('URL')
            if not image_name or not week_label or not image_url:
                continue
            safe_week_folder_name = re.sub(r'[\\/*?:"<>|]', '_', str(week_label))
            save_path = os.path.join(self.image_save_dir, safe_week_folder_name, image_name)
            temp_path = save_path + ".downloading"
            if not os.path.exists(save_path) or os.path.exists(temp_path):
                print(f"[æ–­ç‚¹è¡¥å…¨] é‡æ–°ä¸‹è½½æœªå®Œæˆå›¾ç‰‡: {image_name}")
                self.download_executor.submit(self.download_image, image_url, save_path)
    def process_tag(self, tag, csv_path, enable_download=True):
        # ç‹¬ç«‹åˆ›å»ºSeleniumå®ä¾‹
        service = Service(executable_path=self.chrome_driver_path)
        options = Options()
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument(
            'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        )
        # if self.use_headless:
        #     options.add_argument('--headless=new')
        # options.add_argument("--window-size=1920,1080")
        driver = webdriver.Chrome(service=service, options=options)
        parser_executor = ThreadPoolExecutor(max_workers=self.parser_workers, thread_name_prefix='Parser')
        try:
            search_url = f"{self.base_url}{tag}"
            print(f"\nğŸ“„ å¼€å§‹å¤„ç†æ–°æ ‡ç­¾ï¼šã€{tag}ã€‘\nURL: {search_url}")
            driver.get(search_url)
            wait = WebDriverWait(driver, 30)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.main_container_selector)))
            print("[âˆš] ä¸»å›¾ç‰‡å®¹å™¨åŠ è½½å®Œæˆã€‚")
            post_containers = driver.find_elements(By.CSS_SELECTOR, self.main_container_selector)
            total_cards = len(post_containers)
            print(f"ğŸ–¼ï¸ å…±æ£€æµ‹åˆ° {total_cards} ä¸ªå›¾ç‰‡å®¹å™¨ï¼Œå¼€å§‹å¹¶è¡Œå¤„ç†...")
            futures = []
            for card_ele in post_containers:
                future = parser_executor.submit(self.process_image_card, card_ele, tag, csv_path, enable_download)
                futures.append(future)
            processed_count = 0
            for future in futures:
                if future.result():
                    processed_count += 1
            print(f"âœ… æ ‡ç­¾ã€{tag}ã€‘å¤„ç†å®Œæˆ: æˆåŠŸå¤„ç† {processed_count}/{total_cards} ä¸ªå›¾ç‰‡")
            self.mark_tag_as_processed(tag)
            delay = random.uniform(1, 3)
            print(f"â³ éšæœºç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªæ ‡ç­¾...")
            time.sleep(delay)
        except Exception as e:
            print(f"[âœ—] å¤„ç†æ ‡ç­¾ {tag} æ—¶é‡åˆ°ä¸¥é‡é”™è¯¯: {e}")
        finally:
            parser_executor.shutdown(wait=True)
            driver.quit()

    # ä¸‹è½½åŠŸèƒ½ï¼ˆå·²åŒ…å«æ–­ç‚¹ä¸‹è½½é€»è¾‘ï¼‰
    def download_image(self, image_url, save_path):
        temp_path = save_path + ".downloading"
        try:
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            if os.path.exists(save_path):
                print(f"ğŸŸ¡ [å·²å®Œæˆ] è·³è¿‡: {os.path.basename(save_path)}")
                return
            if os.path.exists(temp_path):
                print(f"ï¿½ [æ–­ç‚¹ç»­ä¼ ] æ£€æµ‹åˆ°æœªå®Œæˆä¸‹è½½ï¼Œé‡æ–°ä¸‹è½½: {os.path.basename(save_path)}")
                os.remove(temp_path)
            print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½: {os.path.basename(save_path)}")
            response = requests.get(image_url, stream=True, timeout=60)
            response.raise_for_status()
            total_size = int(response.headers.get('content-length', 0))
            block_size = 8192
            downloaded = 0
            with open(temp_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=block_size):
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size:
                        percent = int((downloaded / total_size) * 100)
                        print(f"\rğŸ’¾ ä¸‹è½½è¿›åº¦: {percent}% [{downloaded}/{total_size} bytes]", end='', flush=True)
            os.rename(temp_path, save_path)
            print(f"\nâœ… ä¸‹è½½å®Œæˆ: {os.path.basename(save_path)}")
        except requests.exceptions.RequestException as e:
            print(f"âŒ [ä¸‹è½½å¤±è´¥] ç½‘ç»œè¯·æ±‚é”™è¯¯: {e} | URL: {image_url}")
            if os.path.exists(temp_path):
                os.remove(temp_path)
        except Exception as e:
            print(f"âŒ [ä¸‹è½½å¤±è´¥] æœªçŸ¥é”™è¯¯: {e} | ä¿å­˜è·¯å¾„: {save_path}")
            if os.path.exists(temp_path):
                os.remove(temp_path)

    # åŸå›¾æ£€æµ‹é€»è¾‘
    def get_hq_image_url(self, lowres_url):
        try:
            if "/jpeg/" in lowres_url or lowres_url.endswith(".jpg"):
                hq_url = lowres_url.replace("/jpeg/", "/image/").rsplit(".", 1)[0] + ".png"
                resp = requests.head(hq_url, timeout=10)    # å¦‚æœç½‘ç»œæ…¢å¯ä»¥è®¾ç½®æ›´å¤§è¶…æ—¶
                if resp.status_code == 200:
                    print(f"ğŸŸ¢ æ£€æµ‹åˆ°æ›´é«˜æ¸…åŸå›¾: {hq_url[:100]}...") # æ‰“å°å‰100ä¸ªå­—ç¬¦ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                    return hq_url
                else:
                    print(f"âšª æ— é«˜æ¸…åŸå›¾: {hq_url[:100]}... [{resp.status_code}]") # æ‰“å°å‰100ä¸ªå­—ç¬¦ï¼Œé¿å…æ—¥å¿—è¿‡é•¿
                    pass
        except Exception as e:
            print(f"âš ï¸ åŸå›¾æ£€æµ‹å¼‚å¸¸: {e}, é€šå¸¸å› ä¸ºç½‘ç»œé—®é¢˜ã€‚")
        return lowres_url

    # tag è½¬æ¢é€»è¾‘
    def parse_tag_to_week_label(self, tag: str) -> str:
        try:
            match = re.search(r'day=(\d+)&month=(\d+)&year=(\d+)', tag)
            if not match:
                return tag 

            day, month, year = map(int, match.groups())
            start_date = datetime(year, month, day)
            end_date = start_date + timedelta(days=6)
            iso_year, iso_week, _ = end_date.isocalendar()
            week_label = (
                f"{start_date.year}å¹´{start_date.month}æœˆ{start_date.day}æ—¥ - "
                f"{end_date.year}å¹´{end_date.month}æœˆ{end_date.day}æ—¥ï¼ˆ{iso_year}å¹´ç¬¬{iso_week}å‘¨ï¼‰"
            )
            return week_label
        except Exception as e:
            print(f"âš ï¸ tag è½¬æ¢å¤±è´¥: {e} | åŸå§‹: {tag}")
            return tag

    # ---------------------- ä¸»é€»è¾‘ ----------------------
    def get_images(self, tag, csv_path):
        print(f"--- æ­£åœ¨è§£æã€{tag}ã€‘çš„å›¾ç‰‡åˆ—è¡¨...")

        wait = WebDriverWait(self.driver, 30)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.main_container_selector)))
            print("[âˆš] ä¸»å›¾ç‰‡å®¹å™¨åŠ è½½å®Œæˆã€‚")
        except TimeoutException:
            print("[âœ—] é¡µé¢åŠ è½½è¶…æ—¶ï¼Œæ‰¾ä¸åˆ°ä¸»å›¾ç‰‡å®¹å™¨ã€‚")
            return

        post_containers = self.driver.find_elements(By.CSS_SELECTOR, self.main_container_selector)
        print(f"ğŸ–¼ï¸ å…±æ£€æµ‹åˆ° {len(post_containers)} ä¸ªå›¾ç‰‡å®¹å™¨ã€‚")

        for idx, card_ele in enumerate(post_containers):
            try:
                try:
                    img_ele = card_ele.find_element(By.CSS_SELECTOR, 'a.directlink.largeimg')
                except NoSuchElementException:
                    try:
                        img_ele = card_ele.find_element(By.CSS_SELECTOR, 'a[href*="files.yande.re"]')
                    except NoSuchElementException:
                        continue

                image_url = img_ele.get_attribute('href') or ''
                if not image_url:
                    continue

                image_url = self.get_hq_image_url(image_url)

                try:
                    title_ele = card_ele.find_element(By.CSS_SELECTOR, 'a.thumb img.preview')
                    title = title_ele.get_attribute('title').strip()
                except NoSuchElementException:
                    title = "N/A"

                rating, score, tags_text, user = self.parse_title_info(title)

                md5_hash = hashlib.md5(image_url.encode('utf-8')).hexdigest()
                if self.is_duplicate(md5_hash):
                    continue

                week_label = self.parse_tag_to_week_label(tag)
                image_name = os.path.basename(urllib.parse.urlparse(image_url).path)
                
                # å†™å…¥CSVæ˜¯å¿«é€Ÿæ“ä½œï¼Œä¿æŒåœ¨ä¸»çº¿ç¨‹
                self.write_to_csv(rating, score, tags_text, user, image_name, image_url, csv_path, week_label)

                # <--- ä¿®æ”¹ï¼šå°†ä¸‹è½½ä»»åŠ¡æäº¤åˆ°çº¿ç¨‹æ± ï¼Œå®ç°å¼‚æ­¥ä¸‹è½½ --->
                safe_week_folder_name = re.sub(r'[\\/*?:"<>|]', '_', week_label)
                full_save_path = os.path.join(self.image_save_dir, safe_week_folder_name, image_name)
                self.download_executor.submit(self.download_image, image_url, full_save_path)

            except Exception as e:
                print(f"[âœ—] æå–å¤±è´¥ idx={idx}: {e}")

    # æ ‡é¢˜è§£æ
    def parse_title_info(self, title_text):
        rating, score, tags, user = "N/A", "N/A", "N/A", "N/A"
        try:
            match = re.search(r'Rating:\s*([A-Za-z]+)\s+Score:\s*(\d+)\s+Tags:\s*(.*?)\s+User:\s*(\S+)', title_text)
            if match:
                rating, score, tags, user = match.groups()
        except Exception as e:
            print(f"[âš ï¸] æ ‡é¢˜è§£æå¤±è´¥: {e} | åŸå§‹: {title_text}")
        return rating.strip(), score.strip(), tags.strip(), user.strip()

    # <--- ä¿®æ”¹ï¼šä¸ºå†…å­˜Setæ“ä½œå¢åŠ çº¿ç¨‹é” --->
    def is_duplicate(self, md5_hash):
        if hasattr(self, 'redis') and self.redis:
            try:
                if not self.redis.sadd(self.redis_key, md5_hash): # saddè¿”å›0è¡¨ç¤ºå·²å­˜åœ¨
                    return True
            except Exception as e:
                print(f"âš ï¸ Redis æ“ä½œå¤±è´¥: {e}ï¼Œä¸´æ—¶ä½¿ç”¨å†…å­˜å»é‡ã€‚")
                # Fallback to memory set with lock
                with self.set_lock:
                    if md5_hash in self.visited_md5:
                        return True
                    self.visited_md5.add(md5_hash)
        else:
            with self.set_lock:
                if md5_hash in self.visited_md5:
                    return True
                self.visited_md5.add(md5_hash)
        return False

    # CSVå†™å…¥
    def write_to_csv(self, rating, score, tags, user, name, url, csv_path, tag_label):
        try:
            with self.csv_lock:
                is_file_empty = not os.path.exists(csv_path) or os.stat(csv_path).st_size == 0
                os.makedirs(os.path.dirname(csv_path), exist_ok=True)
                with open(csv_path, 'a', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    if is_file_empty:
                        writer.writerow(['Rating', 'Score', 'Tags', 'User', 'ImageName', 'URL', 'WeekLabel'])
                    writer.writerow([rating, score, tags, user, name, url, tag_label])
                    print(f"[âœ“] å·²å†™å…¥CSV è¯„åˆ†: {score} | URL: {url[:50]}... ")
        except Exception as e:
            print(f"[âœ—] å†™å…¥ CSV å‡ºé”™: {e}")

    # <--- æ–°å¢ï¼šæ–­ç‚¹çˆ¬å–ç›¸å…³æ–¹æ³• --->
    def load_processed_tags(self):
        """åŠ è½½å·²å¤„ç†çš„æ ‡ç­¾åˆ—è¡¨"""
        if not os.path.exists(self.progress_file_path):
            return set()
        try:
            with open(self.progress_file_path, 'r', encoding='utf-8') as f:
                return {line.strip() for line in f}
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            return set()

    def mark_tag_as_processed(self, tag):
        """å°†å¤„ç†å®Œæˆçš„æ ‡ç­¾å†™å…¥è¿›åº¦æ–‡ä»¶"""
        try:
            with open(self.progress_file_path, 'a', encoding='utf-8') as f:
                f.write(tag + '\n')
                f.flush()  # ç«‹å³å†™å…¥æ–‡ä»¶
            print(f"\n{'='*20} è¿›åº¦ä¿å­˜ {'='*20}")
            print(f"âœ… å·²å°†æ ‡ç­¾ã€{tag}ã€‘æ ‡è®°ä¸ºå·²å¤„ç†")
            print(f"ğŸ“ ä¿å­˜è‡³: {self.progress_file_path}")
            print('='*50 + '\n')
        except Exception as e:
            print(f"âš ï¸ å†™å…¥è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")

    def process_image_card(self, card_ele, tag, csv_path, enable_download=True):
        """å¼‚æ­¥å¤„ç†å•ä¸ªå›¾ç‰‡å¡ç‰‡"""
        try:
            try:
                img_ele = card_ele.find_element(By.CSS_SELECTOR, 'a.directlink.largeimg')
            except NoSuchElementException:
                try:
                    img_ele = card_ele.find_element(By.CSS_SELECTOR, 'a[href*="files.yande.re"]')
                except NoSuchElementException:
                    return None

            image_url = img_ele.get_attribute('href') or ''
            if not image_url:
                return None

            image_url = self.get_hq_image_url(image_url)

            try:
                title_ele = card_ele.find_element(By.CSS_SELECTOR, 'a.thumb img.preview')
                title = title_ele.get_attribute('title').strip()
            except NoSuchElementException:
                title = "N/A"

            rating, score, tags_text, user = self.parse_title_info(title)

            md5_hash = hashlib.md5(image_url.encode('utf-8')).hexdigest()
            if self.is_duplicate(md5_hash):
                return None

            week_label = self.parse_tag_to_week_label(tag)
            image_name = os.path.basename(urllib.parse.urlparse(image_url).path)
            # å†™å…¥CSVæ˜¯å¿«é€Ÿæ“ä½œï¼Œä¿æŒåœ¨ä¸»çº¿ç¨‹
            self.write_to_csv(rating, score, tags_text, user, image_name, image_url, csv_path, week_label)
            # å¯é€‰ä¸‹è½½
            if enable_download:
                safe_week_folder_name = re.sub(r'[\\/*?:"<>|]', '_', week_label)
                full_save_path = os.path.join(self.image_save_dir, safe_week_folder_name, image_name)
                self.download_executor.submit(self.download_image, image_url, full_save_path)
            return True
        except Exception as e:
            print(f"[âœ—] å¤„ç†å›¾ç‰‡å¡ç‰‡å¤±è´¥: {e}")
            return None

    def main(self, save_dir, tag_file_path, csv_name='all_records_yande_re_v4.csv', enable_download=False):  # å¼€å¯ä¸‹è½½è®¾ä¸ºTrueï¼Œä¸å¼€å¯è®¾ç½®ä¸ºFalse
        print("\n" + "="*50)
        print("ğŸš€ çˆ¬è™«å¯åŠ¨é…ç½®:")
        print(f"ğŸ“‚ ä¿å­˜ç›®å½•: {save_dir}")
        print(f"ğŸ”§ å¼€å¯ä¸‹è½½: {enable_download}")  # æ‰“å°ä¸‹è½½é…ç½®
        print(f"ğŸ“„ æ ‡ç­¾æ–‡ä»¶: {tag_file_path}")
        print(f"ğŸ“Š è®°å½•æ–‡ä»¶: {csv_name}")
        print("="*50 + "\n")

        os.makedirs(save_dir, exist_ok=True)
        csv_path = os.path.join(save_dir, csv_name)
        self.image_save_dir = os.path.join(save_dir, 'images')
        os.makedirs(self.image_save_dir, exist_ok=True)
        print(f"â„¹ï¸ å›¾ç‰‡å°†ä¿å­˜åˆ°: {self.image_save_dir}")
        self.progress_file_path = os.path.join(save_dir, 'processed_tags.txt')
        if not os.path.exists(self.progress_file_path):
            open(self.progress_file_path, 'a').close()
            print(f"âœ… å·²åˆ›å»ºæ–­ç‚¹ç»­ä¼ è®°å½•æ–‡ä»¶: {self.progress_file_path}")
        processed_tags = self.load_processed_tags()
        if processed_tags:
            print(f"âœ… å·²åŠ è½½ {len(processed_tags)} ä¸ªå·²å¤„ç†çš„æ ‡ç­¾è®°å½•ã€‚")
        # å¯åŠ¨æ—¶è¡¥å…¨æœªå®Œæˆå›¾ç‰‡ä¸‹è½½
        print("[æ–­ç‚¹è¡¥å…¨] æ£€æŸ¥å¹¶è¡¥å…¨æœªå®Œæˆå›¾ç‰‡ä¸‹è½½...")
        if enable_download:
            self.resume_incomplete_downloads(csv_path)
        try:
            with open(tag_file_path, 'r', encoding='utf-8') as f:
                all_tags = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"[é”™è¯¯] æœªæ‰¾åˆ°æ ‡ç­¾æ–‡ä»¶: {tag_file_path}")
            return
        print(f"--- æ€»å…±å‘ç° {len(all_tags)} ä¸ªæ ‡ç­¾ï¼Œç°åœ¨å¼€å§‹å¹¶è¡Œå¤„ç† ---")
        tag_executor = ThreadPoolExecutor(max_workers=self.tag_workers, thread_name_prefix='TagWorker')
        futures = []
        for tag in all_tags:
            if tag in processed_tags:
                print(f"â­ï¸ [è·³è¿‡] æ ‡ç­¾ '{tag}' å·²åœ¨ä¹‹å‰çš„è¿è¡Œä¸­å¤„ç†å®Œæ¯•ã€‚")
                continue
            future = tag_executor.submit(self.process_tag, tag, csv_path, enable_download)
            futures.append(future)
        for future in futures:
            future.result()
        tag_executor.shutdown(wait=True)

    # <--- ä¼˜é›…åœ°å…³é—­æµè§ˆå™¨å’Œçº¿ç¨‹æ±  --->
    def quit(self):
        print("\næ­£åœ¨ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ...")
        self.download_executor.shutdown(wait=True)  # ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ
        print("âœ… æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å·²å®Œæˆ")


if __name__ == '__main__':
    chrome_driver_path = r'C:\Program Files\Google\chromedriver-win64\chromedriver.exe'
    save_dir = r'R:\py\Auto_Image-Spider\yande.re\output_v4'
    tag_file_path = r'R:\py\Auto_Image-Spider\yande.re\ram_tag_å‘¨.txt'
    
    # å¯ä»¥é€šè¿‡åœ¨è¿™é‡Œå®ä¾‹åŒ–æ—¶ä¼ å…¥ download_workers=10, tag_workers=3 æ¥è°ƒæ•´çº¿ç¨‹æ•°
    spider = None
    try:
        spider = yande_re(chrome_driver_path, use_headless=True, download_workers=10, tag_workers=3)
        spider.main(save_dir=save_dir, tag_file_path=tag_file_path)
    except Exception as main_e:
        print(f"ä¸»ç¨‹åºè¿è¡Œå‡ºé”™: {main_e}")
    finally:
        if spider:
            spider.quit()